<!DOCTYPE html>
<html lang="ru"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Search | Alexey Evseev</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Search" />
<meta name="author" content="Alexey Evseev" />
<meta property="og:locale" content="ru" />
<link rel="canonical" href="https://st4lk.github.io/search/" />
<meta property="og:url" content="https://st4lk.github.io/search/" />
<meta property="og:site_name" content="Alexey Evseev" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Alexey Evseev"},"headline":"Search","url":"https://st4lk.github.io/search/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="shortcut icon" type="image/png" href="/favicon.png">
  <link rel="stylesheet" href="/assets/main.css">
  <link rel="stylesheet" href="/assets/css/styles.css"><link type="application/atom+xml" rel="alternate" href="https://st4lk.github.io/feed.xml" title="Alexey Evseev" /><link type="application/atom+xml" rel="alternate" href="https://st4lk.github.io/feed.xml" title="Alexey Evseev" /><!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8ZHLL9G1CF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-8ZHLL9G1CF');
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/">Алексей Евсеев</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          <a class="page-link" href="/blog/">Блог</a>
          <a class="page-link" href="/contact/">Контакты</a>
          <a class="page-link" style="margin-right: 0px" href="/search/" ><img src="/assets/images/ru.png" /></a>
          <a class="page-link" href="/en/search/" ><img src="/assets/images/en.png" /></a>
          <a class="page-link" href="/search/" ><img src="/assets/images/search.svg" /></a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <div class="post-content">
    <script src="/assets/js/lunr.js"></script>

<script>

var documents = [{
    "id": 0,
    "url": "https://st4lk.github.io/blog/2019/02/19/sublime-text-and-language-server-protocol-lsp.html",
    "title": "Sublime Text и Language Server Protocol",
    "body": "2019/02/19 -  Language Server Protocol (LSP): Language Server Protocol (LSP) - протокол для общения между IDE и языковым сервером. Сервер предоставляет такие функции, как автокомплит, переход к функции (goto) и прочее. Т. е. когда IDE нужно показать автокомплит на языке, скажем, python - происходит запрос к специальному серверу. В ответе возвращаются необходимые данные, которые IDE уже может отобразить. Радует то, что это инициатива крупной компании - Microsoft. Но в чем же смысл, ведь в большинстве IDE это итак уже работает? Смысл в унификации таких возможностей, в простоте разработки самих IDE. Получается, что сервер - один, а сред разработки - много. И все они могут общаться с одним сервером, им не нужно реализоваывать стандартные функции, специфичные для каждого конкретного языка. Все, что нужно сделать - это реализовать общение с сервером согласно LSP. С точки зрения пользователей самих IDE тоже есть плюс. К примеру, если на сервере добавят что-то новое, то такие обновления вы получите быстрее. Не важно, каким именно IDE или редактором кода вы пользуетесь, теперь это не зависит от этого. Главное, что ваш редактор должен поддерживать - это протокол общения с сервером. Можем считать, что он будет стабильным в ближайшее время. Тем более его поддерживает такой гигант, как Microsoft. Некоторые ссылки:  Официальная страница LSP: https://microsoft. github. io/language-server-protocol/ Список реализаций серверов: https://microsoft. github. io/language-server-protocol/implementors/servers/ Страница, поддерживаемая сообществом: https://langserver. org/Python Server: Давайте попробуем подружить IDE (на примере Sublime Text) с python language server. Первое, что нам понадобиться - это сам сервер, который будет отвечать на запросы LSP. На момент написания статьи, самый поддерживаемый сервер, написанный на python’e, этот: https://github. com/palantir/python-language-server. Есть так же сервер от Microsoft: https://github. com/Microsoft/python-language-server/. И даже есть краткая инструкция по настройке с Sublime Text. Для запуска этого сервера понадобится dotnet. Пробовал ставить, однако в полной мере он не выдавал нужный мне функционал. Возможно, я что-то недонастроил. К тому же я редко сталкиваюсь с C# в работе, поэтому понять внутренности для меня трудно, а иногда это бывает полезно. Но вернемся к серверу на питоне. Схема в голове была такая:  У нас есть отдельный virtualenv, созданный исключительно для установки сервера LSP В virtualenv нашего проекта никаких LSP серверов нет Наш IDE будет общаться с сервером, передавая параметры virtualenv’а нашего проекта. Таким образом, раз установив сервер, мы можем использовать его для разных проектов. Но на момент описания этой статьи сделать это было не просто. По крайней мере четкой документации, как это сделять, я не нашел. В документации к этому серверу как-то неявно предполагается, что он должен быть установлен в той же среде, что и ваш проект. В том же virtualenv. Но мне кажется это не совсем удобно. Поэтому пришлось создать PR. Так же, этот LSP не хотел брать настройки линтера из директории проекта. Например, в корне проекта у меня лежит setup. cfg с такими настройками: ignore = D202,D205,D211max-line-length = 99Чтобы сервер мог подхватывать их, понадобился еще один PR. Который в свою очередь преобразовался в этот. Были и другие проблемы, тоже приходилось погружаться в код и создавать PR’ы. Надеюсь, их скоро примут и включат в релиз. А пока, создал релиз в своем форке, где найденные мной ошибки исправлены. По порядку:  Создаем virtualenv специально для сервера LSP. Лучше python3. 6+.    Устанавливаем туда наш сервер (этот релиз уже содержит упомянутые выше исправления):    pip install https://github. com/st4lk/python-language-server/archive/0. 22. 0a1. zip   Если хотите создать requirements. txt - то туда можно вставлять ссылку на zip файл без каких-либо префиксов/суффиксов. Хотя возможно, что на момент чтения статьи это уже все исправлено в официальной версии &gt; 0. 22. Не поленитесь проверить основной репозиторий. Sublime Text: Теперь перейдем к настройке IDE. В этом посте речь пойдет о Sublime Text. Мне нравится этот редактор кода. Хоть это и не полноценный IDE, но я люблю его скорость, его vintage режим, мультикурсоры и много других маленьких фишек. Итак, как же подружить Sublime с LSP?В первую очередь, нам понадобится плагин для общения с LSP сервером: https://github. com/tomv564/LSP. Теперь самое важное - правильно его настроить. Идем в настройки нашего плагина (на MacOS это Sublime Text -&gt; Preferences -&gt; Package Settings -&gt; LSP -&gt; Settings). В пользовательских настройках вводим примерно следующее: {  clients : {   pyls :  {    enabled : true,    command : [     ~/. virtualenvs/python-language-server/bin/pyls    ],    settings : {     pyls : {      configurationSources : [ flake8 ],      plugins : {       jedi_definition : {        follow_imports : true      }     }    }   },    scopes : [ source. python ],    syntaxes : [     Packages/Python/Python. sublime-syntax ,     Packages/Djaneiro/Syntaxes/Python Django. tmLanguage    ],    languageId :  python   } }}Давайте пройдемся по основным настройкам. “command” - путь к исполняемому файлу нашего pyls сервера. Как видно, у меня он установлен в virtualenv ~/. virtualenvs/python-language-server/ (я использую virtualenvwrapper). Это не виртуальное окружение вашего проекта! Здесь установлен только pyls. “settings” : “pyls” : “configurationSources”. Тут можно указать, где брать настройки линтера для проекта. По умолчанию это “pycodestyle”, однако я чаще использую flake8. Т. о. pyls будет искать настройки в таких файлах: '. flake8', 'setup. cfg' (секция [flake8]), 'tox. ini'Настройка “follow_imports”: true. Без нее, goto будет показывать лишь место импорта, а не саму реализацию. Мне кажется такое поведение не приносит особой пользы, обычно я хочу посмотреть именно на реализацию функции или класса, а не то, откуда он был импорирован. Кстати, чтобы это было воспринято сервером, потребовался небольшой патч: https://github. com/palantir/python-language-server/pull/404/. Еще важный момент: настройка “syntaxes”. Если вы используйте плагин Djaneiro (добавляет ряд фишек для работы с Django проектом), то sublime будет считать синтаксис python файлов - Djaneiro, а не дефолтный Python. Поэтому нужно указать его для нашего LSP сервера, иначе Sublime просто не будет его запускать. Тут мы объявили глобальные настройки, которые будут применяться ко всем файлам с синтаксисом python. А как объявить настройки, специфичные для каждого конкретного проекта? Как ни странно, их можно объявить в настройках проекта :). Идем Project -&gt; Edit project и добавляем следующее: {  folders : [   {      path :  /Path/to/my/project    }, ],  settings : {   LSP :  {    pyls :   {     env :    {      VIRTUAL_ENV :  /path/to/my/project/virtualenv/     }   }  } }}С помощью “VIRTUAL_ENV” мы можем указать, где живет virtualenv проекта. Т. о. pyls будет знать, где искать установленные пакеты для автокомплита, goto и прочего. Итак, запускаем. Предположим, у нас такой django проект: project├── . . . ├── my_app│  ├── . . . │  ├── forms. py│  └── models. py├── manage. py└── setup. cfgПростейший файл models. py: from django. db import modelsclass Book(models. Model):       This is docstring of Book model.   Example for LSP demostration.        title = models. CharField(max_length=50)и forms. py: from django. forms import ModelFormfrom . models import Bookclass BookForm(ModelForm):  class Meta:    model = Book    fields = '__all__'В файле setup. cfg есть секция [flake8]: [flake8]ignore = D202,D205,D211max-line-length = 99Что же нам теперь доступно?Из коробки, сервер предоставляет возможности jedi, например Definitions (goto), References, автокомплит и кое-что еще. При наведении курсора отображается docstring объекта, а так же возможные действия над ним: При нажатии на Definition мы перейдем в файл и место, где класс объявлен. При нажатии на References - увидим, где объект используется: Так же работает автокомплит:  Но ошибки синтаксиса и pep8 отображаться не будут. Чтобы они проверялись, нужно установить в virtualenv с нашим python language server’ом эти пакеты: pip install mccabepip install pyflakespip install pycodestyleВуаля, теперь редактор выдает подсказки:  Напомню, что настройки для линтера автоматически берутся из setup. cfg (согласно моим настройкам pyls). Т. е. правила D202,D205,D211 будут игнорироваться, а максимальная длина строки будет 99, вместо дефолтной 80. Можно еще подключить к серверу mypy. Для этого нужен соответствующий плагин. Но это постараюсь описать в будущей статье, если руки дойдут, там тоже есть вопросы. В итоге у нас есть работающий Python Language Server в Sublime Text 3. Как видно, опыт не из простых, но я все еще держусь за этот редактор кода, привычка. Да, я иногда использую Pycharm, где фишек очень и очень много. И Visual Studio Code неплох, но к sublime’у сердце прикипело что ли. Какие могу стделать выводы:  немного сыровато все, приходится делать PR’ы, а принимаются они долго.  Sublime LSP не работает в правом окне (View -&gt; Layout -&gt; Columns: 2), нужно видимо опять дорабатывать стоит пристальнее посмотреть в сторону реализации python language server от microsoft, т. к. скорее всего тренд будет за ним. Ну а пока получился вполне рабочий вариант, пользуюсь им в повседневной работе. И спасибо Григорию Петрову за его доклад на Moscow Python. Из него я и узнал о существовании language server protocol. "
    }, {
    "id": 1,
    "url": "https://st4lk.github.io/blog/2017/01/14/count-filtered-related-objects-django.html",
    "title": "Ловушка при подсчете связанных объектов в Django",
    "body": "2017/01/14 -  Задача: для каждого объекта подсчитать количество связанных объектов, удовлетворяющих определенному условию. Пример: class Category(models. Model):  title = models. CharField(max_length=50)class Article(models. Model):  title = models. CharField(max_length=50)  category = models. ForeignKey(Category)  approved_at = models. DateTimeField(blank=True, null=True)Видим, что поле Article. approved_at хранит время одобрения статьи, причем оно может быть пустым (т. е. NULL в базе данных). Создадим тестовые данные: from django. utils import timezonec1 = Category. objects. create(title='c1')c2 = Category. objects. create(title='c2')a1 = Article. objects. create(category=c1, title='a1')a2 = Article. objects. create(category=c1, title='a2', approved_at=timezone. now())Итого у нас две категории. У первой есть одна одобренная статья и одна неодобренная. У второй категории статей нет. Для начала подсчитаем, сколько вообще статей в каждой категории: from django. db. models import Count&gt;&gt;&gt; Category. objects. annotate(. . .   article_count=Count('article'). . . ). values('title', 'article_count')&lt;QuerySet [{'article_count': 2, 'title': u'c1'}, {'article_count': 0, 'title': u'c2'}]&gt;SQL запрос, который построила ORM Django, вполне ожидаем: SELECT  main_category .  title , COUNT( main_article .  id ) AS  article_count   FROM  main_category   LEFT OUTER JOIN  main_article  ON ( main_category .  id  =  main_article .  category_id )  GROUP BY  main_category .  id ,  main_category .  title ;Ок, давайте подсчитаем только одобренные статьи. Если бы мы писали на SQL, то можно было бы просто добавить еще одно условие для JOIN: SELECT  main_category .  title , COUNT( main_article .  id ) AS  article_count   FROM  main_category   LEFT OUTER JOIN  main_article      ON ( main_category .  id  =  main_article .  category_id  AND       main_article .  approved_at  IS NOT NULL)  GROUP BY  main_category .  id ,  main_category .  title ;К сожалению, Django ORM не поддерживает фильтры для Count (по крайне мере v1. 10). Но начиная с v1. 8 у нас есть условные выражения и с помощью них можно выполнить такой трюк: from django. db. models import Count, Case, When&gt;&gt;&gt; Category. objects. annotate(. . .   article_count=Count(. . .     Case(When(article__approved_at__isnull=False, then=1)). . .   ). . . ). values('title', 'article_count')&lt;QuerySet [{'article_count': 1, 'title': u'c1'}, {'article_count': 0, 'title': u'c2'}]&gt;Данные верные. SQL запрос получился таким: SELECT  main_category .  title , COUNT(  CASE WHEN  main_article .  approved_at  IS NOT NULL THEN 1 ELSE NULL END) AS  article_count FROM  main_category LEFT OUTER JOIN  main_article  ON ( main_category .  id  =  main_article .  category_id )GROUP BY  main_category .  id ,  main_category .  title ;Ловушка: Теперь интересный вопрос, как нам подсчитать количество неодобренных статей? Первое, что приходит в голову, это просто поменять False на True в запросе: &gt;&gt;&gt; Category. objects. annotate(. . .   article_count=Count(. . .     Case(When(article__approved_at__isnull=True, then=1)). . .   ). . . ). values('title', 'article_count')Вот только ответ получим не совсем правильный: &lt;QuerySet [{'article_count': 1, 'title': u'c1'}, {'article_count': 1, 'title': u'c2'}]&gt;У второй категории откуда-то нашлась неодобренная статья. Смотрим SQL: SELECT  main_category .  title , COUNT(  CASE WHEN  main_article .  approved_at  IS NULL THEN 1 ELSE NULL END) AS  article_count FROM  main_category LEFT OUTER JOIN  main_article  ON ( main_category .  id  =  main_article .  category_id )GROUP BY  main_category .  id ,  main_category .  title ;Условие CASE WHEN  main_article .  approved_at  IS NULL THEN 1срабатывает даже тогда, когда у категории вообще нет статьи. В нашем случае запрос можно исправить так: &gt;&gt;&gt; Category. objects. annotate(. . .   article_count=Count(. . .     Case(. . .       When(. . .         article__id__isnull=False,. . .         article__approved_at__isnull=True,. . .         then=1. . .       ). . .     ). . .   ). . . ). values('title', 'article_count')&lt;QuerySet [{'article_count': 1, 'title': u'c1'}, {'article_count': 0, 'title': u'c2'}]&gt;Мораль: При проверках вида IS NULL нужно быть особенно осторожным и прикидывать возможные побочные эффекты! "
    }, {
    "id": 2,
    "url": "https://st4lk.github.io/blog/2016/08/01/django-signal-or-model-method.html",
    "title": "Django: сигнал или метод модели?",
    "body": "2016/08/01 -  Когда нужно написать какой-либо функционал, который должен быть выполнен при сохранении django модели, у меня всегда возникал вопрос - где его лучше реализовать. В сигнале или в методе save() модели? Давайте разберемся, что и в каких ситуациях более удобно. В каких случаях использовать методы модели save(), delete()?: На мой взгляд, использовать методы уместно, когда функционал касается исключительно данной модели. Например, при сохранении модели нужно заполнить какое-то поле автоматически, исходя из совокупности данных других полей. Часто приводят аргумент в пользу сигналов, что якобы их удобно использовать в похожих случаях. Т. е. один и тот же сигнал можно прикрепить к разным моделям. Аргумент для меня довольно странный, ведь точно так же можно объявить функцию или метод в классе-миксине и использовать их в save(). В принципе, все это можно сделать и в сигналах, почему я предпочитаю метод save? Ответ простой - это нагляднее. Когда вы смотрите на код модели, вы сразу видите, что будет происходить что-то при ее сохранении. В случае сигналов, особенно если нет четкого правила, где они объявлены, логика часто ускользает из виду. Следует отметить, что сигналы на удаление pre_delete, post_delete имеют то преимущество над методом delete(), что они вызываются при каскадном удалении объектов и при удалении всего queryset’а, чего не происходит в случае с методом модели. Тут нужно смотреть по ситуации, возможно массовым удалением можно пренебречь. А вот при массовом создании или обновлении объектов не вызывается ни метод модели save(), ни сигналы pre_save, post_save. Тут они равнозначны. Да, если вы переопределяете save() или delete(), не забудьте вызывать метод родительского класса. Когда лучше использовать сигналы?: Сигналы намного удобнее, если вы создаете переиспользуемое приложение. Тогда пользователи вашего приложения могут легко прикрепить ваши сигналы к своим моделям, не меняя код этих моделей. Альтернатива - это функция или класс-миксин. Но согласитесь, что логику из сторонней аппы все же удобнее прикрепить в виде сигнала. Это красивей и удобней. Кроме того, если вдруг вы решите отказаться от стороннего приложения, вы можете легко отцепить и его сигналы. Это справедливо и в том случае, когда у вас есть два приложения в рамках одного проекта (это не какие-то переиспользуемые аппы), и при сохранении модели из одного приложения вам нужно что-то сделать с моделью из другого. Например, есть аппа пользователей и аппа отчетов. При создании пользователя вам нужно автоматически создать отчет. В этом случае я предпочитаю создать сигнал в той аппе, к которой относится функционал, т. е. в приложении с отчетами. Почему так? Во-первых, мы держим логику в том месте, к которому эта логика относится. Во-вторых, если по каким-то причинам мы решим удалить отчеты из проекта, мы никак не затронем приложение пользователей. Где объявлять сигналы и где их прикреплять?: Как советует документация django (секция “Where should this code live?”), сигналы лучше хранить не в моделях и не в __init__. py, а в отдельном подмодулеsignals приложения. Это уберет головную боль с импортами. Но чтобы сигналы прикрепились, должен быть исполнен код, который их прикрепляет. Когда мы объявляем их в модуле с моделями, то код импортируется автоматически. Однако если код с сигналами объявлен в другом месте - он автоматически не выполнится. Поэтому нужно использовать метод ready() конфигурационного класса приложения. В целом, я следую рекомендации из этого ответа на stackoverflow. Приведу пример кода для уже упомянутого случая, когда есть приложение с отчетами (report) и нам нужно создавать отчет при создании нового пользователя.    Создаем в приложении подмодуль signals, в котором будет файл handlers. py   reports/signals/__init__. py reports/signals/handlers. py      Объявляем наши сигналы именно в файле handlers. py   from django. db. models. signals import post_save from django. dispatch import receiver from django. contrib. auth import get_user_model from reports. models import Report User = get_user_model() @receiver(post_save, sender=User) def create_user_report(sender, instance, created, **kwargs):   if created:     Report. objects. create(user=instance)      Создаем класс конфигурации приложения   reports/apps. py    С кодом:   from django. apps import AppConfig class ReportsConfig(AppConfig):   name = 'reports'   verbose_name = 'Reports'   def ready(self):     import reports. signals. handlers # noqa    Таким образом мы прикрепили сигнал. В данном случаем мы использовали декоратор @receiver, поэтому нам достаточно просто сделать импорт. Вместо декоратора тут можно было явно вызвать метод connect сигнала. Кому что больше нравится.   Не забываем указать, что наш класс ReportsConfig - это конфиг приложения. Для этого в reports/__init__. py добавляем строку:   default_app_config = 'reports. apps. ReportsConfig'    Либо указываем явно ReportsConfig в settings. INSTALLED_APPS. Смотри доки django.  Если придерживаться такой схемы, то мы будем всегда знать, где находятся обработчики. Соответственно, не нужно бегать по всему модулю с моделями в поисках сигналов. "
    }, {
    "id": 3,
    "url": "https://st4lk.github.io/blog/2015/09/30/trying-json-combo-django-and-postgresql.html",
    "title": "Пробуем JSON в Django и PostgreSQL (и сравниваем с MongoDB)",
    "body": "2015/09/30 -  В Django 1. 9 будет добавлено поле JSONField, его можно использовать с базой данных PostgreSQL &gt;= 9. 4. Давайте попробуем с ним поработать и оценить, насколько оно удобно. На данный момент доступна альфа версия django 1. 9, финальная запланирована на декабрь 2015. Установить альфа версию можно так: pip install --pre djangoИтак представим, что у нас есть интернет магазин, в котором мы предлагаем товары разных типов. Например, ноутбуки и футболки. Очевидно, что у таких товаров будет разный набор параметров: у футболок будет размер, цвет, а у ноутбуков - размер экрана, частота процессора, объем жесткого диска и прочее. Один из подходов для работы с такими данными в SQL - Entity–attribute–value model (EAV). Но теперь у нас есть JSON, попробуем организовать данные с помощью этого типа. Создадим простейшую модель для товаров: from django. db import modelsfrom django. contrib. postgres. fields import JSONFieldclass Category(models. Model):  name = models. CharField(max_length=100)class Product(models. Model):  name = models. CharField(max_length=100)  category = models. ForeignKey(Category)  price = models. IntegerField()  attributes = JSONField()  def __str__(self):    return self. nameКак видим, у нас будет несколько общих полей для всех товаров (name, category, price), а также специфичные для каждого наименования атрибуты (attributes) в виде поля JSON. Создадим несколько объектов: tshirt = Category. objects. create(name='tshirts')notebook = Category. objects. create(name='notebook')# TshirtsProduct. objects. create(name='Silk tshirt', category=tshirt, price=100, attributes={  'colors': ['red', 'black'],  'sizes': ['S', 'M'],  'model': 'polo',  'material': 'silk',})Product. objects. create(name='Bamboo tshirt', category=tshirt, price=120, attributes={  'colors': ['white', 'yellow'],  'sizes': ['M', 'L', 'XL'],  'model': 'poet',  'material': 'bamboo',})# NotebooksProduct. objects. create(name='MacBook Pro', category=notebook, price=2000, attributes={  'brand': 'Apple',  'screen': 15. 0,  'speed': 2200,  'hd': 256,})Product. objects. create(name='ATIV Book 9', category=notebook, price=1200, attributes={  'brand': 'Samsung',  'screen': 12. 2,  'speed': 2400,  'hd': 128,})Запросы: Посмотрим, какие запросы мы можем делать.    Получить все футболки размера ‘M’ и ‘L’ (у товара есть оба размера):   &gt;&gt;&gt; Product. objects. filter(category=tshirt, attributes__contains={'sizes': ['M', 'L']}) [&lt;Product: Bamboo tshirt&gt;]      Получить все футболки с размерами ‘M’ и ‘L’, белого и желтого цветов, с надписей (model = poet):   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'sizes': ['M', 'L'], 'colors': ['white', 'yellow'],   'model': 'poet'}) [&lt;Product: Bamboo tshirt&gt;]      Получить все ноутбуки с частотой процессора 2400 и диагональю экрана 12. 2   &gt;&gt;&gt; Product. objects. filter(category=notebook,   attributes__contains={'speed': 2400, 'screen': 12. 2}) [&lt;Product: ATIV Book 9&gt;]      Получить все футболки красного цвета, поло, с размерами ‘M’ или ‘L’   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'colors': ['red'], 'model': 'polo'},   attributes__sizes__has_any_keys=['M', 'L']) [&lt;Product: Silk tshirt&gt;]      Получить все ноутбуки с частотой процессора свыше 2000 и диагональю экрана больше 13   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__gt=2000,   attributes__screen__gt=13) [&lt;Product: MacBook Pro&gt;]      Получить все ноутбуки с частотой процессора 2200 или 2400   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__in=(2200, 2400)) [&lt;Product: ATIV Book 9&gt;, &lt;Product: MacBook Pro&gt;]    или так:   &gt;&gt;&gt; from django. db. models import Q &gt;&gt;&gt; Product. objects. filter(category=notebook). filter(   Q(attributes__contains={'speed': 2200}) | Q(attributes__contains={'speed': 2400}))   Индексы: Посмотрим, насколько эффективны могут быть наши запросы. В PostgreSQL для JSON полей можно применять разные индексы:    GIN   данный индекс в свою очередь может поддерживать различные операторы:      jsonb_ops (по умолчанию), поддерживает операторы @&gt;, ?, ?&amp;, ?|   jsonb_path_ops, поддерживает только оператор @&gt;, но работает быстрее и занимает меньше места      btree   может быть полезен только для поиска полного соответствия json документов     hash   как и btree, полезен для поиска полного соответствия json документов  Соответсвие некоторых операций в django и postgres операторов: Django    Postgres----------------------contains   @&gt;contained_by &lt;@has_key    ?has_any_keys ?|has_keys   ?&amp;В нашем случае наиболее интересный оператор - @&gt;, именно в него django транслирует фильтр contains для json полей. Если мы просто добавим db_index=True, будет создан btree индекс: class Product(models. Model):  . . .   attributes = JSONField(db_index=True)Для наших запросов намного полезней будет GIN индекс. Для его создания воспользуемся операцией RunSQL. Сперва создадим пустую миграцию. В моем случае приложение с товарами называется catalogue_simple python manage. py makemigrations --empty catalogue_simpleВ созданном файле (у меня это 0002_auto_20150928_1610. py) добавим пару импортов, а также команды для создания и отката индекса: from catalogue_simple. models import Productfrom psycopg2. extensions import AsIsclass Migration(migrations. Migration):  # . . .   operations = [    migrations. RunSQL(      [( CREATE INDEX catalogue_product_attrs_gin ON %s USING gin          (attributes jsonb_path_ops); , [AsIs(Product. _meta. db_table)])],      [('DROP INDEX catalogue_product_attrs_gin;', None)],    )  ]Здесь catalogue_product_attrs_gin - это название индекса (выбираем сами), attributes - название JSON поля, Product - модель товара. Создаем индекс jsonb_path_ops, т. к. в основном нам будет интересна только операция contains. Конструкция AsIs нужна, чтобы соответствующий параметр %s не оборачивался в одиночные кавычки. А btree индекс нам не нужен, поэтому не будем добавлять db_index=True: class Product(models. Model):  . . .   attributes = JSONField()Тестовые данные: Я сгенерил 1 000 000 товаров 4-х разных категорий, по 250 000 в каждой. У каждой категории товаров - свои атрибуты, от 4-х до 7. Некоторые значения - скалярные величины (material у футболок), некоторые - списки (sizes у футболок). Запросы по индексам:    Получить все футболки размера ‘M’ и ‘L’ (у товара есть оба размера):   &gt;&gt;&gt; Product. objects. filter(category=tshirt, attributes__contains={'sizes': ['M', 'L']})    Соответсвующий SQL (я заменил перечисление всех полей на * для краткости):   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 5   AND  catalogue_simple_product .  attributes  @&gt; '{ sizes : [ M ,  L ]}');    Без GIN индекса для поля attributes запрос выполняется 292 ms, EXPLAIN ANALYSE.   Этот же запрос после создания GIN индекса для поля attributes - 250 ms, EXPLAIN ANALYSE.   В данном случае большого выйгрыша мы не получили (было 292ms, стало 250ms), но это потому что результат содержит много строк: 66412. Это называется “low selectivity”. Selectivity - отношение отфильтрованных записей к их общему числу. Если это отношение стремится к 1, говорят low selectivity, если к 0 - high selectivity. С помощью этого показателя можно оценить эффективность индекса. При low selectivity индекс не принесет особой пользы.     Получить все футболки с размерами ‘M’ и ‘L’, белого и желтого цветов, с надписей (model = poet):   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'sizes': ['M', 'L'], 'colors': ['white', 'yellow'],   'model': 'poet'})    Соответствующий SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 5   AND  catalogue_simple_product .  attributes  @&gt; '{    colors : [ white ,  yellow ],    model :  poet ,    sizes : [ M ,  L ] }');    Без GIN индекса - 240 ms, EXPLAIN ANALYSE.   После создания GIN индекса - 49 ms, EXPLAIN ANALYSE.   Прирост заметен: 240 ms vs 49 ms. Запрос возращает 3737 строк, большая selectivity чем в предыдущем запросе.     Получить все ноутбуки с частотой процессора 2400 и диагональю экрана 12. 2   &gt;&gt;&gt; Product. objects. filter(category=notebook,   attributes__contains={'speed': 2400, 'screen': 12. 2})    Соответствующий SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 7   AND  catalogue_simple_product .  attributes  @&gt; '{ screen : 12. 2,  speed : 2400}');    Без GIN индекса - 222 ms, EXPLAIN ANALYSE.   После создания GIN индекса - 34 ms, EXPLAIN ANALYSE.   222ms vs 34ms. Запрос возращает 10389 строк.     Получить все футболки красного цвета, поло, с размерами ‘M’ или ‘L’   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'colors': ['red'], 'model': 'polo'},   attributes__sizes__has_any_keys=['M', 'L'])    Соответствующий SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 5   AND  catalogue_simple_product .  attributes  @&gt; '{ colors : [ red ],  model :  polo }'   AND  catalogue_simple_product .  attributes  -&gt; 'sizes' ?| ARRAY['M', 'L']);    Без GIN индекса - 253 ms, EXPLAIN ANALYSE.   После создания GIN индекса - 78 ms, EXPLAIN ANALYSE.   253 ms против 78 ms. Запрос возвращает 18428 строк. В этом запросе операция has_any_keys никак не может использовать индекс, т. к. мы объявили индекс вида jsonb_path_ops. Однако, запрос вида  attributes  -&gt; 'sizes' ?| ARRAY['M', 'L'] не будет использовать и jsonb_ops индекс, т. к. мы ищем не по ключам первого уровня, а по значениям списка. Если нам часто нужен такой запрос и у него будет большая selectivity, то можно создать индекс именно на этот ключ в JSON поле. Примерно так:   CREATE INDEX gin_sizes ON catalogue_simple_product USING gin ((attributes -&gt; 'sizes'));    Но в нашем случае этого делать не нужно, т. к. фильтр  attributes  -&gt; 'sizes' ?| ARRAY['M', 'L'] обладет низкой selectivity:   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'colors': ['red'], 'model': 'polo'},   attributes__sizes__has_any_keys=['M', 'L']). count() 18428 &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'colors': ['red'], 'model': 'polo'}). count() 25162    Т. е. по размеру мы отфильтровываем лишь ~25%.     Получить все ноутбуки с частотой процессора свыше 2000 и диагональю экрана больше 13   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__gt=2000,   attributes__screen__gt=13)    Соответствующий SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 7   AND  catalogue_simple_product .  attributes  -&gt; 'screen' &gt; '13'   AND  catalogue_simple_product .  attributes  -&gt; 'speed' &gt; '2000')    В данном случае GIN индекс нам не поможет. Если такие запросы используются часто, возможно имеет смысл создать btree индекс по конкретным ключам из JSON поля:   CREATE INDEX attrs_screen_speed ON catalogue_simple_product ((attributes -&gt; 'screen'), (attributes -&gt; 'speed'));    Запрос возвращает 10536 строк.   Без btree индекса attrs_screen_speed - 352 ms, EXPLAIN ANALYSE   После создания btree индекса attrs_screen_speed - 46 ms, EXPLAIN ANALYSE.     Получить все ноутбуки с частотой процессора 2200 или 2400   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__in=(2200, 2400))    Соответствующий SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 7   AND  catalogue_simple_product .  attributes  -&gt; 'speed' IN ('2200', '2400'))    Этот запрос не покрывается нашим GIN индексом. Время выполнения ~ 389 ms, EXPLAIN ANALYSE.   Попробуем переписать этот же запрос с использованием существующего GIN индекса   &gt;&gt;&gt; from django. db. models import Q &gt;&gt;&gt; Product. objects. filter(category=notebook). filter(Q(attributes__contains={'speed': 2200}) | Q(attributes__contains={'speed': 2400}))    SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 7   AND ( catalogue_simple_product .  attributes  @&gt; '{ speed : 2200}'     OR  catalogue_simple_product .  attributes  @&gt; '{ speed : 2400}'));    Здесь GIN индекс может быть использован, время выполнения ~ 337 ms EXPLAIN ANALYSE.   Особой разницы нет. Однако, посмотрим на selectivity данного запроса. В результате мы имеем 124 995 строк из 250 000 возможных для данной категории, т. е. имеем очень низкий selectivty.   Создадим 100 ноутбуков с частотой процессора 3200 и 100 ноутбуков с частотой 3500. Других ноутбуков с такими скоростями в БД нет. Посмотрим насколько эффективно будет использовать индекс в этом случае.   Запрос без GIN индекса:   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__in=(3200, 3500))    Получаем те же ~ 391 ms EXPLAIN ANALYSE.   Теперь запрос, в котором может быт использован GIN индекс:   &gt;&gt;&gt; Product. objects. filter(category=notebook). filter(Q(attributes__contains={'speed': 3200}) | Q(attributes__contains={'speed': 3500}))    В итоге получаем время выполнения лишь 0. 773 ms! EXPLAIN ANALYSE.  Резюме по индексам: Как мы видим, мы можем использовать один индекс GIN (jsonb_path_ops) в запросах по всем атрибутам, а не по одному конкретному ключу!Конечно, это не серебряная пуля, всегда нужно анализировать те данные, с которыми мы работаем и выбирать индексы исходя из нужных запросов. NoSQL database (MongoDB): Давайте посмотрим, как мы можем хранить те же данные и выполнять аналогичные запросы в MongoDB (v3. 0. 6). Для того, чтобы использовать один индекс при фильтрации по неизвестным заранее полям в MongoDB, нам нужно будет использовать немного другую структуру данных. Поле attributes у нас будет списком, а значения - вложенные документы с ключами name и value: &gt; db. catalogue_simple. find(). pretty(){   _id  : ObjectId( 560ab1970a0a88fe77d00f02 ),   category  :  tshirts ,   name  :  Silk tshirt ,   price  : 100,   attributes  : [    {       name  :  colors ,       value  :  red     },    {       name  :  colors ,       value  :  black     },    {       name  :  sizes ,       value  :  S     },    {       name  :  sizes ,       value  :  M     },    {       name  :  model ,       value  :  polo     },    {       name  :  material ,       value  :  silk     }  ]}{   _id  : ObjectId( 560ab1dd0a0a88fe77d00f03 ),   category  :  tshirts ,   name  :  Bamboo tshirt ,   price  : 120,   attributes  : [    {       name  :  colors ,       value  :  white     },    {       name  :  colors ,       value  :  yellow     },    {       name  :  sizes ,       value  :  M     },    {       name  :  sizes ,       value  :  L     },    {       name  :  sizes ,       value  :  XL     },    {       name  :  model ,       value  :  poet     },    {       name  :  material ,       value  :  bamboo     }  ]}{   _id  : ObjectId( 560ab2cb0a0a88fe77d00f04 ),   category  :  notebook ,   name  :  MacBook Pro ,   price  : 2000,   attributes  : [    {       name  :  brand ,       value  :  Apple     },    {       name  :  screen ,       value  : 15    },    {       name  :  speed ,       value  : 2200    },    {       name  :  hd ,       value  : 256    }  ]}{   _id  : ObjectId( 560ab2ec0a0a88fe77d00f05 ),   category  :  notebook ,   name  :  ATIV Book 9 ,   price  : 1200,   attributes  : [    {       name  :  brand ,       value  :  Samsung     },    {       name  :  screen ,       value  : 12. 2    },    {       name  :  speed ,       value  : 2400    },    {       name  :  hd ,       value  : 128    }  ]}Запросы (MongoDB):    Получить все футболки размера ‘M’ и ‘L’ (у товара есть оба размера):   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  sizes ,  value :  M }},   { $elemMatch : { name :  sizes ,  value :  L }} ]}, category: 'tshirts'}) { name  :  Bamboo tshirt , /* . . . */}      Получить все футболки с размерами ‘M’ и ‘L’, белого и желтого цветов, с надписей (model = poet):   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  sizes ,  value :  M }},   { $elemMatch : { name :  sizes ,  value :  L }},   { $elemMatch : { name :  colors ,  value :  white }},   { $elemMatch : { name :  colors ,  value :  yellow }},   { $elemMatch : { name :  model ,  value :  poet }} ]}, category: 'tshirts'}) { name  :  Bamboo tshirt , /* . . . */}      Получить все ноутбуки с частотой процессора 2400 и диагональю экрана 12. 2   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  speed ,  value : 2400}},   { $elemMatch : { name :  screen ,  value : 12. 2}} ]}, category: 'notebook'}) { name  :  ATIV Book 9 , /* . . . */}      Получить все футболки красного цвета, поло, с размерами ‘M’ или ‘L’   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  colors ,  value :  red }},   { $elemMatch : { name :  model ,  value :  polo }},   { $elemMatch : { name :  sizes ,  value : { $in : [ M ,  L ]}}} ]}, category: 'tshirts'}) { name  :  Silk tshirt , /* . . . */}      Получить все ноутбуки с частотой процессора свыше 2000 и диагональю экрана больше 13   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  speed ,  value : { $gt : 2000}}},   { $elemMatch : { name :  screen ,  value : { $gt : 13}}} ]}, category: 'notebook'}) { name  :  MacBook Pro , /* . . . */}      Получить все ноутбуки с частотой процессора 2200 или 2400   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  speed ,  value : { $in : [2200, 2400]}}}, ]}, category: 'notebook'}) { name  :  MacBook Pro , /* . . . */}, { name  :  ATIV Book 9 , /* . . . */}   Индексы (MongoDB): В данной орагнизации данных мы можем создать multikey index: &gt; db. catalogue_simple. ensureIndex({ attributes. name  : 1,  attributes. value  : 1})Для честного сравнения создадим индекс по полю category (django автоматически создает его для ForeignKey полей) &gt; db. catalogue_simple. ensureIndex({ category : 1})Еще следует учитывать, что mongodb будет использовать индекс только для первого фильтра оператора $all. Хоть в доках MongoDB говорится о index intersesection, однако похоже, что пересечение индексов не работает в данных запросах. Наглядный пример. Пусть у нас в базе только 1 товар с размером “XXXS”, и очень много товаров с размером “M”. Мы хотим найти товары, у которых есть оба размера “XXXS” и “M”. Можем сделать такой запрос: &gt; db. catalogue_simple. find({attributes: {$all: [  { $elemMatch : { name :  sizes ,  value :  M }},  { $elemMatch : { name :  sizes ,  value :  XXXS }},]}, category: 'tshirts'})В этом случае MongoDB применит индекс к значению “M”. В итоге будет просканированно много сущностей, затрачено много времени, а в результате только 1 документ:  nReturned  : 1, executionTimeMillis  : 1902, totalKeysExamined  : 249934, totalDocsExamined  : 249934,А если сделаем запрос так (“XXXS” на первом месте): &gt; db. catalogue_simple. find({attributes: {$all: [  { $elemMatch : { name :  sizes ,  value :  XXXS }},  { $elemMatch : { name :  sizes ,  value :  M }},]}, category: 'tshirts'})то результат получим сразу:  nReturned  : 1, executionTimeMillis  : 0, totalKeysExamined  : 1, totalDocsExamined  : 1,Мораль такая, что на первое место нужно ставить поле с наибольшим selectivity. Если у нас конечно есть такая инфорамция. Тестовые данные (MongoDB): Тестовые данные полностью идентичны использованным в PostgreSQL (за исключением структуры): 4 категории, по 250 000 товаров в каждой, всего 1 000 000. Запросы по индексам (MongoDB):    Получить все футболки размера ‘M’ и ‘L’ (у товара есть оба размера):   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  sizes ,  value :  M }},   { $elemMatch : { name :  sizes ,  value :  L }} ]}, category: 'tshirts'})    Без индекса по attributes запрос выполняется 706 ms, используется индекс category.   Вывод . explain('executionStats'):    winningPlan  : {   // . . .    indexName  :  category_1 , }  executionStats  : {    nReturned  : 66412,    executionTimeMillis  : 706,    totalKeysExamined  : 250001,    totalDocsExamined  : 250001, }    С индексом по attributes ничего не меняется, т. к. optimizer не считает этот индекс лучше чем category (видимо малая selectivity).     Получить все футболки с размерами ‘M’ и ‘L’, белого и желтого цветов, с надписей (model = poet):   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  sizes ,  value :  M }},   { $elemMatch : { name :  sizes ,  value :  L }},   { $elemMatch : { name :  colors ,  value :  white }},   { $elemMatch : { name :  colors ,  value :  yellow }},   { $elemMatch : { name :  model ,  value :  poet }}, ]}, category: 'tshirts'})    Ситуация аналогична первому примеру.   Она может поменяться, если на первое место мы поставим значение с большой selectivity. В тестовых данных цветов больше, чем размеров. Поэтому, если поставить цвет на первое место, то будет использоваться индекс attributes:   &gt; db. catalogue_simple. find({attributes: {$all: [     { $elemMatch : { name :  colors ,  value :  white }},     { $elemMatch : { name :  sizes ,  value :  M }},     { $elemMatch : { name :  sizes ,  value :  L }},     { $elemMatch : { name :  colors ,  value :  yellow }},     { $elemMatch : { name :  model ,  value :  poet }},   ]}, category: 'tshirts'}). explain('executionStats')    Вывод:    winningPlan  : {   // . . .    indexName  :  attributes. name_1_attributes. value_1 , }  executionStats  : {    nReturned  : 3737,    executionTimeMillis  : 658,    totalKeysExamined  : 124902,    totalDocsExamined  : 124902, }    Время запроса - 658 ms.     Получить все ноутбуки с частотой процессора 2400 и диагональю экрана 12. 2   Аналогично пунктам 1 и 2.     Получить все футболки красного цвета, поло, с размерами ‘M’ или ‘L’   Аналогично пунктам 1 и 2.     Получить все ноутбуки с частотой процессора свыше 2000 и диагональю экрана больше 13   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  speed ,  value : { $gt : 2000}}},   { $elemMatch : { name :  screen ,  value : { $gt : 13}}} ]}, category: 'notebook'}). explain('executionStats')    Как мы помним, в PostgreSQL мы не могли использовать GIN индекс для этого запроса. Однако в случае структуры данных, которую мы используем в MongoDB, существующий индекс будет работать. Вопрос лишь в том, какой процент данных этот фильтр отсекает. Тут selectivity оказалась относительно хорошей:    winningPlan  : {   // . . .    indexName  :  attributes. name_1_attributes. value_1 , }  executionStats  : {    nReturned  : 10536,    executionTimeMillis  : 160,    totalKeysExamined  : 62472,    totalDocsExamined  : 62472, }      Получить все ноутбуки с частотой процессора 2200 или 2400   Аналогично предыдущим пунктам.  Итоги: В PostgreSQL 9. 4 появился тип jsonb, который можно эффективно использовать в запросах. Причем создав лишь один индекс мы можем фильтровать по всем ключам из JSON. Не все операции будут доступны (например больше/меньше, для них придется создавать отдельный индекс на каждый JSON ключ), но тем не менее этого достаточно для широкого круга задач. И начиная с Django 1. 9 эти функции доступны из коробки. В MongoDB аналога оператору @&gt; нет. Можно изменить структуру данных и делать аналогичные запросы по группе полей, используя один индекс. Но это менее эффективно, чем в PostgreSQL. Ведь индекс будет применяться только к первому ключу, а не ко всем. Зато MongoDB поддерживает гораздо больше операций при фильтрации по одному любому ключу, используя один индекс. Пока мне очень нравится JSON в PostgreSQL, оно здорово облегчает решение многих задач. При этом мы сохраняем возможности SQL: транзакции и join’ы, которых нет в MongoDB. И теперь это поддерживается Django ORM. Полезные ссылки:  Django JSONField docs PostgreSQL JSON type docs PostgreSQL JSON functions and operations docs Christophe Pettus - PostgreSQL Proficiency for Python People - PyCon 2015 (video) PostgreSQL and JSON: 2015. Christophe Pettus. PGConf US 2015 (slides) Asya Kamsky, Yandex 2014 MongoDB meetup"
    }, {
    "id": 4,
    "url": "https://st4lk.github.io/blog/2015/07/18/oauth-and-django-rest-framework.html",
    "title": "OAuth и django rest framework",
    "body": "2015/07/18 - Star Тема избитая, но мне не удалось найти готового решения, которое полностью бы меня устроило. Поэтому пишу сам :). Итак, у нас есть “одностраничный” веб сайт, который общается с бекендом посредством REST API.  Клиентская часть может быть написана с помощью ember, angularjs или чего-то подобного. Бекенд - django rest framework (DRF). И есть тривиальная задача - добавить вход через социальные сети (OAuth протокол). Как бы это выглядело в случае обычного (олд-скульного) сайта? Пользователь нажимает логин, открывается страница социальной сети (Oauth провайдера). Человек подтверждает доступ, соц. сеть делает редирект обратно на наш сайт, передавая определенный код. Мы узнаем код из url’а и делаем логин. Для такого процесса есть несколько готовых django библиотек, мне больше всего нравится python-social-auth. В случае одностраничного сайта можно впринципе сделать так же. Однако, часто разработка фронтенда и бекенда разделена. Более того, бывает что апи работает на другом поддомене, таким образом бекенд не может напрямую обработать редирект. В итоге следующая схема получается наиболее оптимальной. Пользователь нажимает логин, открывается попап окно с подтверждением в соц. сети. После подтверждения попап передает параметры родительскому окну (фронтенд приложению), который в свою очередь отправляет запрос на бекенд для конечного логина. Т. о. бекенд разработчику нужно реализовать API ресурс, на вход которого передаются параметры от OAuth провайдера, а на выходе - данные пользователя + сессия (например id сессии в куках или токен). Этот ресурс будет вызываться фронтендом. Вопрос, какие данные лучше передавать в этот ресурс? Возьмем OAuth 2. 0. Тут два варианта - либо request token, либо access token. В первом случае сервер должен будет сам обменять request token на access token. Во втором - этого делать не надо (фронтенд уже получил access token). Казалось бы, последний вариант проще. Однако у этого способа есть недостатки. Во-первых, access token, полученный фронтенд’ом, имеет очень маленький срок жизни (несколько часов). Мы могли бы сохранить access token в базе данных и при определенных условиях обратиться к социальной сети позднее (написать что-то на стене пользователя). С коротким сроком жизни токена это делать неудобно. Во-вторых, access token будет передан фронтендом на наш апи сервер. Возникает потенциальная угроза безопасности. Если наш апи работает не по HTTPS, то access token можно легко перехватить. Этого токена достаточно, чтобы делать валидные запросы. Поискав готовые решения для DRF, я нашел django-rest-auth. Он предлагает готовый ресурс, который получает access token. А вот готового ресурса, который получал бы request token и все остальное делал бы сам, нету. Так же этот пакет основан на библиотеке django-allauth, которая по-моему мнению не очень удобна в части OAuth регистрации. Исходя из всего вышесказанного, я решил написать свою тулзу, которая связывала бы django-rest-framework, python-social-auth и реализовывала ресурс для логина по request token’у. Вот она: django-rest-social-auth. Подробности по использованию можно почитать в readme. Это довольно маленький, но удобный пакет. Вся кастомизация, доступная в python-social-auth, доступна и здесь. Живой пример - сайт woobie. ru, там используется именно эта библиотека. "
    }, {
    "id": 5,
    "url": "https://st4lk.github.io/blog/2015/06/05/tornado-and-pgettext.html",
    "title": "Tornado and pgettext",
    "body": "2015/06/05 -  Недавно (26 мая 2015 года) вышел релиз tornado 4. 2. В него вошли разные дополнения, пожалуй основные из них - модули tornado. locks и tornado. queues. Они перекочевали из пакета Toro, подробное описание процесса от Jesse Jiryu Davis в его блоге. Здесь же хочу рассказать о другой маленькой функции, которая была добавлена с моей помощью - pgettext. Она может быть полезна, когда вы создаете перевод для неоднозначных строк. Допустим есть слово “bat”, которое нужно вывести либо на английском, либо на русском, в зависимости от языка пользователя. Для этого можно воспользоваться соответствующими функциями перевода. Например так (html шаблон): &lt;div&gt;{{_( Bat )}}&lt;/div&gt;Далее мы с помощью утилиты xgettext создадим файл перевода, в котором будет что-то такое (подробно про процесс i18n можно почитать тут) msgid  Bat msgstr   Теперь на месте пустой строки нам нужно вставить перевод. Но что означает слово “Bat”? В английском языке это слово может означать “летучая мышь”, а может “дубина”, в зависимости от контекста. Переводчику будет очень трудно понять, что же имелось в виду. Вот где пригодится функция pgettext, ей в качестве первого аргумента передается контекст фразы: &lt;div&gt;{{ pgettext( mammal ,  Bat ) }}&lt;/div&gt;При генерации перевода нужно дополнительно указать такие опции для утилиты xgettext: --keyword=pgettext:1c,2 --keyword=pgettext:1c,2,3После этого файл перевода будет выглядеть так: msgctxt  mammal msgid  Bat msgstr   Переводчик поймет, что в данном случае имелась в виду именно летучая мышь, поэтому перевод однозначен: msgctxt  mammal msgid  Bat msgstr  Летучая мышь Множественные формы так же поддерживаются: &lt;div&gt;{{ pgettext( mammal ,  Bat ,  Bats , 2) }}&lt;/div&gt;В python коде (не в шаблоне) это будет выглядеть так: class HomeHandler(tornado. web. RequestHandler):  def get(self):    self. render( home. html , text=self. locale. pgettext( mammal ,  Bat ,  Bats , 2))Сам перевод множественных значений: msgctxt  mammal msgid  Bat msgid_plural  Bats msgstr[0]  Летучая мышь msgstr[1]  Летучие мыши msgstr[2]  Летучих мышей "
    }, {
    "id": 6,
    "url": "https://st4lk.github.io/blog/2015/05/16/oauth-step-step.html",
    "title": "OAuth step by step",
    "body": "2015/05/16 -   Gist OAuth протокол бывает двух версий: 1. 0 и 2. 0. Большинство сервисов сегодня используют версию 2. 0, вероятно потому чтоее проще реализовать. Так же версию 2. 0 можно относительно безопасно использовать вstandalone-приложениях (те, которые без сервера). Для понимания протоколов очень полезно взглянуть на их реализацию. Тут я приведу несколько скриптов, которые общаются с OAuth-провайдерами разных версий. Т. е. все скрипты реализуют функционал клиента (не сервера). Используются только стандартные python библиотеки. Вот почему глядя на них лучше понимаешь сам протокол OAuth - все перед глазами и все более-менее знакомое. Конечно, для реальной работы нужно использовать только готовые и проверенные временем пакеты. Эти скрипты только для понимания процесса. Разбираться с готовыми библиотеками порой бывает сложно, они разбиты на много модулей, могут использоваться разные сторонние пакеты, и в итоге общая картина ускользает из виду. Для начала немного теории. Наверняка вы знаете, что есть два понятия - аутентификация (authentication) и авторизация (authorization). Они вроде бы об одном и том же, но все-таки немного о разном. На всякий случай напомню:  аутентификация - это процесс подтверждения подлинности.  Т. е. нам нужно просто узнать, что данный человек действительно владеет аккаунтом google с таким id и нам этого достаточно.  Просто залогинить пользователя, не получая никаких прав на какие-либо действия с аккаунтом google’а.  Этим занимается например протокол OpenID (хотя сейчас google предлагает другой способ, OpenID - deprecated).  авторизация - процесс предоставления полномочий что-то делать с аккаунтом.  Авторизация уже включает в себя аутентификацию, но дает дополнительные возможности.  Например, не просто подтвердить, что пользователь действительно является владельцем аккаунта с определенным id, но еще и узнать его email. А возможно и написать что-то на его стене.  Вот это предоставляет протокол OAuth. Чтобы легче запомнить я использую слово “автор”. Если есть “автор” - значит речь идет о правах (авторстве). Если нет - значит просто проверка подлинности. OAuth 1. 0: Спецификация: http://tools. ietf. org/html/rfc5849 Самое главное, что нужно запомнить - в OAuth 1. 0 все запросы подписываются секретным ключом. Секретный ключ нужно хранить только в безопасном месте, единственное такое место - это сервер. Благодаря этому протокол обеспечивает полную безопасность, даже если не используется https. Безопасность в том плане, что даже подслушав запрос злоумышленник не сможет сделать другой валидный запрос. Подслушать сами передаваемые данные он конечно сможет, чтобы их скрыть нужен https. OAuth 1. 0 less-legged (2-legged, 1-legged, 0-legged): Это модификация протокола OAuth 1. 0, в котором пользователь никак не зайдествован. Формально это уже не OAuth протокол, т. к. в спецификции такая последовательность не описана. Просто используются те же приемы, поэтому и называют так же. В этом случае клиентское приложение является пользователем, оно может запрашивать либо общедоступные ресурсы, либо ресурсы, доступные самому клиентскому приложению (даже приватные). OAuth 2. 0 с участием сервера: Спецификация: http://tools. ietf. org/html/rfc6749 Интересно, что в заглавии спецификации OAuth 2. 0 назван фреймворком. В то время как в заглавии спецификации OAuth 1. 0 назван протоколом. Для обеспечения полной безопасности OAuth 2. 0 необходимо отправлять запросы по https (https должен обеспечивать service provider, например facebook). Получив access_token уже не нужно подписывать запросы секретным ключом. Т. е. если кто-то подслушает запрос и увидит access_token, то он сможет сделать валидный запрос. Вот зачем нужен https. Кроме того при получении access_token, секрет передается по http в открытом виде. У полученного access_token всегда есть ограниченное время жизни. В связи с этими особенностями (и некоторыми другими), один из проектировщиков протокола OAuth 1. 0 даже отказался от участия в разработке OAuth 2. 0, ведь последний очень легко реализовать неправильно и в результате безопасность не будет гарантирована. Подробности можно почитать по ссылке. Последовательность шагов для получения access_token по OAuth 2. 0, которая включает сервер. Сервер для получения access_token отправляет секретный код. Обратите внимание, не используется ни одна крипто-библиотека. OAuth 2. 0 без участия сервера: С OAuth 2. 0 можно работать и без сервера по упрощенной схеме. В этом случае мы тоже получаем access_token, но для его получения не нужно знать секрет приложения! Обычно время жизни у access_token, полученного таким способом, маленькое (1-2 часа), в то время как время жизни, полученного с участием сервера больше (может быть несколько десятков дней). "
    }, {
    "id": 7,
    "url": "https://st4lk.github.io/blog/2015/04/30/base-python-tips-tricks.html",
    "title": "Python tips & tricks",
    "body": "2015/04/30 -  Недавно прочитал книгу Марка Лутца “Learning Python”, 5-ое издание. Привожу список самых интересных фишек (по моему мнению) оттуда, что-то вроде конспекта.    генерация set’a:    {x for x in [1,2]} set(x for x in [1,2]) assert set(x for x in [1,2]) == {x for x in [1,2]}      генерация dict’а:    {x:x**2 for x in [1,2]} dict((x, x**2) for x in [1,2]) assert {x:x**2 for x in [1,2]} == dict((x, x**2) for x in [1,2])      деление целых чисел   В python 3 деление целых чисел возвращает float    &gt;&gt;&gt; 1 / 2 0. 5 &gt;&gt;&gt; - 1 / 2 -0. 5    В python 2 деление - округление в меньшую сторону, это не truncate    &gt;&gt;&gt; 1 / 2  # 0. 5 floor округление -&gt; 0 0 &gt;&gt;&gt; - 1 / 2 # -0. 5 floor округление -&gt; -1 (не 0) -1    В python 2 и 3 целочисленное деление    &gt;&gt;&gt; 1 // 2 0 &gt;&gt;&gt; - 1 // 2 -1 &gt;&gt;&gt; 13 // 2. 0 6. 0      is - проверяет, что переменные указывают на один и тот же адрес, == проверяет, что объекты имеют одинаковые значения     python 3: [1, 'spam']. sort() возбуждает исключение (разные типы)     python 3: dict(). keys() возвращает итератор (view объект, зависимый от dict). Это set-like объект, к нему можно применять set операции (union и пр. )    &gt;&gt;&gt; dict(a=1, b=2). keys() dict_keys(['b', 'a']) &gt;&gt;&gt; dict(a=1, b=2). keys() | {'c', 'd'} {'b', 'd', 'a', 'c'}      frozenset - immutable set, он является hashable, можно использовать например как ключ в dict    &gt;&gt;&gt; fz = frozenset([1,2]) &gt;&gt;&gt; fz. add(3) AttributeError: 'frozenset' object has no attribute 'add' &gt;&gt;&gt; {fz: 5} {frozenset([1, 2]): 5}      list поддерживает операторы сравнения: ==, &lt;, &gt;, &lt;=, &gt;=. Сравнение аналогично сравнению срок. Для py3 все объекты должны быть одного типа    &gt;&gt;&gt; [1, 2] == [1, 2] True &gt;&gt;&gt; [2, 2] &gt; [1, 2] True &gt;&gt;&gt; [1] &gt; ['sh'] # python2 False &gt;&gt;&gt; [1] &gt; ['sh'] # python3 TypeError: unorderable types: int() &gt; str()      сравнение dict’ов   python 2 and 3    &gt;&gt;&gt; dict(a=1) == dict(a=1) True    python 2 only    &gt;&gt;&gt; dict(a=3) &gt; dict(a=2) True &gt;&gt;&gt; dict(a=3) &gt; dict(a=2, b=1) False      нельзя list + string, list + tuple, однако можно list += string    &gt;&gt;&gt; L = [] &gt;&gt;&gt; L + 'spam' TypeError: can only concatenate list (not  str ) to list &gt;&gt;&gt; L = [] &gt;&gt;&gt; L += 'spam' &gt;&gt;&gt; L ['s', 'p', 'a', 'm']      L += a is faster that L = L + a.     L += [1,2] is in place modification! (не создается новый список)    &gt;&gt;&gt; L = [] &gt;&gt;&gt; id(L) 4368997048 &gt;&gt;&gt; L += [1,2] &gt;&gt;&gt; id(L) 4368997048 &gt;&gt;&gt; L = L + [1,2] &gt;&gt;&gt; id(L) 4368996976      ‘spam’[0][0][0] можно до бесконечности, каждый раз будет возвращатся односимвольная строка ‘s’     распаковка аргументов в python 3 при присваивании    &gt;&gt;&gt; a, *b = 'spam' &gt;&gt;&gt; a 's' &gt;&gt;&gt; b ['p', 'a', 'm'] &gt;&gt;&gt; *a, b = 'spam' &gt;&gt;&gt; a ['s', 'p', 'a'] &gt;&gt;&gt; b 'm' &gt;&gt;&gt; a, *b, c = 'spam' &gt;&gt;&gt; a 's' &gt;&gt;&gt; b ['p', 'a'] &gt;&gt;&gt; c 'm'      python 2: True = 0, но не в python 3   python 2    &gt;&gt;&gt; True = 0 &gt;&gt;&gt; True 0    python 3    &gt;&gt;&gt; True = 0 SyntaxError: can't assign to keyword      sys. stdout = open(‘temp. txt’, ‘w’) - все print’ы будут идти в файл temp. txt     and, or возвращают объект, а не True/False     while has else     python 3: . . . все равно что pass     reversed works with lists, not generator    &gt;&gt;&gt; reversed([1,2,3]) &lt;list_reverseiterator object at 0x10127c550&gt; &gt;&gt;&gt; reversed((x for x in [1,2,3])) TypeError: argument to reversed() must be a sequence      zip итерирует до самой маленькой последовательности    &gt;&gt;&gt; [x for x in zip([1,2,3], [4,5])] [(1, 4), (2, 5)]      python 2: map(None, s1, s2) тоже самое, что zip, но добавялет None для элементов из более длинной последовательности   python 2    &gt;&gt;&gt; map(None, [1,2,3], [4,5]) [(1, 4), (2, 5), (3, None)] &gt;&gt;&gt; map(None, [1,2], [4,5,6]) [(1, 4), (2, 5), (None, 6)]    python 3    &gt;&gt;&gt; list(map(None, [1,2,3], [4,5])) TypeError: 'NoneType' object is not callable      map can take more than one iterators (похоже на zip)   python 2    &gt;&gt;&gt; map(lambda x, y: (x, y), [1,2], [3,4]) [(1, 3), (2, 4)] &gt;&gt;&gt; map(lambda x, y: (x, y), [1,2], [3,4,5]) [(1, 3), (2, 4), (None, 5)]    python 3    &gt;&gt;&gt; list(map(lambda x, y: (x, y), [1,2], [3,4])) [(1, 3), (2, 4)] &gt;&gt;&gt; list(map(lambda x, y: (x, y), [1,2], [3,4,5])) [(1, 3), (2, 4)]      nested list comprehensions    &gt;&gt;&gt; [x+y for x in 'abc' for y in 'lmn'] ['al', 'am', 'an', 'bl', 'bm', 'bn', 'cl', 'cm', 'cn'] # flat list of lists &gt;&gt;&gt; csv = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] &gt;&gt;&gt; [col for row in csv for col in row] [1, 2, 3, 4, 5, 6, 7, 8, 9]      sorted возвращает список (не генератор) в py2 и py3    &gt;&gt;&gt; sorted(x for x in [2,1,3]) [1, 2, 3]      *args понимает любой итератор, не обязательно list     unzip: zip(*zip(a,b))    &gt;&gt;&gt; zip(*zip([1,2],[3,4])) [(1, 2), (3, 4)]      py3: map возвращает генератор, пройти по нему можно только раз    &gt;&gt;&gt; m = map(lambda x: x, [1,2,3]) &gt;&gt;&gt; [x for x in m] [1, 2, 3] &gt;&gt;&gt; [x for x in m] []      py3: хоть range и генератор (это хитрый генератор), он поддерживает len() и доступ по индексам    &gt;&gt;&gt; r = range(10) &gt;&gt;&gt; r range(0, 10) &gt;&gt;&gt; len(r) 10 &gt;&gt;&gt; r[3] 3      generator allows only single scan     циклические импорты работают! Но только с import, без from     у try есть else, который вызовится, если exception не случилось     with similar to finally     except (name1, name2) - orders from top to bottom, from left to right     except Exception: vs except: - первое не перехватывает системные исключения (KeyboardInterrupt, SystemExit, GeneratorExit например)     set(). remove(x) - удаляет x или KeyError, set(). discard(x) - удаляет x или ничего     py3. 3+ accept u””, U”” для обратной совместимости с py2     default encoding is in sys module sys. getdefaultencoding()   python 2    &gt;&gt;&gt; sys. getdefaultencoding() 'ascii'    python 3    &gt;&gt;&gt; sys. getdefaultencoding() 'utf-8'      [c for c in sorted([1,2,3], key=lambda c: -c)] - тут переменная c конфликтовать не будет     в py2 переменная внутри comprehension может изменять внешние переменные а также доступна после, в py3 - нет.   python 2    &gt;&gt;&gt; x = 1 &gt;&gt;&gt; [x for x in range(3)] [0, 1, 2] &gt;&gt;&gt; x 2 # creates new var &gt;&gt;&gt; [y for y in range(3)] [0, 1, 2] &gt;&gt;&gt; y 2    python 3    &gt;&gt;&gt; x = 1 &gt;&gt;&gt; [x for x in range(3)] &gt;&gt;&gt; x 1 # no new var &gt;&gt;&gt; [y for y in range(3)] [0, 1, 2] &gt;&gt;&gt; y NameError: name 'y' is not defined      py3 имеет инструкцию nonlocal. Используется для ссылки на имя во внешнем def блоке (в py2 к такой переменной нельзя обратиться)    def f():   x = 2 # local for f   def g():     nonlocal x # python3 only     x = 3 # local for g   g()   print(x) &gt;&gt;&gt; f() # python3 only 3 &gt;&gt;&gt; f() # with commented nonlocal 2      LEGB rule (local, enclosing, global, builtin) или LNGB (N=nonlocal) - порядок поиска переменной в python     py3 переменная исключения as name удаляется после выполнения блока (даже если переменная была объявлена до try)   python 2    &gt;&gt;&gt; x = 1 &gt;&gt;&gt; try: . . .   1/0 . . . except Exception as x: . . .   pass &gt;&gt;&gt; x ZeroDivisionError('integer division or modulo by zero',)    python 3    &gt;&gt;&gt; x = 1 &gt;&gt;&gt; try: . . .   1/0 . . . except Exception as x: . . .   pass &gt;&gt;&gt; x NameError: name 'x' is not defined      переопределить builtin и отменить переопределение    &gt;&gt;&gt; open = 99 &gt;&gt;&gt; open 99 &gt;&gt;&gt; del open &gt;&gt;&gt; open &lt;built-in function open&gt;      py2 fun: __builtins__. True = False     lambda может принимать аргументы по умолчанию     nonlocal можно заменить mutable объектом или аттрибутом функции    def f():   x = [1]   def g():     print x[0]     x. append(2)   g()   print x &gt;&gt;&gt; f() 1 [1, 2] def f():   x = 1   def g():     print g. x     g. x = 2   g. x = x   g()   print g. x &gt;&gt;&gt; f() 1 2      py3 keyword only arguments    def f(*args, name):   print( args , args)   print( name , name) &gt;&gt;&gt; f(1, 2) TypeError: f() missing 1 required keyword-only argument: 'name' &gt;&gt;&gt; f(1, 2, name=3) args (1, 2) name 3 def f(*args, name=3):   print( args , args)   print( name , name) &gt;&gt;&gt; f(1, 2) args (1, 2) name 3      py3 есть распаковка при присваивании, она возвращает list, а распаковка при вызове функции возвращает tuple   python 2 and 3    def f(a, *b):   print(b) &gt;&gt;&gt; f(1, *[2, 3]) (2, 3)    python 3    &gt;&gt;&gt; a, *b = [1, 2, 3] &gt;&gt;&gt; print(b) [2, 3] &gt;&gt;&gt; a, *b = (1, 2, 3) &gt;&gt;&gt; print(b) [2, 3]      добавить список в начало существующего: L[:0] = [1, 2, 3]     посмотреть и задать максимальный уровень рекурсии    &gt;&gt;&gt; sys. getrecursionlimit() # 1000 &gt;&gt;&gt; sys. setrecursionlimit(10000) &gt;&gt;&gt; help(sys. setrecursionlimit)      аргументы функций    &gt;&gt;&gt; def f(a): . . .   b = 1 . . .  &gt;&gt;&gt; f. __name__ 'f' &gt;&gt;&gt; f. __code__. co_varnames ('a', 'b') &gt;&gt;&gt; f. __code__. co_argcount 1      py3 к аргументам функции можно добавить аннотации. Эти данные доступны в func. __annotations__. Автоматически ничего с этими аннотациями не происходит, но с ними можно работать вручную по ситуации (например, для проверки типа или диапазона с помощью своего декоратора)    &gt;&gt;&gt; def func(a: 'spam', b: (1, 10), c: float): . . .   return a + b + c &gt;&gt;&gt; func. __annotations__ {'b': (1, 10), 'c': &lt;class 'float'&gt;, 'a': 'spam'} # default values &gt;&gt;&gt; def func(a: 'spam'=4, b: (1, 10)=5, c: float=0. 1): . . .   return a + b + c      внутри lambda нельзя присвоить, но можно setattr, __dict__     operator module in std lib    import operator as op reduce(op. add, [2, 4, 6]) # same as reduce(lambda x, y: x+y, [2, 4, 6])      KISS: Keep It Simple [Sir/Stupid]     comprehension vs map в общем случае (лучше проверить на вашей системе)   map(lambda x: x . . ) slower than [x for x . . ]   [ord(x) for x . . ] slower than map(ord for x . . )   map(lambda x: L. append(x+10), range(10)) even slower than for x in range(10): L. append(x+10)     распаковка в lambda отличатся для py2 и py3   python 2    &gt;&gt;&gt; map(lambda (a, b, c): a, [(0,1,2), (3,4,5)]) [0, 3]    python 3    &gt;&gt;&gt; list(map(lambda (a, b, c): a, [(0,1,2), (3,4,5)])) SyntaxError: invalid syntax &gt;&gt;&gt; list(map(lambda a, b, c: a, [(0,1,2), (3,4,5)])) TypeError: &lt;lambda&gt;() missing 2 required positional arguments: 'b' and 'c' &gt;&gt;&gt; list(map(lambda row: row[0], [(0,1,2), (3,4,5)])) [0, 3]      многие встроенные функции могут принимать генераторы, тогда не нужны дополнительные скобки    &gt;&gt;&gt;   . join(str(x) for x in [1, 2]) '12' &gt;&gt;&gt; sorted(str(x) for x in [1, 2]) ['1', '2']    but for args () is needed    &gt;&gt;&gt; sorted(str(x) for x in [1, 2], reverse=True) SyntaxError: Generator expression must be parenthesized if not sole argument &gt;&gt;&gt; sorted((str(x) for x in [1, 2]), reverse=True) ['2', '1']      py3: yield from iterator (приведенные ниже функции идентичны)    def f():   for i in range(5):     yield i def g():   yield from range(5)      поместить первый элемент в конец списка    L = L[1:] + L[:1]      zip одного списка    &gt;&gt;&gt; zip([1,2,3]) [(1,), (2,), (3,)]      map и zip похожи    map(lambda x,y: (x,y), S1, S2) == zip(S1, S2)      python -m script_name - запускает модуль (модуль - это файл . py, т. е. считай скрипт), который ищется в текущих путях поиска. Модуль может лежать где-нибудь в site-packages, а запустится от main (__name__ == '__main__'). Если это package (директория с __init__. py), то запустится файл __main__. py. Если такого нет, то ошибка. Некоторые модули умные и берут аргументы из командной строки, например timeit: python -m timeit ' - . join(str(n) for n in range(100))'     прямой возможности использовать одноименную переменную в одной функции: и глобальную и локальную нет. Можно только играть c __main__. my_global_var    # OK X = 99 def f():   print(X) &gt;&gt;&gt; f() 99 # ERROR def f():   print(X) # &lt;- error   X = 99 &gt;&gt;&gt; f() UnboundLocalError: local variable 'X' referenced before assignment # global everywhere def f():   global X   print(X)   X = 88 # hack with main def f():   import __main__   print(__main__. X)   X = 88      скорость вычисления корня числа    math. sqrt(x) # fastest x ** . 5 # fast pow(x, . 5) # slow      py3. 2+ создает папку __pycache__, чтобы сохранять разные байткоды для разных версий python’а и не пересоздавать их впоследствии. В корне уже нет *. pyc.     . pyc для вызывающегося скрипта (__name__ = '__main__') не создается, только для import     порядок поиска при импорте (можно посмотреть в sys. path):      home of program (+ in some versions current dir, from where program is launched, i. e. current dir)   PYTHONPATH   std lib dir   content of any . pth file (if exists)   site-packages dir      sys. path можно изменять в runtime, это затронет всю программу     python -O создает слегка опимизированный байткод . pyo вместо . pyc, он на ~5% быстрее. Также этот флаг убирает все assert’ы из кода. А так же влияет на переменную __debug__    # main. py print __debug__ assert True == False # python main. py True AssertionError # python -O main. py False      в py2 в функции можно сделать from some_module import *, но будет warning. В py3 - error    # python 2 def f():   from urllib import *   print('after import') &gt;&gt;&gt; f() SyntaxWarning: import * only allowed at module level after import # python 3 &gt;&gt;&gt; f() SyntaxError: import * only allowed at module level      reload не обновляет объекты, загруженные с помощью from: from x import y. y не обновится после reload(x)     reload не обновляет c-шные модули     py3: в package текущей папки пакета нету в sys. path. Если модуль в пакете хочет импортировать другой модуль из этого же пакета, надо использовать относительный импорт: from . import smth. Однако, если модуль запускается как __main__, то есть.     py2: from __future__ import absolute_import делает поведение import в py2 таким же, как в py3. Это позволит импортировать модуль string из стандартной библиотеки в данном случае довольно просто:    mypkg ├── __init__. py ├── main. py # import string from std here? └── string. py      относительный импорт запрещен вне пакета:    # test. py from . import a # python 2 python test. py ValueError: Attempted relative import in non-package # python 3 python test. py SystemError: Parent module '' not loaded, cannot perform relative import      минусы относительного импорта:      модуль, в котором используются относительные импорты нельзя использовать как скрипт (__main__). Решение: использовать в модуле абсолютный импорт с именем пакета в начале.    как следствие предыдущего пункта, нельзя запустить тесты, которые запускаются при запуске модуля как скрипта      в py3. 3+ есть namespace packages. Это такие пакеты, в которых нет __init__. py. Два (или более) namespace package с одним и тем же именем могут лежать в разных директориях из sys. path. При этом модули пакетов собираются под этим именем. Если у модулей одинаковые имена - берется тот, который найден раньше в порядке sys. path. namespace пакет всегда имеет меньший приоритет над обычным пакетом (с __init__. py). Как только где-то найден обычный пакет - используется он, все найденные namespace packages отметаются. namespace пакеты медленнее импортятся, чем обычные.    # collect modules in namespace package current_dir └── mypkg   └── mymod1. py site-packages └── mypkg   └── mymod2. py &gt;&gt;&gt; import mypkg. mymod1 &gt;&gt;&gt; import mypkg. mymod2 # redefine module in namespace package current_dir └── mypkg   └── mymod1. py   └── mymod2. py site-packages └── mypkg   └── mymod2. py &gt;&gt;&gt; import mypkg. mymod1 &gt;&gt;&gt; import mypkg. mymod2 # current_dir. mypkg. mymod2 # regular package is used current_dir └── mypkg   └── mymod1. py site-packages └── mypkg   └── mymod2. py another-packages └── mypkg   └── mymod1. py &gt;&gt;&gt; import sys &gt;&gt;&gt; sys. append('another-packages') &gt;&gt;&gt; import mypkg. mymod1 # another-packages. mypkg. mymod1 &gt;&gt;&gt; import mypkg. mymod2 ImportError: No module named 'mypkg. mymod2'      В py3 и в py2 new style classes (отнаследованные от object) магические методы при выполнении оператора ищутся в классе, минуя инстанс (__getattr__, __getattribute__ не вызываются). Но если явно вызвать магический метод - то вызывается от инстанса (__getattr__, __getattribute__ вызываются).    class A(object):   def __repr__(self):     return  class level repr    def normal_method(self):     return  class level normal method  def instance_repr():   return  instance level repr  def instance_normal_method():   return  instance level normal method  a = A() print(a) # class level repr print(a. normal_method()) # class level normal method a. __repr__ = instance_repr a. normal_method = instance_normal_method print(a) # class level repr print(a. normal_method()) # instance level normal method print(a. __repr__()) # instance level repr      ZODB - объектно ориентированная база данных для python объектов, поддерживает ACID транзакции (включая savepoints)     slice object:    L[2:4] == L[slice(2,4)]      iteration context (for, while, …) will try      __iter__       __getitem__     class Gen(object):   def __getitem__(self, index):     if index &gt; 5:       raise StopIteration()     return index for x in Gen():   print x, # output 0 1 2 3 4 5             for вызывает __iter__(). Потом к полученному объекту вызывает __next__() (в py2 . next()), пока не получит StopIteration. В классе можно использовать __iter__(): yield . . . , тогда не надо реализовать __next__     __call__ вызывается, когда скобки () применяются к инстансу, а не к классу    class A(object):   def __call__(self):     print( call ) a = A() # nothing a() # print call      __eq__ = True не подразумевает, что __ne__ = False     boolean context:      __bool__ (__nonzero__ in py2)   __len__   True      паттерны ООП      inheritance - “is a”   composition - “has a” (контейнер хранит другие объекты)   delegation - вид composition, когда хранится только один объект. Wrapper сохраняет основной интерфейс, добавляя какие-то шаги.       аттрибуты и методы класса, которые начинаются с двух подчеркиваний __, но не заканчиваются ими, имеют особое поведение. Они не пересекаются с одноименными аттрибутами и методами унаследованного класса. В __dict__ они попадают под именем _ClassName__attrname.    class A(object):   __x = 1   def show_a(self):     print self. __x class B(A):   def show_b(self):     print self. __x &gt;&gt;&gt; a = A() &gt;&gt;&gt; a. show_a() 1 &gt;&gt;&gt; b = B() &gt;&gt;&gt; b. show_a() 1 &gt;&gt;&gt; b. show_b() AttributeError: 'B' object has no attribute '_B__x' class B(A):   __x = 2   def show_b(self):     print self. __x &gt;&gt;&gt; b = B() &gt;&gt;&gt; b. show_a() 1 &gt;&gt;&gt; b. show_b() 2      в py3 можно в методе класса не указывать аргумент self, и использовать его только от имени класса (не инстанса) - он будет работать как static method. В py2 так нельзя.    class A(object):   def f():     print( f ) # python 2 &gt;&gt;&gt; A. f() TypeError: unbound method f() must be called with A instance as first argument (got nothing instead) # python 3 &gt;&gt;&gt; A. f() f &gt;&gt;&gt; a = A() &gt;&gt;&gt; a. f() TypeError: f() takes 0 positional arguments but 1 was given      bound function:    class A(object):   def f(self):     pass a = A() print(a. f. __self__) # вот где хранится self      поиск аттрибутов в classic (old-style) классах и new-style классах:      classic. DFLR: Depth First, Left to Right   new-style. Diamond pattern, L-R, D-F; MRO (хитрее, чем просто LRDF)    MRO исключает класс, от которого унаследованны &gt;= 2 других класса, от поиска аттрибуты дважды. Т. е. класс пападает в поиск только 1 раз.    # python 2 old-style class A: attr = 1 class B(A): pass class C(A): attr = 2 class D(B,C): pass &gt;&gt;&gt; x = D() &gt;&gt;&gt; print(x. attr) # x, D, B, A 1 # python 2 new-style class A(object): attr = 1 class B(A): pass class C(A): attr = 2 class D(B,C): pass &gt;&gt;&gt; x = D() &gt;&gt;&gt; print(x. attr) # x, D, B, C 2 # scheme A   A |   | B   C \   /   |   D   |   X    Посмотреть порядок поиска в new-style (по алгоритму mro):    &gt;&gt;&gt; D. __mro__ (&lt;class '__main__. D'&gt;, &lt;class '__main__. B'&gt;, &lt;class '__main__. C'&gt;, &lt;class '__main__. A'&gt;, &lt;type 'object'&gt;) &gt;&gt;&gt; D. mro() # все равно что list(D. __mro__) [&lt;class '__main__. D'&gt;, &lt;class '__main__. B'&gt;, &lt;class '__main__. C'&gt;, &lt;class '__main__. A'&gt;, &lt;type 'object'&gt;]      format() конструкция вызывает метод __format__. Если его нет, то в py2 - TypeError.   python 2    &gt;&gt;&gt; print('{0}'. format(object)) &lt;type 'object'&gt; &gt;&gt;&gt; print('{0}'. format(object. __reduce__)) TypeError: Type method_descriptor doesn't define __format__ # явно вызовим __str__ &gt;&gt;&gt; print('{0!s}'. format(object. __reduce__)) &lt;method '__reduce__' of 'object' objects&gt;    python 3. 4    &gt;&gt;&gt; print('{0}'. format(object. __reduce__)) &lt;method '__reduce__' of 'object' objects&gt;    python 2 &amp; 3    class A(object):   def __format__(self, *args):     return  A. __format__    def __str__(self):     return  A. __str__  &gt;&gt;&gt; a = A() &gt;&gt;&gt;  {0} . format(a) 'A. __format__' &gt;&gt;&gt; print(a) A. __str__ &gt;&gt;&gt; '%s' % a 'A. __str__'      В __dict__ не попадают “виртуальные” аттрибуты:      new-style properties (@property)   slots   descriptors   dynamic attrs computed with tools like __getattr__      MRO - method resolution order     diamond pattern - разновидность ‘multi inheritance’, когда 2 или более класса могут быть потомками одного и того же класса (object). Этот паттерн используется в python.     прокси объект, который возвращает super(), не работает с операторами:    # python 3 class A(list):   def get_some(self):     return super()[0] &gt;&gt;&gt; a = A([1, 2]) &gt;&gt;&gt; a. get_some() TypeError: 'super' object is not subscriptable class A(list):   def get_some(self):     return super(). __getitem__(0) &gt;&gt;&gt; a = A([1,2]) &gt;&gt;&gt; a. get_some() 1 # python 2 class A(list):   def get_some(self):     return super(A, self)[0] &gt;&gt;&gt; a = A([1,2]) &gt;&gt;&gt; a. get_some() TypeError: 'super' object has no attribute '__getitem__' class A(list):   def get_some(self):     return super(A, self). __getitem__(0) &gt;&gt;&gt; a = A([1,2]) &gt;&gt;&gt; a. get_some() 1      super()          Плюсы super():          если superclass нужно изменить в runtime, без super не получится: C. __bases__ = (Y, )           когда нужно вызвать цепочки унаследованных методов в multi inheritance классе, в порядке MRO.       Если попытаться вызвать без super, то можем вызвать метод какого-то класса дважды.        class A(object):   def __init__(self):     print( A ) class B(A):   def __init__(self):     print( B )     super(B, self). __init__() class C(A):   def __init__(self):     print( C )     super(C, self). __init__() class D(B, C):   pass &gt;&gt;&gt; d = D() B C A &gt;&gt;&gt; B. mro() [&lt;class '__main__. B'&gt;, &lt;class '__main__. A'&gt;, &lt;type 'object'&gt;] &gt;&gt;&gt; D. mro() [&lt;class '__main__. D'&gt;, &lt;class '__main__. B'&gt;, &lt;class '__main__. C'&gt;, &lt;class '__main__. A'&gt;, &lt;type 'object'&gt;]            Вызов цепочки методов        class B(object):   def __init__(self):     print( B )     # for B super is C here, by MRO order     super(B, self). __init__() class C(object):   def __init__(self):     print( C )     # it is ok here to call super(). __init__     # because object also has __init__     super(C, self). __init__() class D(B, C):   pass &gt;&gt;&gt; d = D() B C                super будет искать метод в иерархии MRO. Он будет искать пока не найдет или пока не упрется. Т. е. допустим иерархия для super: A, С, и в A нет метода, а в C есть, то вызовится C. method без ошибки              минусы (или особенности) super():          при использовании super все методы в цепочке должны принимать одинаковые аргументы     super(). m - все классы должны иметь метод m и вызывать super(). m, кроме последнего, который вызывать super не должен             унаследовать метод от конкретного класса:    class A(B, C):   other = C. other # not B other      finally вызовится даже если исключение случается в except блоке или else блоке     исключение - всегда instance, даже если raise ExceptionClass (без ()), автоматически (без аргументов) создается instance:    raise Exception # == raise Exception() raise # возбуждается перехваченное исключение      py2, посмотреть исключения builtin:    import exceptions help(exceptions)      минус чтения байтов из файла с последующим ручным decode в том, что если мы будем читать по кускам, то может быть сложный случай, когда один байт одного символа попадает в один кусок, а другой - в другой. Поэтому в py2 лучше использовать codecs.     Когда имя файла даем в unicode, python автоматически декодит и кодит в байты. Когда имя файла - байты, то не кодит нечего. encoding для имен файлов (дефолтный):    &gt;&gt;&gt; sys. getfilesystemencoding() 'utf-8'      дескриптор - это класс, который реализует хотя бы один из методов      __get__   __set__   __delete__      Если в дескрипторе не реализовать __set__, то это еще не значит, что соответствующий аттрибут будет read-only. Аттрибут просто перезатрется. Надо делать __set__ с исключением.     декоратры можно совмещать, тогда они будут применяться в порядке снизу вверх:    @A @B @C def f(): pass # все равно что f = A(B(C(f)))      декоратор может принимать аргументы. Реализовать можно через вложенные функции    @dec(a, b) def f(): pass # все равно что f = dec(a, b)(f) # реализация: def dec(a, b):   def actual_dec(f):     return f   return actual_dec    Т. е. декоратор может включать 3 уровня callables:      callable to accept decorator args   callable to serve as decorator   callable to handle calls to the original function      при создании класса вызываются два метода класса type:    type. __new__(type_class, class_name, super_classes, attr_dict) type. __init__(class, class_name, super_classes, attr_dict) # python 3 class Eggs: pass class Spam(Eggs):   data = 1   def method(self, arg): pass # все равно, что Eggs = type('Eggs', (), . . . ) # в () object добавится автоматически Spam = type('Spam', (Eggs, ), {'data': 1, 'method': method, '__module__': '__main__'})      Задать метакласс для класса   python 2    class Spam(object):   __metaclass__ = Meta    Наследовать от object не обязательно, но если его нет, а __metaclass__ есть, то результат все равно будет new-style, и в __bases__ будет object.  Но лучше явно указать object, т. к. могут быть проблемы, например с наследованием.   python 3    class Spam(Eggs, metaclass=Meta):   pass    аттрибут __metaclass__ просто игнорируется     Метакласс сам не обязательно должен быть классом. Просто его вызов должен возвращать класс. Это может быть и функция:    def meta_func(class_name, bases, attr_dict):   return type(class_name, bases, attr_dict) # python 2 class Spam(object):   __metaclass__ = meta_func      У обычных классов тоже есть метод __new__. Но он не создает класс, он вызывается при создании инстанса класса (получает готовый класс в качестве аргумента). Он же и вызывает __init__.     Магические методы метакласса и класса:    class Meta(type): pass    при создании класса Class (class Class(metaclass=Meta): . . . ) вызываются методы    Meta. __new__ Meta. __init__    при создании инстанса класса Class (instance = Class(. . . )) вызываются методы (внешний вызывает вложенный)    Meta. __call__   calls Class. __new__     calls Class. __init__    при вызове инстанса класса Class (instance()) вызывается метод    Class. __call__      Метакласс можно не наследовать от type, а определить метод __new__. Но тогда методы __init__, __call__ нашего метакласса не будут вызываться:    class MySimpleMetaClass(object):   def __new__(cls, *args, **kwargs):     new_class = type. __new__(type, *args, **kwargs)     return new_class   def __init__(new_class, *args, **kwargs):     print( __init__ won't be called. . .  )   def __call__(*args, **kwargs):     print( __call__ won't be called. . .  )      Метакласс класса вызывается и для всех потомков класса. Когда __new__ метакласса вызывается для родительского класса, в bases будет (&lt;type 'object'&gt;,), а для дочернего класса - класс родителя.     Аттрибуты метакласса наследуются классом, но не инстансами класса.   python 2 (в python 3 небольшие отличия в синтаксисе)    class MyMetaClass(type):   attr = 2   def __new__(*args, **kwargs):     return type. __new__(*args, **kwargs)   def toast(*args, **kwargs):     print(args, kwargs) class A(object):   __metaclass__ = MyMetaClass    Метакласс входит в цепочку поиска аттрибутов класса    &gt;&gt;&gt; A. toast() ((&lt;class '__main__. A'&gt;,), {})    Интересно, что метод от метакласса - bound, хотя вызывается от класса, не от инстанса. На самом деле класс - это инстанс метакласса:    &gt;&gt;&gt; A. toast &lt;bound method MyMetaClass. toast of &lt;class '__main__. A'&gt;&gt;    Но метакласс не входит в цепочку поиска аттрибутов инстанса класса    &gt;&gt;&gt; a = A() &gt;&gt;&gt; a. toast() AttributeError: 'A' object has no attribute 'toast'    Если в каком-нибудь super классе объявлен аттрибут с тем же именем, что и в метаклассе, то он имеет преимущество (не важно насколько глубоко super)    class B(object):   attr = 1 class C(B):   __metaclass__ = MyMetaClass &gt;&gt;&gt; C. attr 1 # MyMetaClass. attr = 2 is ignored    Аттрибуты инстанса ищутся в его __dict__, дальше в __dict__‘ах __class__. __mro__.  Аттрибуты класса ищутся еще и в __class__. __mro__, это другой класс, со стороны инстанса это будет __class__. __class__. __mro__.    &gt;&gt;&gt; inst = C() &gt;&gt;&gt; inst. __class__ -&gt; &lt;class '__main__. C'&gt; &gt;&gt;&gt; C. __bases__  -&gt; (&lt;class '__main__. B'&gt;,) &gt;&gt;&gt; C. __class__  -&gt; &lt;class '__main__. MyMetaClass'&gt;    Инстансы наследуют аттрибуты от всех суперклассов. Классы - от суперклассов и метаклассов. Метаклассы - от супер-метаклассов (и вероятно от мета-метаклассов).   Data descriptor’ы (те, которые определяют __set__) вносят небольшие поправки в порядок поиска аттрибутов для инстанса.  Для инстанса, data descriptor будут иметь преимущество в поиске, даже если они объявлены в супер классах:    class DataDescriptor(object):   def __get__(self, instance, owner):     print( DataDescriptor. __get__ )     return 5   def __set__(self, instance, value):     print( DataDescriptor. __set__ , value) class B(object):   attr = DataDescriptor() class C(B):   pass &gt;&gt;&gt; c = C() &gt;&gt;&gt; c. __dict__['attr'] = 88 &gt;&gt;&gt; c. attr DataDescriptor. __get__ 5 &gt;&gt;&gt; c. attr = 8 ('DataDescriptor. __set__', 8)    Вызвался дескриптор, несмотря на то, что мы задали аттрибут с тем же именем в c. __dict__.  Аттрибут не затер дескриптор суперкласса, сработал дескриптор.  Такого поведения не будет с обычным дескриптором (nondata):    class SimpleDescriptor(object):   def __get__(self, instance, owner):     print( SimpleDescriptor. __get__ )     return 5 class B(object):   attr = SimpleDescriptor() class C(B):   pass &gt;&gt;&gt; c = C() &gt;&gt;&gt; c. attr SimpleDescriptor. __get__ 5 &gt;&gt;&gt; c. __dict__['attr'] = 88 &gt;&gt;&gt; c. attr 88    Так же, для builtin операторов, которые вызывают магические методы, поиск особый. Он минует instance. __dict__, сразу идет поиск в __dict__ классов из __mro__.     магические методы, которые вызываются неявно путем использования builtin операторов для классов ищутся в метаклассе, минуя сами классы (сам класс и всего его суперклассы):   python 2 (в python 3 небольшие отличия в синтаксисе)    class MyMetaClass(type):   def __new__(*args, **kwargs):     return type. __new__(*args, **kwargs)   def __str__(cls):     return  __str__ from meta  class A(object):   __metaclass__ = MyMetaClass   def __str__(self):     return  __str__ from class A     Вызывается метод __str__ метакласса, а не класса:    &gt;&gt;&gt; print A __str__ from meta    А тут вызывается метод __str__ от object:    &gt;&gt;&gt; print MyMetaClass &lt;class '__main__. MyMetaClass'&gt;      Автор Марк Лутц немного беспокоится, что python становится слишком сложным и обрастает дублирующим функционалом, например:      str. format и %   with и try/finally    Это противоречит import this  "
    }, {
    "id": 8,
    "url": "https://st4lk.github.io/blog/2015/04/17/listen-wifi-with-wireshark.html",
    "title": "Слушаем wifi с помощью wireshark",
    "body": "2015/04/17 -  Всегда знал, что можно посмотреть сетевые пакеты, которые передаются по wifi сети. Но на практике этого не делал (на работе анализировал сетевые пакеты, но то был не HTTP протокол). Решил восполнить этот пробел, ведь это интересно и полезно. Более стройно выстраивается понимание протоколов TCP-IP и HTTP. Видно, как летят наши пароли и сессии, после такого опыта поневоле начинаешь относиться к безопасности сайта с большим трепетом. Трафик будем слушать с помощью программы Wireshark. Есть много утилит для анализа сетевой активности (ngrep, tcpdump, mitmproxy), но Wireshark пожалуй самая популярная и имеет огромный функционал. Опишу работу программы на примере таких задач:  послушать сетевые пакеты, которые мы отправляем/принимаем внутри локальной машины (localhost) послушать сетевые пакеты, которые отправляет/принимает наша локальная машина в/из внешнего мира (интернет) послушать сетевые пакеты, которые отправляют/принимают другие участники открытой wifi сети послушать сетевые пакеты, которые отправляют/принимают другие участники закрытой wifi сетиВсе действия я выполнял на ноутобуке MacBook Pro с OS X Yosemite, на других устройствах возможно что-то будет по-другому. Небольшой дисклеймер: все ваши действия на вашей совести и ответсвенности. Не используйте описанные здесь техники во вред кому-либо. Сетевые пакеты локального интерфейса (localhost): Итак, устанавливаем wireshark. Запускаем, заходим в меню Capture -&gt; Intefaces.  Я запускаю на ноутобуке, который подключен только к wifi сети (en0 интерфейс). Насколько я понимаю, awdl0 - это кабельный сетевой интерфейс. По кабелю ноутбук никуда не подключен, поэтому и пакетов нет. А lo0 - это локальный интерфейс, им сейчас и займемся. Ставим галочку напротив него и нажимаем Start. Чтобы сосредоточиться только на HTTP протоколе, зададим Display filter: http (этот фильтр применяется к уже перехваченным и обработанным пакетам, в отличие от Capture filter, но о нем позднее): Сейчас мы будем ловить сетевые пакеты, передающееся от нашего браузера к django development серверу и обратно. Есть особенности, связанные именно с django сервером.  Во-первых, он отдает HTTP 1. 0, а не HTTP 1. 1.  Во-вторых, что более важно, в ответных HTTP заголовках может не быть ни Content-Length: &lt;response length&gt;, ни Transfer-Encoding: chunked. В этом случае для определения конца ответа нужно дождаться закрытия соединения с сервером, чего не происходит. Это имеет значение при работе с wireshark. Wireshark понимает большое количество протоколов, в том числе и HTTP. Данные HTTP протокола могут передаваться в нескольких TCP сегментах, но программа группирует эти сегменты и показывает нам итоговый HTTP запрос или ответ. С запросом проблем не возникает, но HTTP ответа не видно в списке фреймов wireshark, потому что она не понимает, что ответ уже закончен (нет ни Content-Length, ни Transfer-Encoding). В принципе это не страшно, т. к. мы может нажать на фрейм запроса и выбрать из меню Analyse-&gt;Follow TCP Stream. В отдельном окне мы увидим HTTP запрос и соответствующий HTTP ответ (не важно, завершен он или нет). Follow TCP Stream так же удобен в том случае, если HTTP запрос и ответ идут не по порядку (между ними могут случиться другие запросы). Т. е. мы выбираем запрос, нажимаем Follow TCP Stream и видим всю цепочку сообщений. Однако, мне хотелось видеть и запросы и ответы в списке фреймов. Это мы можем сделать, добавив ConditionalGetMiddleware в список наших MIDDLEWARE_CLASSES: if DEBUG:  MIDDLEWARE_CLASSES = (    'django. middleware. http. ConditionalGetMiddleware',  ) + MIDDLEWARE_CLASSESПрослойка будет выставлять Content-Length в ответах. Такое нужно только при работе с django development сервером и wireshark. В остальных случаях все работает: продакшн серверы выставляют Transfer-Encoding: chunked и отдают HTTP по кусочкам (вероятно это делает proxy сервер (nginx, apache)). Теперь запустим простой django проект, который на главной странице отображает имя текущего пользователя, а так же форму для логина и пароля. Если пользователь не залогинен, то вместо имени показываем AnonymousUser. Для чистоты эксперимента почистим все куки в браузере для адреса 127. 0. 0. 1. Откроем страницу http://127. 0. 0. 1:8000/. Если мы не задали ‘ConditionalGetMiddleware’, то скорее всего увидим только запрос: Ответ все же можно увидеть, выбрав запрос и зайдя в Analyse-&gt;Follow TCP Stream: А если мы включили ‘ConditionalGetMiddleware’, то видим HTTP ответ уже в списке фреймов: Content-Length задан: Ну пока не очень интересно, только поковырялись с wireshark. Но давайте попробуем залогиниться. Вводим логин+пароль и нажимаем Login. В wireshark увидим 4 новых фрейма: POST запрос, редирект на главную страницу (ответ 302), запрос на получение главной, ответ главной страницы: Проследим внимательно за передаваемой информацией. Фрейм с POST запросом помимо HTTP заголовков содержит данные формы. Вот как они выглядят: Логин и пароль как на ладони. Ответом на POST запрос был фрейм с HTTP кодом 302 (редирект). В этом ответе сервер просит сохранить сессию в куках: Следующим идет запрос главной страницы, в куках передается id сессии: Вообщем вот так можно смотреть за данными, которыми обменивается ваше приложение с клиентом. Все эти данные можно увидеть и в wifi сети, которые передаются любым пользователем (если идут запросы по незащищенному соединению http). Если мы логинимся - виден логин/пароль. Если просто отправляем запросы - виден id сессии. Зная id сессии мы легко можем зайти под ее обладателем, просто записав их в наши куки. Для простоты, можно проверить какой-нибудь консольной утилитой (curl, httpie). Пример с httpie: $ http 127. 0. 0. 1:8000  Cookie: sessionid=tmpocxkz6zsir6xe6i03kspucvlqq385 HTTP/1. 0 200 OKContent-Length: 567Content-Type: text/html; charset=utf-8Date: Thu, 16 Apr 2015 13:06:58 GMTServer: WSGIServer/0. 1 Python/2. 7. 6Set-Cookie: csrftoken=3bUoLB28WyzcH7qG5GXreWPm0Pj11861; expires=Thu, 14-Apr-2016 13:06:58 GMT; Max-Age=31449600; Path=/Vary: CookieX-Frame-Options: SAMEORIGIN  &lt;html&gt;    &lt;body&gt;      Hello, alex      &lt;div&gt;        &lt;form action= /login/  method= POST &gt;          &lt;input type= text  name= username  /&gt;          &lt;input type= password  name= password  /&gt;          &lt;input type= text  hidden name= next  value= /  /&gt;          &lt;input type='hidden' name='csrfmiddlewaretoken' value='3bUoLB28WyzcH7qG5GXreWPm0Pj11861' /&gt;          &lt;input type= submit  name= Login  value= Login  /&gt;        &lt;/form&gt;      &lt;/div&gt;    &lt;/body&gt;  &lt;/html&gt;Вывелось Hello, alex, значит мы зашли под пользователем alex. Сетевые пакеты нашей локальной машины во внешний мир: Послушаем внешние сетевые запросы нашего компьютера. В wireshark выбираем Capture -&gt; Intefaces, ставим галочку напротив en0 и нажимаем Start: Зайду в админку этого сайта (lexev. org). В wireshark поставлю Display фильтр http. request. full_uri contains lexev. orgчтобы видеть только запросы на домен lexev. org. Вот что получилось: Видно id сессии, можно делать с ней все что захочешь. Сетевые пакеты других пользователей открытой wifi сети: До сих пор мы слушали только свои запросы и ответы. Но гораздо интересней послушать других пользователей. Зайдем в кафе с открытой wifi сетью, запустим wireshark. Заходим в Capture -&gt; Intefaces, выбираем соответствующий интерфейс и нажимаем Options (не Start). Видим что-то такое: Дважды щелкаем на интерфейс и ставим галочку напротив Capture packets in monitor mode: Ok, Start. Все, теперь мы слушаем всю сеть (кроме нас самих). В публичной wifi сети будет летать очень много пакетов, за час легко можно наловить больше Гб информации. Соответственно неудобно ее анализировать и сохранять. Тут пригодятся Capture фильтры, они применяются к еще не обработанным фреймам. Отсеченные фреймы этим фильтром не сохраняются. Отличие от Display фильтров в том, что мы работаем с нераскодированными данными, мы не знаем, что это - http или что-то другое. Поэтому Capture фильтры сложней составить. Итак, давайте попробуем сохранять только HTTP запросы GET или POST на 80 порту. Для этого зададим такой хитрый фильтр: (port 80 and tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420) or (tcp dst port 80 and (tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354))на этапе выбора интерфейса (кнопка Options): Попробуем подключиться с другого девайса (телефона) к этой wifi сети и опять отправим GET запрос на lexev. org. Для удобства добавим тот же Display фильтр домена (Capture и Display фильтры можно комбинировать). Видим id сессии: Ради интереса попробуем войти, введя логин и пароль. Смотрите все: Все как на ладони. Сетевые пакеты других пользователей в закрытой wifi сети: Допустим есть wifi сеть с WPA шифрованием. В интерфейсах выберем wifi, включим для него monitor mode (все как для открытой сети, только без Capture фильтра) и попробуем послушать сетевые пакеты. В результатах будет что-то такое: Мы видим зашифрованные данные. Но, если мы знаем пароль от wifi, то можем расшифровать пакеты. Для этого заходим в Edit -&gt; Preferences. Выбираем Protocol -&gt; IEEE 802. 11.  Нажимаем Edit напротив Decryption Keys. Далее New. В новом окошке вводим: Key type: wpa-pwdKey: password:wifinameСоответственно заменяем password на пароль от wifi сети, а wifiname - на название сети.  Apply, ok. Все, теперь wireshark расшифровывает пакеты, и мы можем смотреть http данные как и раньше: Послушаем открытую wifi сеть: Ради интереса зашел в макдональдс и примерно на час запустил wireshark (ловил только GET и POST http запросы). Далее сохранил все пакеты в файл pcap (File -&gt; Save as). Теперь вопрос, как проанализировать сохраненные фреймы? Их накопилось довольно много, вручную лазить по ним не удобно. Воспользуемся программой tshark, с помощью нее можно выбрать нужные данные и записать их в CSV. Вот так можно сохранить поля “номер фрейма”, “HTTP метод”, “full_uri” tshark -r macdak_pushkin_get_post_only. pcap -T fields -e frame. number -e http. request. method -e http. request. full_uri &gt; results. csvНаписав небольшой python скриптик, подсчитал количество запросов на каждый url и сгруппировал по доменам второго уровня. Вот 20 самых популярных доменов, на которые заходили:       Домен   Количество запросов         vk. com   6280       beeline. ru   5407       vk. me   2817       instagram. com   867       google. com   544       apple. com   536       yandex. ru   473       symcb. com   471       msftncsi. com   441       msn. com   304       yandex. net   302       trendmicro. com   292       co. uk   270       badoocdn. com   258       yadro. ru   188       marketgid. com   184       adfox. ru   183       mycdn. me   165       interfax. ru   154       scorecardresearch. com   137   Да, пару интересных сессий удалось перехватить. Например, для сайта знакомств mamba. ru, они передаются в открытую по HTTP. Скопировал куки, вставил их в chrome с помощью плагина EditThisCookie, зашел на сайт и вуаля, я - Сергей. Могу читать сообщения, смотреть настройки и прочее. Сразу скажу, что ничего я там не делал, посмотрел и вышел :). Способы защиты: Пожалуй единственным способом защиты является использование TLS (https). Здесь тоже есть нюанcы, его нужно использовать правильно. Описание правильной (и безопасной) работы с https заслуживает отдельного поста, так что тут не буду на этом останавливаться. Вот как выглядит https трафик в wireshark: Все данные зашифрованы, ничего не узнать. Итог:  По возможности используйте https, особенно если вы получаете какие-либо важные данные от пользователя (если что-то связано с банковскими данными/карточками, так вообще обязательно).  Будучи в открытой wifi сети, заходя на сайт по незащищенному каналу (http) всегда помните, что вас можно легко прослушать. Это же относится и к закрытой wifi, злоумышленнику достаточно лишь узнать пароль от wifi. Полезные ссылки:  Dan Callahan: Quick Wins for Better Website Security - PyCon 2014 Hynek Schlawack: The Sorry State of SSL - PyCon 2014 Benjamin Peterson - A Dive into TLS - PyCon 2015 Ashwini Oruganti, Christopher Armstrong - Introduction to HTTPS: A Comedy of Errors - PyCon 2015 Getting comfortable with web security: A hands-on session - PyCon 2015"
    }, {
    "id": 9,
    "url": "https://st4lk.github.io/blog/2015/03/04/debug-sql-django-test.html",
    "title": "Отладка SQL в django тестах",
    "body": "2015/03/04 -  В django тестах можно замерять количество сделанных SQL запросов: def test_home(self):  with self. assertNumQueries(1):    response = self. client. get('/')  self. assertEqual(response. status_code, 200)Если код в контексте assertNumQueries сделает иное количество обращений к БД, чем ожидается (здесь 1), то тест выдает ошибку. Но когда такой тест не проходит, бывает трудно определить лишний отправленный запрос. Для отладки такого случая удобно вывести в консоль отправленные SQL запросы. Далее опишу, как этого добиться. Кстати, если вы используете Django 1. 7+, то вместе с ошибкой будут показаны все SQL выражения, т. е. если этого достаточно - то делать вообще ничего не надо. Ура! Если же вы используете более старые версии, то данная статья будет полезна. Настройка логов для вывода SQL запросов в консоль: Django будет логировать SQL запрос, если соблюдается одного из следующих условий: settings. DEBUG = Trueили connection. use_debug_cursor = TrueПо умолчанию в тестах всегда DEBUG = False вне зависимости от того, что стоит в вашем settings. DEBUG. Это правильно, тестировать лучше с боевыми настройками. Т. о. остается connection. use_debug_cursor, которое по умолчанию всегда None или False (в разных версиях по разному). Но контекстный менеджер assertNumQueries сам выставляет это значение в True на время работы соответствующего блока! Нам остается лишь правильно задать настройки логов. Создадим отдельный файл настроек для тестов и назовем его settings_test. py. Тесты гонять будем с ним. Рекомендую всегда делать так - это удобно. Структура проекта примерно такая: project├── project│  ├── __init__. py│  ├── settings. py│  ├── settings_test. py│  ├── urls. py│  └── wsgi. py│├── spam # some app│  ├── __init__. py│  ├── views. py│  ├── tests. py│  └── models. py│└── manage. pyФайл tests. py содержит тесты приложения spam. Так организовывать тесты не самый хороший выбор, лучше выделить отдельную папку и держать все тесты там. Но для простоты нашего примера годится. Содержимое settings_test. py: from settings import *try:  from settings import LOGGINGexcept ImportError:  LOGGING = dict(version=1, disable_existing_loggers=False,    handlers={}, loggers={})# use database in memory to not lose your data!DATABASES = {  'default': {    'ENGINE': 'django. db. backends. sqlite3',    'NAME': ':memory:',    'USER': '',    'PASSWORD': '',    'TEST_CHARSET': 'utf8',  }}LOGGING['handlers']['console'] = {  'level': 'DEBUG',  'class': 'logging. StreamHandler',}LOGGING['loggers']['django. db. backends'] = {  'handlers': ['console'],  'level': 'DEBUG',  'propagate': False,}LOGGING['loggers']['django. db. backends. schema'] = {  'propagate': False, # don't log schema queries, django &gt;= 1. 7}Тест (tests. py) выглядит так: from django. test import TestCasefrom spam. models import Fooclass SpamTestCase(TestCase):  def setUp(self):    Foo. objects. create(title= Foo )  def test_home(self):    with self. assertNumQueries(1):      response = self. client. get('/')    self. assertEqual(response. status_code, 200)Давайте посмотрим, что будет при прогоне теста используя разные runner’ы. Django 1. 4+: $ python manage. py test (нет SQL): Creating test database for alias 'default'. . . . ----------------------------------------------------------------------Ran 1 test in 0. 009sOKDestroying test database for alias 'default'. . . $ python manage. py test --settings=project. settings_test (SQL в консоле) Creating test database for alias 'default'. . . (0. 000) SELECT  spam_foo .  id ,  spam_foo .  title  FROM  spam_foo  LIMIT 21; args=(). ----------------------------------------------------------------------Ran 1 test in 0. 009sOKDestroying test database for alias 'default'. . . Django 1. 4+ и django-nose:  pip install django-nose в settings. py выставляем TEST_RUNNER = ‘django_nose. NoseTestSuiteRunner’$ python manage. py test (нет SQL): . . . $ python manage. py test --settings=project. settings_test (SQL в консоле) (0. 000) SELECT  spam_foo .  id ,  spam_foo .  title  FROM  spam_foo  LIMIT 21; args=(). . . Django 1. 4+ и pytest-django:  pip install pytest-django создаем файл pytest. ini рядом с manage. py и задаем файл настроек:  [pytest]  DJANGO_SETTINGS_MODULE = project. settings переименуем tests. py в test_spam. py (стандартные имена для py. test)$ py. test (нет SQL): . . . $ py. test --ds=project. settings_test (нет SQL, py. test перехватывает весь вывод) . . . $ py. test --ds=project. settings_test -s (SQL в консоле) (0. 000) SELECT  spam_foo .  id ,  spam_foo .  title  FROM  spam_foo  LIMIT 21; args=(). . . Итого: Как видно из предыдущих примеров, при использовании нашего settings_test в консоль выводятся все запросы к БД, сделанные в блоке контекстного менеджера assertNumQueries. Если мы хотим гонять тесты всегда с этими настройками, а не только когда хотим что-то отладить, то очень скоро устанем от обилия SQL. Можно сделать так: закомментировать строчку 'handlers': ['console'],, т. о. ничего выводится не будет. А когда нужно их посмотреть - просто убираем комментарий. Вывод ошибки assertNumQueries и Django 1. 7+: Если assertNumQueries регистрирует отличное от ожидаемого количество обращений к БД, то выводится traceback и ошибка: Traceback (most recent call last): . . . AssertionError: 1 queries executed, 2 expectedНо в django 1. 7+ так же выводятся и сделанные запросы: Captured queries were:QUERY = u'SELECT  spam_foo .  id ,  spam_foo .  title  FROM  spam_foo  LIMIT 21' - PARAMS = ()Как вы понимаете, настройки логов на это не влияют, очень удобно! Вывод всех SQL запросов в консоль не используя assertNumQueries: До сих пор речь шла об использовании assertNumQueries. Но что если нужно посмотреть запросы вне этого менеджера? Для этого нужно самому выставить connection. use_debug_cursor = True перед прогоном тестов. Это можно сделать в test runner’е или используя специальный hook в py. test. SQL во всех тестах: Django 1. 4+: Создаем файл test_runner. py, кладем его рядом с settings. py и вставляем такой код: try:  from django. test. runner import DiscoverRunner as DjangoTestSuiteRunnerexcept ImportError:  # django &lt; 1. 6  from django. test. simple import DjangoTestSuiteRunnerfrom django. db import connections, DEFAULT_DB_ALIASclass SqlDebugTestSuiteRunner(DjangoTestSuiteRunner):  def setup_test_environment(self, **kwargs):    super(SqlDebugTestSuiteRunner, self). setup_test_environment(**kwargs)    connections[DEFAULT_DB_ALIAS]. use_debug_cursor = TrueТеперь либо в settings (или settings_test, если используем его для тестов) указываем наш runner: TEST_RUNNER = 'project. test_runner. SqlDebugTestSuiteRunner'Запускаем python manage. py test --settings=project. settings_test и видим все запросы, созданные за время тестов. Либо можно не задавать TEST_RUNNER, а использовать аргумент –testrunner: python manage. py test --settings=project. settings_test --testrunner=project. test_runner. SqlDebugTestSuiteRunner Результат будет тем же. SQL во всех тестах: Django 1. 4+ и django-nose: Для nose почти все так же, только runner нужно унаследовать от NoseTestSuiteRunner. test_runner. py: from django_nose import NoseTestSuiteRunnerfrom django. db import connections, DEFAULT_DB_ALIASclass SqlDebugTestSuiteRunner(NoseTestSuiteRunner):  def setup_test_environment(self, **kwargs):    super(SqlDebugTestSuiteRunner, self). setup_test_environment(**kwargs)    connections[DEFAULT_DB_ALIAS]. use_debug_cursor = TrueДа, на момент написания статьи версия django-nose==1. 3. В этой версии не поддерживается аргумент –testrunner. Но я отправил пул реквест, возможно, он уже попал в релиз. SQL во всех тестах: Django 1. 4+ и pytest-django: В py. test все немного по другому, ведь там не используется стандартный runner из django. Вместо этого для установления тестового окружения можно использовать py. test хуки. Создаем файл plugin_debug_sql. py, кладем рядом с settings. py, вставляем код: def pytest_runtest_setup(item):  from django. db import connections, DEFAULT_DB_ALIAS  connections[DEFAULT_DB_ALIAS]. use_debug_cursor = TrueЗапускать так: PYTHONPATH=`pwd`:$PYTHONPATH py. test -s --ds=sql. settings_test -p project. plugin_debug_sql Тут я явно добавляю текущую папку в PYTHONPATH, потому что иначе py. test не сможет найти наш плагин. "
    }, {
    "id": 10,
    "url": "https://st4lk.github.io/blog/2015/01/31/tornado-internationalization-and-localization.html",
    "title": "Tornado i18n и l10n",
    "body": "2015/01/31 - Star Статья о том, что такое i18n и i10n и как это реализовать в приложении на tornado. Получилось довольно много букв, но хотелось рассказать доступно обо всем процессе. Сама пошаговая инструкция - во второй половине статьи. Общие определения: i18n: i18n - сокращение от internationalization. Так называют процесс поддержки разных языков в приложении. Это не сам перевод, а именно техническая составляющая проекта, которая позволяет отображать текст на разных языках, в зависимости от предпочтений пользователя. Обычно реализацией занимается разработчик. l10n: l10n - сокращение от localization. Означает сам процесс перевода текста на нужные языки. Обычно реализацией занимается переводчик. Языковые теги (language tags): Языковые теги (language tags) указывают язык текста. Формат тегов содержит много нюансов, все они определены в rfc5646. Но наиболее часто встречающийся такой: en-USПервая часть означает язык, вторая - регион применения. В данном случае тег en-US означает английский язык, которым пользуются в США. А скажем en-GB будет означать ангийский, которым пользуются в Англии (я полагаю, эти языки имеют небольшие различия). В языковом теге только одна часть обязательная - это язык. Т. е. это абсолютно нормальный тег: enБолее того, когда нет особой надобности указывать регион - не указывайте его. CLDR: CLDR - Common Locale Data Repository (общее хранилище языковых данных).  Содержит часто используемые данные на разных языках:  формат даты, цены в разных валютах, временных зон название стран, дней недели, месяцев правила написания чисел, формат множественного числа, направление письма, первый день недели и много всего другогоgettext: gettext - библиотека для реализации процесса i18n. В этой статье описывается работа именно с ней. Tornado поддерживает так же работу и с CSV файлам, но этот способ имеет гораздо меньше возможностей, поэтому его рассматривать не будем. Краткий принцип работы с gettext:  Все текстовые строки в коде пишем на английском.  Все строки подставляем в виде аргумента одной из специальных функций, реализованных в gettext. Обычно в python’е для такого случая используют функцию с именем _.  Было:    Hello world!     Стало:   _( Hello world! )    Создаем шаблонный файл . pot для перевода текста.  Это делается с помощью команды xgettext. Она парсит указанные файлы, находит в них вызов той самой функции _ и генерирует файл messages. pot.  В этом файле, помимо некоторых служебных данных (заголовков), будут содержаться строки, требующие перевода:   msgid  Hello world!  msgstr       Этот файл не нужно редактировать, пусть он всегда будет таким. В принципе, особой нужды в этом файле нет, непосредственно в переводе он не задействован. Например скрипты django для перевода его нигде не сохраняют. Но все же лучше оставить, ведь его всегда можно дать переводчику, чтобы он мог оценить объем работы.     Создаем файл перевода на конкретном языке.   Для этого воспользуемся командой msginit, которая из шаблона messages. pot создаст новый файл messages. po. По сути, это будет копия нашего шаблона, за исключением некоторых частей, специфичных для выбранного языка.     Переводим строки в нашем новом messages. po, а так же заполняем заголовки нашими данными. Перевод будет выглядеть так:   msgid  Hello world!  msgstr  Привет, мир!       Компилируем перевод командой msgfmt. На выходе получаем messages. mo.     Если где-то в коде добавились строки, которые нужно переводить, то следует лишь обновить messages. po, а не создавать его заново.   Таким образом старые переводы сохранятся. Для этого:      опять создаем шаблон . pot (пункт 3). Да, это перезатрет предыдущий messages. pot, но это не страшно, ведь мы там ничего не меняем сами   используем команду msgmerge, которая синхронизирует файлы . po и . pot   повторяем пункты 5 и 6.    Все, если функция _ в нашем проекте работает правильно, то англичанин увидит текст “Hello world!”, а русский - “Привет, мир!”. Пока осталось неясным, что из себя представляет эта самая функция _ и откуда ее взять. Так же, как мы определим предпочитаемый язык пользователя, кто он - англичанин или русский? И где взять эти команды xgettext, msginit, msgmerge, msgfmt? По порядку. Функция _: Эту функцию можно написать самому, ведь в python уже встроена работа с gettext. Однако, в tornado и в django она уже определена. Надо лишь задать ей это имя. В django это выглядит так: from django. utils. translation import ugettext as __( Hello world! )А в tornado примерно так: class SomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    _( Hello world! )Как функция _ определяет предпочитаемый язык пользователя?: В самом простом случае в веб приложении язык узнают из HTTP заголовка Accept-Language (предпочитаемый язык задается в настройках браузера). И в django и в tornado это уже реализовано.  Для более сложной логики, когда например пользователь сам может задать нужный ему язык в настройках своего профиля на сайте, в обоих фреймворках есть соответствующие средства. Да, в отличие от django, tornado из коробки не умеет определять язык из префикса в url. Т. е. при правильной настройке django, запрос вида /ru/. . . / отобразит русский язык, а /en/. . . / - английский. Но tornado так не умеет, его этому нужно обучать вручную. Где взять команды xgettext, msginit, msgmerge, msgfmt: Их нужно установить. В некоторых версиях Ubuntu они доступны из коробки. Установить можно так: sudo apt-get install gettextНа OSX: brew install gettextbrew link gettext --forceДля windows бинарники можно скачать отсюда (не проверял): http://gnuwin32. sourceforge. net/packages/gettext. htm Если устанавливать не хочется или нет возможности, то можно воспользоваться встроенными в python командами: pygettext. py, msgfmt. py. Но у них гораздо меньше возможностей, чем у xgettext. К тому же нет msgmerge, который крайне удобен. Есть еще одно решение - библиотека babel. Она поддерживает многие функции xgettext, включая msgmerge. При этом не требует установленной xgettext, чистый питон. Расскажу о ней чуть позже. Отличия tornado от django: Прежде чем приступать к инструкции по реализации i18n в tornado, хочу отметить одну особенность.  Она крайне важна для понимания работы tornado в целом. Основное отличие tornado от django состоит в том, что tornado выполняется в одном процессе, а django - нет. Как это влияет на перевод строк? В django мы можем задать переводимые строки где угодно, хоть в моделях. Для этого в django есть понятие “ленивого” перевода, например ugettext_lazy: from django. db import modelsfrom django. utils. translation import ugettext_lazy as _class SomeModel(models. Model):  title = models. CharField(_( Title ), max_length=50)Функция ugettext_lazy возвращает строку не сразу, а в момент непосредственного обращения к ней. Только в тот момент, когда предпочитаемый язык уже будет определен. Но откуда она все-таки узнает этот язык? Очевидно, что первоначально клиент должен сделать какой-то запрос, из которого мы узнаем информацию о посетителе и определим его локализацию. В этот момент django сохранит найденный язык (с помощью функции activate()) в глобальную переменную для данного потока (thread). Напомню, что для обработки каждого запроса в django создается отдельный изолированный поток.  Вот почему функция ugettext_lazy может быть использована где угодно, она отобразит текст на верном языке. Ей не нужно передавать никакие данные о запросе, их она узнает из глобальной переменной. А в tornado так не получится, потому что тут нет изолированных потоков, поток всегда один. В этом и фишка асинхронности.  Что может получится, если мы попытаемся реализовать “ленивый” перевод в tornado? Давайте рассмотрим простейший проект на tornado. Для реализации “ленивости” воспользуемся пакетом speaklater: import os. pathfrom threading import localimport tornado. webfrom speaklater import make_lazy_gettext_active = local()def activate(current_locale):  _active. value = current_localedef gettext(s):  return _active. value. translate(s)_ = make_lazy_gettext(lambda: gettext)class HiModel(object):  hello = _( Hello, world! )class DoneModel(object):  done = _( Fuh, done )class HomeHandler(tornado. web. RequestHandler):  def prepare(self):    super(HomeHandler, self). prepare()    activate(self. locale) # set globally locale from request  @tornado. gen. coroutine  def get(self):    hello_model = HiModel()    self. write(unicode(hello_model. hello))    self. write( &lt;br/&gt; )    done_model = DoneModel()    self. write(unicode(done_model. done))    self. write( &lt;br/&gt; )    self. finish()application = tornado. web. Application([  tornado. web. url(r / , HomeHandler, name='home'),])if __name__ ==  __main__ :  application. listen(8888)  project_folder = os. path. dirname(os. path. abspath(__file__))  tornado. locale. load_gettext_translations(os. path. join(project_folder, 'locale'), 'messages')  tornado. ioloop. IOLoop. instance(). start()Тут HiModel и DoneModel подразумевают какие-то модели, неважно какие. Главное, что у них есть переводимые строки.  Файл перевода выглядит примерно так: msgid  Hello, world! msgstr  Привет, мир! msgid  Fuh, done msgstr  Фух, вроде готово Запустим наш маленький сервер.  Условимся, что в браузере “browser_en” в настройках стоит английский язык, а в браузере “browser_ru” - русский. Откроем адрес http://127. 0. 0. 1:8888/ в browser_ru и увидим: Привет, мир!Фух, вроде готовоТо же самое в browser_en: Hello, world!Fuh, doneВроде все работает правильно. Но попробуем добавить асинхронную задачу. Для простоты воспользуемся асинхронным таймером: @tornado. gen. coroutinedef get(self):  hello_model = HiModel()  self. write(unicode(hello_model. hello))  self. write( &lt;br/&gt; )  io_loop = tornado. ioloop. IOLoop. instance()  yield tornado. gen. Task(io_loop. add_timeout, timedelta(seconds=5))  done_model = DoneModel()  self. write(unicode(done_model. done))  self. write( &lt;br/&gt; )  self. finish()Т. е. мы просто добавили эти строки:   io_loop = tornado. ioloop. IOLoop. instance()  yield tornado. gen. Task(io_loop. add_timeout, timedelta(seconds=5))между работой с HiModel и DoneModel.  А теперь попробуем зайти из browser_ru и сразу, не дожидаясь окончания таймера, из browser_en. В browser_ru мы увидим это: Привет, мир!Fuh, doneа в browser_en: Hello, world!Fuh, doneДумаю вы уже догадались, почему в browser_ru мы видим часть текста на русском, а часть - на английском. На всякий случай давайте разберемся. В момент, когда было обращение из browser_ru, мы выставили глобально русский язык.  Потом пошла асинхронная задача.  Дальше пришел другой запрос (browser_en), который выставил глобально английский язык.  После чего асинхронная команда из первого запроса завершилась и обработчик продолжил работу. Но язык уже поменялся другим обработчиком, и строка “Fuh, done” не перевелась. Из всего этого можно сделать вывод, что в tornado определить язык можно только в контексте запроса (handler), и никак иначе. Реализация i18n в tornado с помощью xgettext : В документации tornado процесс i18n описан довольно скудно, поэтому здесь хочу описать прям по шагам, что и как нужно делать. По-моему, лучше всего объяснять на конкретном примере. Поэтому давайте создадим простейший проект на tornado. Структура проекта элементарная: └── project  ├── app. py  ├── home. html  └── requirements. txtapp. py: import tornado. ioloopimport tornado. webclass HomeHandler(tornado. web. RequestHandler):  def get(self):    self. render( home. html , text= Hello, world! )application = tornado. web. Application([  tornado. web. url(r / , HomeHandler, name='home'),])if __name__ ==  __main__ :  application. listen(8888)  tornado. ioloop. IOLoop. instance(). start()home. html: &lt;html&gt;  &lt;head&gt;&lt;title&gt;Home page&lt;/title&gt;&lt;/head&gt;  &lt;body&gt;   &lt;div&gt;Home page&lt;/div&gt;   &lt;div&gt;{{text}}&lt;/div&gt;  &lt;/body&gt; &lt;/html&gt;requirements. txt: tornado==4. 0. 2Цель: Добавить в проект поддержку двух языков: английского и русского. Если у пользователя в браузере предпочитаемый язык английский - показывать текст на английском, если русский - то соответственно русский. 1. Весь текст в коде должен быть на английском: У нас это уже выполнено, на данный момент в проекте есть такие строки:  Hello, world!  # app. py Home page   # home. html2. Маркируем текст: Первым делом нужно обозначить строки, требующие перевода (пункт 2. раздела gettext).  Как мы помним, это делается функцией _.  В коде обработчика ее можно получить так: _ = self. locale. translateКод хендлера (файл app. py): class HomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    self. render( home. html , text=_( Hello, world! ))В шаблоне эта функция уже доступна, она определена в методе get_template_namespace().  Код шаблона (файл home. html): &lt;html&gt;  &lt;head&gt;&lt;title&gt;{{_( Home page )}}&lt;/title&gt;&lt;/head&gt;  &lt;body&gt;   &lt;div&gt;{{_( Home page )}}&lt;/div&gt;   &lt;div&gt;{{text}}&lt;/div&gt;  &lt;/body&gt; &lt;/html&gt;}Шаблонизатор tornado выполняет python код, который заключен внутри двойных фигурных скобок {{ . . . }}. 3. Создаем файл перевода: Сперва создадим папку locale в корне нашего проекта: mkdir localeДалее создаем файлик makemessages. sh в корне проекта и кладем туда такой bash код: #!/bin/bash # get arguments and init variablesif [  $#  -lt 1 ]; then  echo  Usage: $0 &lt;locale&gt; [optional: &lt;domain_name&gt;]   exit 1filocale=$1domain= messages if [ ! -z  $2  ]; then  domain=$2filocale_dir= locale/${locale}/LC_MESSAGES pot_file= locale/${domain}. pot po_file= ${locale_dir}/${domain}. po # create folders if not existsmkdir -p $locale_dir# create . pot filefind . -iname  *. html  -o -iname  *. py  | xargs \  xgettext --output=${pot_file} --language=Python --from-code=UTF-8 \  --sort-by-file --keyword=_ --keyword=_:1,2 --no-wrap# init . po file, if it doesn't exist yetif [ ! -f $po_file ]; then  msginit --input=${pot_file} --output-file=${po_file} --no-wrap --locale=${locale}else  # update . po file  msgmerge --no-wrap --sort-by-file --output-file=${po_file} ${po_file} ${pot_file}fiДаем права на запуск: chmod a+x makemessages. shПри запуске нужно указать языковой тег. Для этого языка мы будем делать перевод, в данном примере это русский: . /makemessages. sh ruПри первом запуске может потребоваться ввести ваш email. После выполнения появятся файлы messages. pot и messages. po: project├── locale│  ├── ru│  │  └── LC_MESSAGES│  │    └── messages. po│  └── messages. pot├── app. py├── home. html├── requirements. txt└── makemessages. shСодержимое messages. po: # Russian translations for tornado_i18n package. # Copyright (C) 2015 THE tornado_i18n'S COPYRIGHT HOLDER# This file is distributed under the same license as the tornado_i18n package. # stalk &lt;alexevseev@gmail. com&gt;, 2015. #msgid   msgstr    Project-Id-Version: tornado_i18n\n  Report-Msgid-Bugs-To: \n  POT-Creation-Date: 2015-01-30 12:27+0300\n  PO-Revision-Date: 2015-01-30 12:27+0300\n  Last-Translator: stalk &lt;alexevseev@gmail. com&gt;\n  Language-Team: Russian\n  Language: ru\n  MIME-Version: 1. 0\n  Content-Type: text/plain; charset=ASCII\n  Content-Transfer-Encoding: 8bit\n  Plural-Forms: nplurals=3; plural=(n%10==1 &amp;&amp; n%100!=11 ? 0 : n%10&gt;=2 &amp;&amp; n%10&lt;=4 &amp;&amp; (n%100&lt;10 || n%100&gt;=20) ? 1 : 2);\n #: app. py:8msgid  Hello, world! msgstr   #: home. html:2 home. html:4msgid  Home page msgstr   Здесь нужно обязательно поменять charset=ASCII на charset=UTF-8, остальные заголовки - опционально:  Content-Type: text/plain; charset=UTF-8\n 4. Переводим: Заполняем пустые строки вида msgstr “” в messages. po (не . pot!): #: app. py:8msgid  Hello, world! msgstr  Привет, мир! #: home. html:2 home. html:4msgid  Home page msgstr  Домашняя страница 5. Компилируем перевод: Создаем файлик compilemessages. sh с кодом: #!/bin/bash # get arguments and init variablesif [  $#  -lt 1 ]; then  echo  Usage: $0 &lt;locale&gt; [optional: &lt;domain_name&gt;]   exit 1filocale=$1domain= messages if [ ! -z  $2  ]; then  domain=$2filocale_dir= locale/${locale}/LC_MESSAGES po_file= ${locale_dir}/${domain}. po mo_file= ${locale_dir}/${domain}. mo # create . mo file from . pomsgfmt ${po_file} --output-file=${mo_file}Даем права на запуск: chmod a+x compilemessages. shПри запуске указываем тот же языковой тег, что и для makemessages. sh: . /compilemessages. sh ruПолучился файл locale/ru/LC_MESSAGES/messages. mo: project├── locale│  ├── ru│  │  └── LC_MESSAGES│  │    ├── messages. po│  │    └── messages. mo│  └── messages. pot├── app. py├── home. html├── requirements. txt├── compilemessages. sh└── makemessages. sh6. Связываем перевод с нашим проектом: Для этого вызываем функцию load_gettext_translations(). app. py: if __name__ ==  __main__ :  tornado. locale. load_gettext_translations('locale', 'messages')  application. listen(8888)  tornado. ioloop. IOLoop. instance(). start()Попробуем запустить проект. python app. pyОткроем в browser_en адрес http://127. 0. 0. 1:8888/: Home pageHello, world!и brower_ru: Домашняя страницаПривет, мир!Вот и все, перевод готов! 7. Обновление перевода: Допустим, в файле home. html у нас добавился текст, который нужно перевести: &lt;div&gt;{{_( Good buy! )}}&lt;/div&gt;}В этом случае мы просто выполняем наш скрипт: . /makemessages. sh ruзаполняем перевод в обновленном messages. po: #: home. html:6msgid  Good buy! msgstr  До свидания! компилируем: . /compilemessages. sh ruи перезапускаем сервер python app. pyВсе! 8. Множественные значения: gettext поддерживает множественные значения (plural forms). Вот как это выглядит: ngettext( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)ngettext - стандартное имя функции для множественных форм, которое используется в gettext.  Но в tornado функция self. locale. translate, которую мы назвали как _, так же поддерживает аргументы ngettext. Вообщем, мы можем вместо ngettext использовать привычный нам _: _( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)Обратите внимание на аргументы функции xgettext в нашем скрипте выше: --keyword=_ --keyword=_:1,2Так мы указываем для парсера, что функция _ может принимать как одну строку, так и две вместе с числом. Смысл этого в том, что в зависимости от значения count строка будет принимать либо множественное значение, либо единичное.  В случае английского языка это будет выглядеть так: _( {count} event is gonna happen ,  {count} events are gonna happen , 1). format(count=1)# вывод 1 event is gonna happen _( {count} event is gonna happen ,  {count} events are gonna happen , 2). format(count=2)# вывод 2 events are gonna happen Как мы видим, у английского языка только 1 форма множественного числа. Т. е. всегда, когда count &gt; 1, вывод будет один и тот же. Однако в русском языке может быть 3 формы (а в некоторых других языках и того больше). Давайте попробуем перевести вручную эту строку: 1,21,31 событие должно случиться2,3,4,22 события должно случиться5,6,7,8,9,20,25 событий должно случитьсяЕсли мы все сделаем правильно, gettext будет выводить верную форму. Реализуем это в нашем примере.  Добавим в шаблон home. html такую строку: &lt;div&gt;{{_( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)}}&lt;/div&gt;а в обработчик из файла app. py: class HomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    count = int(self. get_argument('count', 1))    self. render( home. html , text=_( Hello, world! ), count=count)Обновим наш файл переводов: . /makemessages. sh ruВидим такие строки в locale/ru/LC_MESSAGES/messages. po: #: home. html:6#, python-brace-formatmsgid  {count} event is gonna happen msgid_plural  {count} events are gonna happen msgstr[0]   msgstr[1]   msgstr[2]   gettext сам знает, что в русском языке для множественного значения может быть 3 формы. Обратите внимание на заголовок, который был создан командой msginit:  Plural-Forms: nplurals=3; plural=(n%10==1 &amp;&amp; n%100!=11 ? 0 : n%10&gt;=2 &amp;&amp; n%10&lt;=4 &amp;&amp; (n%100&lt;10 || n%100&gt;=20) ? 1 : 2);\n Условия из этого заголовка и определяют нужный вариант в зависимости от числа.  Итак, добавим перевод: #: home. html:6#, python-brace-formatmsgid  {count} event is gonna happen msgid_plural  {count} events are gonna happen msgstr[0]  {count} событие должно случиться msgstr[1]  {count} события должно случиться msgstr[2]  {count} событий должно случиться Скомпилируем: . /compilemessages. sh ruЗапустим сервер и попробуем обратиться из browser_ru http://127. 0. 0. 1:8888/: 1 событие должно случитьсяhttp://127. 0. 0. 1:8888/?count=2 2 события должно случитьсяhttp://127. 0. 0. 1:8888/?count=5 5 событий должно случитьсяОтлично! 9. Особая логика для выбора языка: До сих пор язык пользователя определялся из настроек его браузера. Но допустим мы хотим, чтобы пользователь мог сам выбрать язык и сохранить его где-нибудь у себя в профиле на нашем сайте.  Логику выбора языка можно легко изменить. Если мы это сделаем, то настройки браузера уже не будут влиять.  Для этого просто нужно определить язык в методе get_user_locale(): class HomeHandler(tornado. web. RequestHandler):  def get_user_locale(self):    return tornado. locale. get( ru )  def get(self):    _ = self. locale. translate    self. render( home. html , text=_( Hello, world! ))Теперь независимо от браузера, текст всегда будет отображаться на русском языке. Библиотека babel: Идем дальше. babel предоставляет доступ к CLDR данным из python’а. Так же она реализует команды xgettext на python’e, т. о. xgettext можно не устанавливать. Но остановимся пока на CLDR. Давайте попробуем вывести цену какого-нибудь продукта в долларах. В разных странах приняты разные способы отображения цен.  Например, в России обычно указывают цену так: 99,00 $ а в США так: $99. 00 а в какой-то другой стране еще по другому. И эта информация, помимо всего прочего, есть в CLDR (рубли там тоже есть :))! Мы можем легко использовать ее благодаря babel.  Установим babel: pip install babelИзменим app. py: import tornado. ioloopimport tornado. webfrom babel. numbers import format_currencyclass HomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    count = int(self. get_argument('count', 1))    format_usd = lambda p: format_currency(p, currency= USD ,      locale=self. locale. code)    self. render( home. html , text=_( Hello, world! ), count=count,      format_usd=format_usd)а в шаблон home. html добавим цену: &lt;div&gt;{{format_usd(99)}}&lt;/div&gt;И все, ничего больше делать не надо.  browser_ru отобразит: 99,00 $browser_en: $99. 00Это лишь одна из множества функций, доступная в babel. Там есть дни недели, месяцев, форматы даты и многое другое. И этим можно и нужно пользоваться, не стоит переводить все вручную. Реализация i18n в tornado с помощью babel : Как я уже говорил, в babel реализованы основные функции xgettext на python’e. Т. о. можно не устанавливать xgettext. Так же в babel есть возможность задавать свой синтаксис для парсинга. Это может быть удобно для шаблонов html, где синтаксис отличается от python. Возьмем все тот же наш маленький проект. По идее, нам нужно будет лишь изменить скрипты makemessages. sh и compilemessages. sh. Для чистоты эксперимента удалим все файлы внутри папки locale. Т. о. структура проекта будет такая: project├── locale├── app. py├── home. html├── requirements. txt├── makemessages. sh└── compilemessages. shУстановим babel, если еще не установили: pip install babelТак же для парсинга шаблонов tornado нам понадобится пакет tornado-babel: pip install tornado-babelПервым делом нужно создать файл locale/babel. cfg с содержимым: [python: **. py][tornado: **. html]Так мы указываем, какие файлы парсить и какой синтаксис использовать при парсинге. Перепишем наш makemessages. sh, чтобы он вызывал команды babel вместо xgettext: #!/bin/bash # get arguments and init variablesif [  $#  -lt 1 ]; then  echo  Usage: $0 &lt;locale&gt; [optional: &lt;domain_name&gt;]   exit 1filocale=$1domain= messages if [ ! -z  $2  ]; then  domain=$2fibase_dir= locale locale_dir= ${base_dir}/${locale}/LC_MESSAGES pot_file= ${base_dir}/${domain}. pot po_file= ${locale_dir}/${domain}. po babel_config= ${base_dir}/babel. cfg # create pot templatepybabel extract . / --output=${pot_file} \  --charset=UTF-8 --no-wrap --sort-by-file \  --keyword=_ --mapping=${babel_config}# init . po file, if it doesn't exist yetif [ ! -f $po_file ]; then  pybabel init --input-file=${pot_file} --output-dir=${base_dir} \    --domain=${domain} --locale=${locale} --no-wrapelse  # update . po file  pybabel update --domain=${domain} --input-file=${pot_file} \  --output-dir=${base_dir} --locale=${locale} --no-wrapfiА так же compilemessages. sh: #!/bin/bash # get arguments and init variablesif [  $#  -lt 1 ]; then  echo  Usage: $0 &lt;locale&gt; [optional: &lt;domain_name&gt;]   exit 1filocale=$1domain= messages if [ ! -z  $2  ]; then  domain=$2filocale_dir= locale/${locale}/LC_MESSAGES po_file= ${locale_dir}/${domain}. po mo_file= ${locale_dir}/${domain}. mo # create . mo file from . popybabel compile --locale=${locale} --domain=${domain} --directory=locale/Если вы заметили, мы указываем только --keyword=_. Без --keyword=_:1,2. Почему? Дело в том, что в версии babel==1. 3, которая доступна из pypi на момент написания статьи, не поддерживаются разные аргументы для одной и той же функции.  На что это влияет в нашем случае? Нам придется для множественных форм использовать функцию ngettext, а не _.  Для этого поправим чуть-чуть app. py, определив ngettext и передав ее в шаблон: class HomeHandler(tornado. web. RequestHandler):  def get(self):    ngettext = _ = self. locale. translate    # . . .     self. render( home. html , text=_( Hello, world! ), count=count,      ngettext=ngettext)и в шаблоне для перевода множественных форм используем ngettext: &lt;div&gt;{{ngettext( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)}}&lt;/div&gt;Дальше все по старому. Создаем файлы перевода: . /makemessages. sh ruОпять добавляем перевод в locale/ru/LC_MESSAGES/messages. po. Компилируем: . /compilemessages. sh ruПри выполнении этого скрипта можно увидеть что-то вроде catalog 'locale/ru/LC_MESSAGES/messages. po' is marked as fuzzy, skippingТогда нужно в файле messages. po удалить такие строки, говорящие, что перевод не подтвержденный: #, fuzzyи опять скомпилировать перевод. И теперь все работает аналогично. Но без xgettext! Исправляем babel: Согласитесь, что неудобно иметь две функции _ и ngettext. Давайте это исправим! :) Я отправил pull-request’ы в babel репозиторий и в tornado-babel. Возможно, они уже приняты.  Однако, чтобы не ждать, есть готовые версии с этими исправлениями. Но вначале удалим текущие babel и tornado-babel: pip uninstall babelpip uninstall tornado-babelСтавим исправленные версии: pip install https://github. com/st4lk/babel/archive/2. 1. 2-draft. tar. gzpip install https://github. com/st4lk/tornado-babel/archive/0. 3b. tar. gzДобавим --keyword=_1,2 в makemessages. sh.  Было: --keyword=_ --mapping=${babel_config}Стало: --keyword=_ --keyword=_:1,2 --mapping=${babel_config}Уберем теперь уже не нужную функцию ngettext. app. py: class HomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    # . . .     self. render( home. html , text=_( Hello, world! ), count=count)home. html: &lt;div&gt;{{_( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)}}&lt;/div&gt;Ура, теперь мы работаем с babel точно так же, как и с xgettext! Если что, код примера на github’e: https://github. com/st4lk/tornado_i18n_example "
    }, {
    "id": 11,
    "url": "https://st4lk.github.io/blog/2015/01/18/timestamp-objectid-mongodb.html",
    "title": "Timestamp и ObjectId в mongoDB",
    "body": "2015/01/18 -  У каждой записи в mongoDB есть поле _id, которое должно быть уникальным в коллекции. По умолчанию тип этого поля - ObjectId, и оно присваивается автоматически, если поле не заполнено при сохранении. Давайте рассмотрим, что из себя представляет тип ObjectId. Это 12 байт, которые состоят из:  4 байта, содержащие количество секунд с начала Unix эпохи 3 байта, содержащие идентификатор устройства 2 байта, содержащие id процесса 3 байта, содержащие счетчик, который стартует со случайного значенияКак видим, первые 4 байта содержат дату создания, и ее можно использовать:  Сортируя по полю _id мы получаем документы в порядке их создания Мы можем получить время создания документа, имея только поле _id. Но надо иметь в виду, что эта дата создания доступна с точностью до секунды. Если два документа созданы в течение одной секунды, то их порядок при сортировке по _id не определен. Т. о. если нам достаточна секундная точность, то можем НЕ создавать поля наподобие created_at: {  created_at: ISODate( 2015-01-18T12:07:47. 036Z )  // остальные поля}т. к. дата создания содержится в поле _id. Получение даты из ObjectId: В mongoDB shell дату можно получить с помощью метода getTimestamp(): &gt; db. users. findOne(). _id. getTimestamp()ISODate( 2015-01-18T09:07:47Z )А в коде питона - с помощью аттрибута generation_time &gt;&gt;&gt; from pymongo import MongoClient&gt;&gt;&gt; db = MongoClient(). db_name&gt;&gt;&gt; user = db. users. find_one()&gt;&gt;&gt; user['_id']. generation_timedatetime. datetime(2015, 1, 18, 9, 7, 47, tzinfo=&lt;bson. tz_util. FixedOffset object at 0x10e758d50&gt;)Дата возвращается в UTC, причем в питоне это aware datetime c таймзоной. На всякий случай, в этих примерах я использовал такие версии:  mongoDB v2. 6. 6 pymongo v2. 7. 2P. S. спасибо @eugeneglybin за наводку. "
    }, {
    "id": 12,
    "url": "https://st4lk.github.io/blog/2014/12/15/set-url-for-tornado-handlers.html",
    "title": "Задание url для обработчиков Tornado",
    "body": "2014/12/15 -  В tornado, для привязки обработчиков к url’ам, можно передать список из кортежей (url regex, handler) при инициализации приложения: application = tornado. web. Application([  (r / , MainHandler),  (r /some/path/page/(?P&lt;pk&gt;[0-9]+)$ , PageHandler),])Но не секрет, что гораздо удобнее использовать обертку tornado. web. url, которая позволяет присваивать имена для путей (похожа на django’вский url). Однако, в паре рабочих проектах, с которыми приходилось работать, эта обертка не использовалась. А так же в некоторых примерах из tornado документации (раз, два, три) тоже используются обычные кортежи, без url, что может сбить с толку. Поэтому считаю полезным описать те преимущества, которые дает эта обертка. Итак, какие неудобства мы испытываем при работе без использования url. Без url(): Чтобы в коде или в шаблоне воспроизвести нужный путь, приходится вручную вводить строку. Пример app. py: import tornado. ioloopimport tornado. webclass MainHandler(tornado. web. RequestHandler):  def get(self):    self. render( home. html , title= My title , pages=[1, 2, 3])class PageHandler(tornado. web. RequestHandler):  def get(self, page_n):    email_text =  Please visit this page: '/some/path/page/{page_n}/' . format(      page_n=1)    send_email('some@person. com', email_text)    self. render( page. html , title= Page , page_n=page_n)application = tornado. web. Application([  (r / , MainHandler),  (r /some/path/page/(?P&lt;page_n&gt;[0-9]+)/$ , PageHandler),])if __name__ ==  __main__ :  application. listen(8888)  tornado. ioloop. IOLoop. instance(). start()home. htm: &lt;html&gt;  &lt;head&gt;   &lt;title&gt;{{ title }}&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;   &lt;div&gt;View pages:&lt;/div&gt;   &lt;ul&gt;    {% for page_n in pages %}     &lt;li&gt;&lt;a href= /some/path/page/{{ page_n }}/ &gt;{{ page_n }}&lt;/a&gt;&lt;/li&gt;    {% end %}   &lt;/ul&gt;  &lt;/body&gt; &lt;/html&gt;page. html: &lt;html&gt;  &lt;head&gt;   &lt;title&gt;{{ title }}&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;   &lt;div&gt;You are viewing page #{{ page_n }}&lt;/div&gt;   &lt;div&gt;Back to &lt;a href= / &gt;Home&lt;a&gt;&lt;/div&gt;  &lt;/body&gt; &lt;/html&gt;Видим, что даже в этом простом коде пришлось трижды повторять путь /some/path/page/. А если нам понадобится чуть-чуть изменить эту строку? Придется делать автозамену, что неудобно и чревато ошибками. К тому же, некоторые пути могут быть громоздкими, что ухудшает читабельность кода. Используя url(): Этот же пример, но с оберткой url: app. py: import tornado. ioloopimport tornado. webfrom tornado. web import urlclass MainHandler(tornado. web. RequestHandler):  def get(self):    self. render( home. html , title= My title , pages=[1, 2, 3])class PageHandler(tornado. web. RequestHandler):  def get(self, page_n):    email_text =  Please visit this page: '{url}' . format(      url=self. reverse_url('page', 1))    send_email('some@person. com', email_text)    self. render( page. html , title= Page , page_n=page_n)application = tornado. web. Application([  url(r / , MainHandler, name= home ),  url(r /some/path/page/(?P&lt;page_n&gt;[0-9]+)/$ , PageHandler, name= page ),])if __name__ ==  __main__ :  application. listen(8888)  tornado. ioloop. IOLoop. instance(). start()home. html: &lt;html&gt;  &lt;head&gt;   &lt;title&gt;{{ title }}&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;   &lt;div&gt;View pages:&lt;/div&gt;   &lt;ul&gt;    {% for page_n in pages %}     &lt;li&gt;&lt;a href= {{reverse_url('page', page_n)}} &gt;{{ page_n }}&lt;/a&gt;&lt;/li&gt;    {% end %}   &lt;/ul&gt;  &lt;/body&gt; &lt;/html&gt;page. html: &lt;html&gt;  &lt;head&gt;   &lt;title&gt;{{ title }}&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;   &lt;div&gt;You are viewing page #{{ page_n }}&lt;/div&gt;   &lt;div&gt;Back to &lt;a href= {{reverse_url('home')}} &gt;Home&lt;a&gt;&lt;/div&gt;  &lt;/body&gt; &lt;/html&gt;Путям присвоены значащие имена, которые используются для воспроизведения url’a с помощью метода reverse_url. Если нам нужно будет подправить какой-либо путь, то мы сделаем это лишь в одном месте. Гораздо удобнее! "
    }, {
    "id": 13,
    "url": "https://st4lk.github.io/blog/2014/11/02/django-celery-setup.html",
    "title": "Подключение celery к django",
    "body": "2014/11/02 -  Для подключения celery к новому проекту так или иначе приходится подглядывать в предыдущие, чтобы вспомнить необходимые шаги: какие настройки задавать, как запускать, как останавливать и т. д. Хочу собрать все в одном месте. Что нужно получить в итоге:  Посредством celery добавить возможность django проекту выполнять задачи в фоне, чтобы не загружать текущий python процесс. Пример таких задач: отправка емейлов, работа со сторонним апи, долгие вычисления и т. д.  В качестве брокера используем redis.  В админке нужно видеть все запущенные и выполненные задачи.  В админке нужно видеть статус текущих воркеров celery (online/offline). Подключаем celery: Установим redis: Для того, чтобы процессы django и celery могли общаться между собой, нужен посредник (broker), который будет передавать сообщения. В качестве этого брокера будем использовать redis. Это распространенное решение, redis быстр, легко устанавливается, требует мало памяти, надежен. Список всех возможных посредников можно посмотреть здесь. На всякий случай проверим сервер (все примеры на ubuntu): sudo apt-get updatesudo apt-get install build-essentialsudo apt-get install tcl8. 5Скачиваем последнюю версию отсюда. На момент написания статьи это версия 2. 8. 17 wget http://download. redis. io/releases/redis-2. 8. 17. tar. gztar xzf redis-2. 8. 17. tar. gzcd redis-2. 8. 17makemake testsudo make installcd utilssudo . /install_server. shЗапускаем redis сервер sudo service redis_6379 startЕсли что, остановить его можно так sudo service redis_6379 stopЧтобы redis запускался при загрузке системы, выполним команду sudo update-rc. d redis_6379 defaultsТак же нам понадобится python драйвер для redis, установим его: pip install redisУстановка и настройка django-celery: В принципе, сelery и django можно подружить не используя специальной библиотеки, следуя инструкциям из документации. Однако, для удобной интеграции celery в админку django проще установить специальное приложение django-celery (см. почему). pip install django-celeryДобавляем такие настройки в settings. py: INSTALLED_APPS += ( djcelery , )# адрес redis сервераBROKER_URL = 'redis://localhost:6379/0'# храним результаты выполнения задач так же в redisCELERY_RESULT_BACKEND = 'redis://localhost:6379/0'# в течение какого срока храним результаты, после чего они удаляютсяCELERY_TASK_RESULT_EXPIRES = 7*86400 # 7 days# это нужно для мониторинга наших воркеровCELERY_SEND_EVENTS = True# место хранения периодических задач (данные для планировщика)CELERYBEAT_SCHEDULER =  djcelery. schedulers. DatabaseScheduler # в конец settings. py добавляем строчкиimport djcelerydjcelery. setup_loader()Создаем таблицы в базе. Eсли используем south, то $ python manage. py migrate djceleryА если нет, то обычный syncdb $ python manage. py syncdbСоздание задач: Задачи создаются в файле tasks. py, который нужно положить в папочку приложения: - proj/ - proj/__init__. py - proj/settings. py - proj/urls. py- users/ # some app - users/__init__. py - users/models. py - users/views. py - users/tasks. py # задачи для приложения users кладем сюда- products/ - products/__init__. py - products/models. py - products/views. py - products/tasks. py # задачи для приложения products кладем сюда- manage. pyСоздадим простейшую задачу. users/tasks. py: # -*- coding: utf-8 -*-from celery. task import task@task(ignore_result=True, max_retries=1, default_retry_delay=10)def just_print():  print  Print from celery task Запуск задач: ОтладкаДля проверки работы задач запускаем  python manage. py runserver # проект django python manage. py celery worker --concurrency=1 # celery worker: процесс, который будет выполнять задачи python manage. py celery beat # celery beat: процесс, который будет запускать периодические задачиПоследние две команды можно объединить в одну (ключик -B): python manage. py celery worker -B --concurrency=1Попробуем запустить нашу задачу just_print. Условно можно выделить 2 способа вызова задачи:  Планировщиком задач, вызывать через определенный интервал времени (например, каждые 10 секунд) или в определенное время (аналогично crontab) Из кода, в нужном месте и при нужных условияхВызов задачи планировщикомЗаходим в админку по адресу http://{host}/admin/djcelery/periodictask/ и нажимаем “Add periodic task”. Заполняем поля как на фото ниже и сохраняем.  Для указания времени запуска, вместо интервала, делаем все то же самое, что и в предыдущем случае, только вместо Interval указываем Crontab: ЗамечаниеПериодические задачи можно создать автоматически при запуске проекта (при запуске процесса celery), чтобы не делать этого через админку вручную (но в админке они так же будут видны). Для этого их нужно указать в settings. py. Каждые 10 секунд: CELERYBEAT_SCHEDULE = {  'example-task': {    'task': 'apps. users. tasks. just_print',    'schedule': 10, # в секундах, или timedelta(seconds=10)  },}Раз в минуту (задача будет запускаться в 0 секунд каждой минуты): from celery. schedules import crontabCELERYBEAT_SCHEDULE = {  'example-task': {    'task': 'apps. users. tasks. just_print',    'schedule': crontab(),  },}Или например каждую 7-ю минуту каждого часа: from celery. schedules import crontabCELERYBEAT_SCHEDULE = {  'example-task': {    'task': 'apps. users. tasks. just_print',    'schedule': crontab(minute=7),  },}Подробности про создание периодических задач в settings. py в документации celery. Так же, можно пометить функцию декоратором @periodic_task вместо @task, и эта задача станет периодической. Период задается аргументом run_every, в качестве значения ему передается то же самое, что и для ключа ‘schedule’ в CELERYBEAT_SCHEDULE: from celery. task import periodic_task@periodic_task(ignore_result=True, run_every=10) # 10 секунд, или timedelta(seconds=10)def just_print():  print  Print from celery task или crontab from celery. task import periodic_taskfrom celery. schedules import crontab@periodic_task(ignore_result=True, run_every=crontab()) # раз в минутуdef just_print():  print  Print from celery task Вызов задачи из кодаДля вызова задачи из кода используется метод . delay(). Например, из view: from . tasks import just_printclass UserListView(ListView):  model = User  def get_context_data(self, **kwargs):    just_print. delay()    return super(UserListView, self). get_context_data(**kwargs)Мониторинг: В админке в секции Djcelery видим строки Tasks и Workers.  Однако, сейчас они пустые. Чтобы там была информация о текущем состоянии воркеров и задач, нужно запустить celerycam: python manage. py celerycam --frequency=10. 0Теперь видим, что у нас работает 1 воркер: И видим статус выполнения задач: По умолчанию, celerycam удаляет старые задачи из Tasks по таким правилам:  сборщик запускается с интервалом 1 час (см. код celery 3. 1, способа поменять этот интервал из настроек не нашел) в каждом вызове сборщик удаляет задачи, превышающие установленное время жизни. Время жизни отчетов по задачам можно задать настройками в settings. py (ниже приведены значения по умолчанию): from datetime import timedeltaCELERYCAM_EXPIRE_SUCCESS = timedelta(days=1)CELERYCAM_EXPIRE_ERROR = timedelta(days=3)CELERYCAM_EXPIRE_PENDING = timedelta(days=5)Запуск в продакшн: В продакшне процессы celery должны быть daemon’ами. Для запуска/остановки всех процессов celery можно написать отдельный bash скрипт, а можно запускать с помощью supervisor. Итак, по порядку. Bash скриптыЗапуск, celery_start. sh: #!/bin/bashPYTHON=/path/to/bin/pythonPROJECT_FOLDER=/project_dir/project/PID_FOLDER=/path/to/pid/LOGS_FOLDER=/path/to/logs/BEAT_SHEDULE_FILE=/path/to/shedule/celerybeat-schedule # celery beat need to store the last run times of the tasks in a local database file$PYTHON ${PROJECT_FOLDER}manage. py celery worker --concurrency=1 --detach --pidfile=${PID_FOLDER}celery_worker. pid --logfile=${LOGS_FOLDER}celery_worker. log$PYTHON ${PROJECT_FOLDER}manage. py celery beat --detach --pidfile=${PID_FOLDER}celery_beat. pid --logfile=${LOGS_FOLDER}celery_beat. log -s ${BEAT_SHEDULE_FILE}$PYTHON ${PROJECT_FOLDER}manage. py celerycam --frequency=10. 0 --detach --pidfile=${PID_FOLDER}celerycam. pid --logfile=${LOGS_FOLDER}celerycam. logОстановка, celery_stop. sh: #!/bin/bashPYTHON=/path/to/bin/pythonPID_FOLDER=/path/to/pid/$PYTHON -m celery multi stopwait worker1 --pidfile=${PID_FOLDER}celerycam. pid$PYTHON -m celery multi stopwait worker1 --pidfile=${PID_FOLDER}celery_beat. pid$PYTHON -m celery multi stopwait worker1 --pidfile=${PID_FOLDER}celery_worker. pidsupervisord: Лучше всего запустить celery процессы под контролем supervisord. Для этого создаем где-нибудь у себя в проекте такие файлы (например, в папке deploy): supervisor. celeryd. conf [program:djangoproject. celeryd]command=/path/to/bin/python /path/to/django_project/manage. py celeryd --concurrency=1user=www-datanumprocs=1directory=/path/to/django_projectstdout_logfile=/path/to/log/celery_worker. logstderr_logfile=/path/to/log/celery_worker. logautostart=trueautorestart=truestartsecs=10stopwaitsecs = 120priority=998supervisor. celerybeat. conf [program:djangoproject. celerybeat]command=/path/to/bin/python /path/to/django_project/manage. py celery beat -s /path/to/celerybeat-scheduleuser=www-datanumprocs=1directory=/path/to/django_projectstdout_logfile=/path/to/log/celery_beat. logstderr_logfile=/path/to/log/celery_beat. logautostart=trueautorestart=truestartsecs=10stopwaitsecs = 120priority=998supervisor. celerycam. conf [program:djangoproject. celerycam]command=/path/to/bin/python /path/to/django_project/manage. py celerycam --frequency=10. 0user=www-datanumprocs=1directory=/path/to/django_projectstdout_logfile=/path/to/log/celerycam. logstderr_logfile=/path/to/log/celerycam. logautostart=trueautorestart=truestartsecs=10stopwaitsecs = 120priority=998Заменяем все /path/to/ на нужные для конкретного проекта, а так же указываем нужного юзера, под которым будут запускаться процессы celery. Теперь создадим symlink на наши файлы конфигурации в папке /etc/supervisor/conf. d/, чтобы supervisor знал о них: cd /etc/supervisor/conf. dsudo ln -s /path/to/django_project/deploy/supervisor. celeryd. confsudo ln -s /path/to/django_project/deploy/supervisor. celerybeat. confsudo ln -s /path/to/django_project/deploy/supervisor. celerycam. confИ перезапустим supervisor sudo supervisorctl reloadПроверим, что все нужные процессы запущены: ps aux | grep pythonПрочее: Если в админке зайти в периодические задачи (http://{host}/admin/djcelery/periodictask/), то увидим там celery. backend_cleanup: Эта задача подчищает все устаревшие результаты задач, которые хранятся в базе данных. Устаревшие - это те, которые старше указанного нами интервала времени в settings. CELERY_TASK_RESULT_EXPIRES. Но, т. к. результаты задач мы храним в redis, а не в базе данных, то данная периодическая задача нам не важна. Т. о. ее можно удалить. Redis сама удаляет те значения, время жизни которых истекло. "
    }, {
    "id": 14,
    "url": "https://st4lk.github.io/blog/2014/09/09/nested-sql-queries-django.html",
    "title": "Вложенные SQL запросы в Django",
    "body": "2014/09/09 -  Вы знали, что Django ORM умеет делать вложенные SQL запросы? К своему стыду я узнал это не так давно. Допустим, у нас есть такие модели питомника (Nursery) и питомца (Pet): class Nursery(models. Model):  title = models. CharField(max_length=50)class Pet(models. Model):  name = models. CharField(max_length=50)  nursery = models. ForeignKey(Nursery, related_name='pets')Нам нужно получить всех питомцев (Pet), которые находятся в заданных питомниках (Nursery). Например в питомниках, title который начинается с “Moscow”: nurseries = Nursery. objects. filter(title__startswith= Moscow )Pet. objects. filter(nursery__in=nurseries)Эти строчки сделают лишь один запрос к базе данных, в котором будет вложенный запрос : SELECT  users_pet .  id ,  users_pet .  name ,  users_pet .  nursery_id  FROM  users_pet  WHERE  users_pet .  nursery_id  IN (SELECT  users_nursery .  id  FROM  users_nursery  WHERE  users_nursery .  title  LIKE Moscow%)Однако, следует иметь в виду, что хотя выполняется один запрос к БД, это не всегда означает лучшую производительность. Тут все зависит от выбранной базы данных. Например, как советуют в документации django, в случае MySQL более эффективно выполнить два запроса вместо одного, т. к. эта БД не всегда хорошо оптимизирует вложенные запросы. Т. е. для MySQL такой код будет эффективнее (судя по докам django): nurseries = Nursery. objects. filter(title__startswith= Moscow ). values_list('pk', flat=True)Pet. objects. filter(nursery__in=list(nurseries))несмотря на то, что выполнится два запроса: SELECT  users_nursery .  id  FROM  users_nursery  WHERE  users_nursery .  title  LIKE Moscow%SELECT  users_pet .  id ,  users_pet .  name ,  users_pet .  nursery_id  FROM  users_pet  WHERE  users_pet .  nursery_id  IN (1, 2)"
    }, {
    "id": 15,
    "url": "https://st4lk.github.io/blog/2014/05/29/new-relic-free-shirt.html",
    "title": "Бесплатная футболка от New Relic",
    "body": "2014/05/29 -  New Relic - сервис для мониторинга веб приложения. Позволяет в подробных деталях смотреть статистику работы, где программа тратит больше всего времени, как часто обращается к базе данных и много всего прочего. Для описания этого прекрасного сервиса следует уделить отдельный пост. Здесь же хочу рассказать о другом - как я получил бесплатную футболку от New Relic. На самом деле, для этого нужно лишь зарегистрироваться и настроить New Relic на работающем сайте. Как и сказано в их предложении. Конечно, когда я настраивал newrelic на одном из сайтов, у меня не было цели получить футболку. Об этом я узнал случайно. Ради интереса ввел свой адрес, ожидая, что в Россию из Америки не будут просто так ничего высылать. Как ни странно, никаких предупреждений не было. Более того, через несколько дней пришло письмо с подтверждением, что начата отгрузка посылки. Было приятно, но все же ожидал, что через какое-то время мне сообщат об отмене. К моему удивлению где-то через 20 дней в почтовом ящике нашел такое извещение: На почте получил пакет: Ну а в пакете прикольная футболка:  "
    }, {
    "id": 16,
    "url": "https://st4lk.github.io/blog/2014/05/09/remote-url-localhost-server.html",
    "title": "Внешний url для localhost сервера",
    "body": "2014/05/09 -  Есть замечательная тулза под названием ngrok. Она позволяет привязать URL для вашего localhost сервера! Например, вы запускаете тестовый сервер django у себя на компьютере:  python manage. py runserverи этот сервер будет доступен через внешний URL. Для чего?: Как минимум есть такие задачи:  продемонстрировать проект заказчику проверить интеграцию вашего сайта с платежной системой, которая отправляет уведомления. Например paypal, где для получения IPN сообщения нужен работающий URL, даже в sandboxКак:  Скачиваем ngrok отсюда Распаковываем скаченный архив Запускаем тестовый django сервер (по умолчанию это 8000 порт) Запускаем ngrok:  . /ngrok 8000    В коносле видим примерно следующее:  ngrokTunnel Status         onlineVersion            1. 6/1. 6Forwarding          http://51c85c8a. ngrok. com -&gt; 127. 0. 0. 1:8000Forwarding          https://51c85c8a. ngrok. com -&gt; 127. 0. 0. 1:8000Web Interface         127. 0. 0. 1:4040# Conn            0Avg Conn Time         0. 00ms   Теперь заходим по адресу http://51c85c8a. ngrok. com и видим наш сервер! Улучшения: Не совсем удобно, что при каждом запуске ngrok будет присваивать новый url вида ********. ngrok. com. Но можно присвоить свой поддомен и сервер будет доступен по этому адресу. Для этого надо лишь:  зарегистрироваться получить auth token   один раз указать ngrok полученный auth token (после первого запуска создается файл ~/. ngrok, в котором сохраняется введенный token):   . /ngrok -authtoken your_auth_token 8000      теперь можно запускать так:   . /ngrok -subdomain=mysupersite 8000   Теперь наш локальный сервер доступен по адресу http://mysupersite. ngrok. com "
    }, {
    "id": 17,
    "url": "https://st4lk.github.io/blog/2014/03/14/send-email-django-project-mandrill-service.html",
    "title": "Отправка писем в django проекте с помощью сервиса mandrill",
    "body": "2014/03/14 -  Отправлять email сообщения с сервера можно просто по SMTP протоколу. Но есть другой способ - через специальные сервисы рассылки. Про один такой, mandrill. com, я немного расскажу. Преимущества относительно SMTP:  Подробная статистика отправленных писем. Сколько писем отправлено, кому, когда. Сколько писем открыли, какие ссылки нажимали.  Шаблоны писем. Их можно редактировать через сервис mandrill, т. о. образом не нужно ничего придумывать в админке django. В шаблоне можно использовать переменные, задавать тему письма, даже адрес отправителя. Из django вы просто указываете, какой шаблон использовать и передаете нужные переменные.  Не нужен свой почтовый сервис. И не обязательно подключать свой домен к яндекс/гугл почте (но все же лишним не будет, это удобно).  Есть бесплатный тариф, который позволяет отправить 12000 писем в месяц. Недостатки:  Для подсчета статистики все ссылки в вашем письме будут заменены на специальные редиректы. Обычный пользователь скорее всего ничего не заметит.  В некоторых почтовых клиентах (например gmail в браузере) в поле адресат вместе с указанным вами адресом-отправителем будет так же указан истинный адрес mandrill. Однако можно настроить DNS записи DKIM и SPF для вашего домена, тогда адрес отправителя будет отображаться корректно везде.  Если нужно отправить больше 12000 писем в месяц - нужно купить соответствующий тариф. По-моему, преимущества превышают недостатки. Подключаем:    Регистриуемся https://mandrill. com/signup/     Создаем API_KEY:    Создаем шаблон для письма: 3. 1. Вводим имя шаблона (например template-1):  Создаем шаблон письма: Как выглядит статистика:  График отправления по времени:  График открытия писем и кликов по ссылкам:  Список всех отправленных писем:  Статистика кликов по каждой ссылке: Интеграция с django: В gist’e приведены примеры интеграции с django, а также использования просто из python скрипта. "
    }, {
    "id": 18,
    "url": "https://st4lk.github.io/blog/2014/03/07/mongodb-indexes.html",
    "title": "Что нужно знать об индексах в mongodb",
    "body": "2014/03/07 -  Недавно закончил курс “M101P: MongoDB for Developers” (он периодически повторяется, например следующий стартует в апреле). В процессе прохождения натолкнулся на некоторые интересные моменты. 1. Выбор индекса для запроса. : Допустим у нас коллекция с такими данными: {  _id  : . . . ,  a  : 81810,  b  : 97482,  c  : 44288 }{  _id  : . . . ,  a  : 11734,  b  : 27893,  c  : 19485 }// и т. д. Всего 99999 объектов. У коллекции есть индексы: db. foo. ensureIndex({a: 1, b: 1, c: 1})db. foo. ensureIndex({c: -1})Вопрос: какой индекс будет использован при запросе db. foo. find({'a':{'$lt':10000}, 'b':{'$gt': 5000}}). sort({'c':-1}) ? Интуитивно очень хотелось бы, чтобы был использован индекс {a: 1, b: 1, c: 1}, ведь вроде бы он покрывает все нужные нам поля. Но, к сожалению, это не так. Во-первых, индекс {a: 1, b: 1, c: 1} в этом случае не может быть использован одновременно для find и для sort, т. к. find содержит операторы сравнения ($lt, $gt). Т. е. в таком запросе db. foo. find({'a': 1, 'b': 2}). sort({'c':-1})индекс был бы использован полностью. Но, к сожалению, у нас не такой запрос. Ну ладно, бог с ней с сортировкой, наверно индекс {a: 1, b: 1, c: 1} будет использован только для find, а сортировка уже будет сделана без индексов. Эх, смотрим: db. foo. find({'a':{'$lt':10000}, 'b':{'$gt': 5000}}). sort({'c':-1}). explain(){   cursor  :  BtreeCursor c_-1 ,   n  : 9498,   nscanned  : 99999,   scanAndOrder  : false,  // другие поля не так интересны}{a: 1, b: 1, c: 1} даже не был задействован, вместо него индекс {c: -1} был использован для сортировки, потому что так решил mongodb’шный query optimizer. Вот где пригодится принудительный выбор индексов оператором $hint: db. foo. find({'a':{'$lt':10000}, 'b':{'$gt': 5000}}, {'a':1, 'c':1}). sort({'c':-1}). hint({a: 1, b: 1, c: 1}). explain(){   cursor  :  BtreeCursor a_1_b_1_c_1 ,   n  : 9498,   nscanned  : 9974,   scanAndOrder  : true,  // другие поля не так интересны}Сейчас индекс использовался для find, а сортировка осуществилась без индексов. Думаю, использовать индекс для фильтрации 9498 элементов из 99999 и затем к ним применить сортировку намного эффективнее нежели применить полный перебор к 99999 элементам и затем сортировку найденных 9498 осуществить с помощью индексов. 2. Направление индекса: Возвращаясь к предыдущему примеру, видно, что один из индексов имеет значение “-1”: db. foo. ensureIndex({c: -1})Это значит, что создан “нисходящий” индекс по полю “c”. Какое это вообще имеет значение? db. foo. find(). sort({'c':-1}). explain(){   cursor  :  BtreeCursor c_-1 ,  // . . . }db. foo. find(). sort({'c':1}). explain(){   cursor  :  BtreeCursor c_-1 reverse ,  // . . . }В обоих случаях применился индекс, только во втором случае он использовался в обратном порядке. Зачем тогда нужно указывать “направление” индекса? Направление индекса пригодится при сортировке по двум и более полям: db. foo. ensureIndex({a:1, b:1, c:1})// тут индекс не может быть использованdb. foo. find(). sort({a:-1, b:1}). explain(){   cursor  :  BasicCursor ,   scanAndOrder  : true,  // . . . }// а тут - можетdb. foo. find(). sort({a:1, b:1}). explain(){   cursor  :  BtreeCursor a_1_b_1_c_1 ,   scanAndOrder  : false,}// но для сортировки по одному полю направление индекса не важно:db. foo. find(). sort({a:1}). explain(){   cursor  :  BtreeCursor a_1_b_1_c_1 ,   scanAndOrder  : false,}db. foo. find(). sort({a:-1}). explain(){   cursor  :  BtreeCursor a_1_b_1_c_1 reverse ,   scanAndOrder  : false,}Т. е. правило такое: при сортировке по двум и более полям направление сортировки должно совпадать с направлением индекса для всех полей. 3. Индексы и aggregation: Aggregation - очень, очень классная функция в mongodb. Было бы крайне полезно получать explain данные и об aggregation запросах. Такая возможность появится в версии 2. 6 (на момент написания статьи эта версия официально еще не вышла). Вот как можно будет применить explain в версии 2. 6: db. foo. aggregate([    {$match: {a: {'$lt':10000}, b: {'$gt': 5000}}},    {$sort: {c: -1}},    {$group: {_id: null, a_total: {$sum:  $a }}}  ],  {explain: true})Но!Оказывается, explain можно использовать для aggregation и в текущей версии 2. 4, только эта функция не документирована!Это можно сделать не напрямую, а используя runCommand: db. foo. runCommand('aggregate', {pipeline:[  {$match: {a: {'$lt':10000}, b: {'$gt': 5000}}},  {$sort: {c: -1}},  {$group: {_id: null, a_total: {$sum:  $a }}}], explain: true})Вывод будет таким (сокращенно): {   serverPipeline  : [    {       query  : {         a  : {           $lt  : 10000        },         b  : {           $gt  : 5000        }      },       sort  : {         c  : -1      },      // . . .        cursor  : {         cursor  :  BtreeCursor c_-1 ,         n  : 9498,         nscanned  : 99999,         scanAndOrder  : false,        // . .       },      // . . .   ],   ok  : 1}Вот правда hint к aggregation применить пока нельзя: SERVER-7944. P. S. : Кстати, всем завершившим курс “M101P: MongoDB for Developers” выдают вот такой сертификат: M101P. "
    }, {
    "id": 19,
    "url": "https://st4lk.github.io/blog/2014/01/30/async-bitcoin-rpc-client.html",
    "title": "Асинхронный Bitcoin RPC клиент на python",
    "body": "2014/01/30 - Star Для работы с Bitcoin RPC на python’е есть библиотека Python-BitcoinRPC. Но недавно мне понадобилось обратиться к API из приложения на tornado. Указанная библиотека работает в синхронном, т. е. блокирующем режиме. Для торнадо было бы намного лучше использовать асинхронную версию. Готовой найти не удалось, поэтому написал свой форк - асинхронный, который использует торнадовский AsyncHTTPClient: https://github. com/st4lk/python-bitcoinrpc-tornado. Пример (выводит текущее количество блоков в сети Bitcoin): from bitcoinrpc_async. authproxy import AsyncAuthServiceProxyfrom tornado import ioloop, genBITCOIN_RPC_URL =  http://user:password@127. 0. 0. 1:8332 @gen. coroutinedef show_block_count():  service = AsyncAuthServiceProxy(BITCOIN_RPC_URL)  result = yield service. getblockcount()  print resultio_loop = ioloop. IOLoop. instance()io_loop. add_callback(show_block_count)io_loop. start()"
    }, {
    "id": 20,
    "url": "https://st4lk.github.io/blog/2013/12/29/tornado-web-application-example.html",
    "title": "Tornado: пример веб приложения",
    "body": "2013/12/29 - Star Tornado - асинхронный веб фреймворк для python’а. Вначале я приведу краткий перечень плюсов и минусов tornado, а потом расскажу о типовом веб проекте с использованием этого инструмента. Плюсы tornado: В плюсах и минусах я буду приводить свое личное ощущение по сравнению с django. 1. Асинхронность. : Торнадо представляет из себя бесконечный цикл (ioloop), который постоянно проверяет наличие событий. Все это происходит в одном потоке. К примеру кто-то обратился по адресу /home/. Допустим в качестве обработчика этого события зарегистрирован HomeHandler (handler в торнадо примерно тоже самое, что view в django). ioloop вызывает HomeHandler и начинается выполняться его код. Что происходит в это время с ioloop? Он блокируется. Eсли обратиться другой пользователь, он будет ждать, пока обработается предыдущее событие. В чем смысл тогда? А в том, что есть специальный механизм callback’ов. Для выполнения долгих действий мы “говорим”: выполни эту операцию (например, обращение к БД или внешний http запрос), а когда закончишь, вызови вот эту функцию. И тогда, после регистрации callback’a ioloop продолжает работать, обрабатывая другие события. В какой-то момент наша долгая задача выполниться и вызовет наш зарегистированный callback. Однако, стоит иметь в виду, функция сама по себе должна поддерживать асинхронность, если сказать “выполни time. sleep(10) и вызови этот callback”, то ioloop все равно будет заблокирована из-за time. sleep. Поэтому для асинхронных задач используются специальные функции, библиотеки. Но все же, какое преимущество дает только один запущенный поток? Чем плохо на каждый запрос создать новый процесс или поток? А в том, что создание потока, а тем более процесса - довольно дорогостоящая операция с точки зрения ресурсов компьютера. Представим, что на каждый запрос мы создаем новый поток. Тогда если обратяться одновременно 1000 пользователей, то мы получим 1000 потоков. Серверу придется очень тяжело, скорее всего он не справится. Конечно, мы всегда ограничиваем максимальное количество потоков и процессов, тогда следующий пользователь будет ждать, когда освободиться какой-либо из потоков. Представим проблему посложнее - создать онлайн чат. Когда кто-то пишет в чате, все должны об этом узнать. Какие варианты решения будут на django? Например, опрашивать каждым участником по ajax о наличии нового сообщения скажем раз в 5 секунд. Такое решение быстро исчерпает все ресурсы сервера при увеличении количества участников. На каждый ajax запрос нужно создавать новое соединение и новый поток и делать это постоянно, а этого дорого. Есть вариант с keep-alive, когда мы постоянно держим соединение открытым, не обрываем его. Но это прямая дорога к C10K проблеме. Т. е. мы же не сможем держать запущенными 10000 потоков. Вот тут на помощь приходят асинхронные решения. С ними можно использовать WebSocket’ы, или те же keep-alive http запросы, в асинхронных фреймворках они не съедят все ресурсы. 2. Работа с WebSocket’ами: Это отчасти следствие асинхронности, но лучше выделить это отдельным пунктом. Про вебсокеты можно почитать тут. 3. Менее слабая зависимость от ORM и html-шаблонизатора. : For example django’s built-in ORM works only with SQL databases. If you want to connect to mongodb, you’ll К примеру django’вский встроенный ORM работает только с SQL базами данных. Если вы хотите подключить mongodb, то вам придется отказаться (либо допиливать самому) от всех готовых приложений, которые завязаны на ORM django, включая админку. Это же вас ждет, если вы хотите использовать например SQL Alchemy. Так же, если вы хотите сменить встроенный шаблонизатор на другой (Jinja2), то опять-таки, многие готовые аппы для django могут так просто не заработать. С админкой та же беда. У tornado этих проблем меньше. Но в то же время для tornado вообще меньше готовых приложений, встроенной админки тоже, как вы понимаете, нет. И да, нужно использовать асинхронный драйвер для работы с выбранной БД, чтобы использовать все преимущества асинхронности. Не для всех БД такие драйвера есть. Точно есть для mongodb, posgresql. Для mysql - не знаю. Минусы tornado: 1. Меньшая популярность (чем у django). : Это означает, что многое из того, что есть готового для django, для tornado придется делать самому. Админку например. 2. Сложность кода. : Как ни крути, но асинхронный код писать сложнее, чем синхронный. Т. е. более высокий порог вхождения. Проект типового приложения на tornado: Здесь https://github. com/st4lk/acl_webapp. Преставляет из себя ACL приложение, т. е. приложение с правами доступа. Права основаны на модели: у каждого юзера есть поле permissions: permissions = {  'model_name_1': ['read',],  'model_name_2': ['read', 'write'],  'model_name_3': ['read', 'write', 'delete'],}В данном случае у пользователя есть права “только чтение” для модели model_name_1, “читать и записывать” для model_name_2, и “читать, записывать, удалять” для model_name_3. Проект следует структуре django: есть приложения (apps), каждое из которых выполняет определеную функцию. Вот пример приложений: accountsnewspagesи т. д. Каждое приложение содержит модели, хендлеры, формы. Базовые хендлеры:  ListHandler DetailHandler CreateHandler DeleteHandler и т. д. Все настройки определены в settings. py. Использумые технологии:  mongodb (база данных) motor (асинхронный драйвер для БД) schematics (построение абстрактных моделей БД) WTForms (формы) Jinja2 (html шаблоны)P. S. Возможно, я где-то ошибся в понимании tornado. Буду рад, если вы сообщите об этом или о чем-то другом в комментариях! "
    }, {
    "id": 21,
    "url": "https://st4lk.github.io/blog/2013/11/13/django-project-beginning-working-server.html",
    "title": "Django проект: с нуля до работающего сервера",
    "body": "2013/11/13 - В этом посте хочу описать свой опыт создания и запуска django проекта на VPS-хостинге. Разделим задачу на этапы:  Создание django-проекта Настройка деплоя Настройка сервера (установка нужных библиотек) Деплой проектаСоздание django-проекта: Итак. Пусть наш проект называется myproject. Создадим его на локальном компьютере. Будем считать, что python у нас в системе установлен. Для начала нам понадобятся pip и virtualenv (если еще не стоят). Создаем виртуальное окружение: virtualenv venv-myprojectАктивируем его: # on *nix systemsource venv-myproject/bin/activateREM on windowsvenv\Scripts\activate. batДалее нужно создать заготовку (минимально необходимые файлы проекта). Самый простой вариант - установить django и использовать команду django-admin. py startproject. Так создастся совсем базовый набор файлов. Нужно будет еще много вещей доделывать руками, это долго. Лучше иметь свой шаблон, где уже определены все наиболее часто используемые настройки, библиотеки. Для этих целей существуют разные утилиты, мне нравится cookiecutter. Я взял за основу cookiecutter шаблон для django и создал свой: https://github. com/st4lk/cookiecutter-django. Для начала установим cookiecutter: pip install cookiecutterТеперь создадим django проект из шаблона: cookiecutter https://github. com/st4lk/cookiecutter-django. gitПосле этого в консоле будут заданы вопросы (название проекта, имя и т. д. ) и создание заготовки проекта готово. Работаем над проектом локально, тестим его. И вот пришло время запустить проект на сервере. Но наш сервер пока представляет из себя голую операционную систему, в которую необходимо установить утилиты и библиотеки для работы веб приложения. Я буду описывать сервер с ОС Ubuntu. Настройка деплоя: Решил осветить этот вопрос до настройки сервера, т. к. способ деплоя будет влиять на все дальнейшие действия. Как-то я принимал проект, который использовал утилиту makesite. Она мне понравилась, использую ее и сейчас для некоторых проектов. Будет использоваться связка nginx, uwsgi, supervisor. Cсылки:  проект на github: https://github. com/klen/makesite описание процесса от создателя утилиты: http://klen. github. io/deploy-setup-ru. htmlmakesite умеет создавать, обновлять, удалять проект на сервере. При создании проекта makesite создает все необходимое окружение автоматически, основываясь на специальных шаблонах. В окружение входит: создание virtualenv, настройка nginx, uwsgi, supervisor, инициализация БД. Т. е. все необходимое, чтобы проект работал. Команда создания проекта: makesite install &lt;project_name&gt;где &lt;project_name&gt; - название нашего проекта. Позднее мы будем пользоваться этим названием для обновления. Процесс обновления (деплоя) выглядит так. Мы разрабатываем проект локально, коммитим. При этом мы имеем внешний репозиторий на каком-либо сервере. Это может быть github, bitbucket или же свой собственный. В какой-то момент мы решаем обновить проект на сервере. Для этого мы push’им проект в наш внешний репозиторий. Далее, заходим по ssh на наш сервер (production сервер, на котором проект работает) и вводим такую команду: makesite update &lt;project_name&gt;Все, проект обновился. Makesite обновил код на сервере из репозитория, установил новые зависимости, выполнил миграции для БД, перезапустил Nginx. Все эти действия записаны в bash скриптах makesite’а и при желании их можно изменять, удалять, добавлять. Настройка makesite:    Создаем файл makesite. ini. Назовем его “главный”, чтобы не путаться, т. к. будет еще один. Вот шаблон для него:   [Main] # режим 'продакшн' mode=project # домен нашего сервера, будет использован например для создания файла конфигурации nginx domain=example. com # имя пользователя БД postgres (для создания базы проекта) pguser=postgres # пароль пользователя БД postgres pgpassword=secret_password # имя базы данных проекта dbname=project_name # имя пользователя базы проекта dbuser=project_user # пароль пользователя базы проекта dbpassword=project_user_password # адрес процесса postgres. Скорее всего это менять не надо. pghost=127. 0. 0. 1 # адрес нашего внешнего репозитория src=git+git@bitbucket. org:username/example. git # пользователь, от которого makesite будет осуществлять свои действия src_user=root      Создаем в корневой папке нашего проекта еще один файл makesite. ini, назовем его “дополнительный”:   [Main] template=virtualenv,db-postgres,django,uwsgi django_settings=&lt;project_name. settings&gt;   где &lt;project_name. settings&gt; заменяем на путь к модулю settings, относительно файла manage. py. Здесь именно python путь, т. е. используем . , а не /. Дефолтные настройки для uwsgi, nginx и supervisor: https://github. com/klen/makesite/tree/master/makesite/templates/uwsgi/deploy. Если понадобиться их изменить, мы добавляем такие строки к текущему makesite. ini (дополнительному) [Templates]uwsgi=%(source_dir)s/deploy/makesite_templates/uwsgiСоздаем директорию ‘deploy/makesite_templates’ в нашем проекте, копируем туда папку uwsgi отсюда: https://github. com/klen/makesite/tree/master/makesite/templates. Затем ищем и изменяем нужные нам конфигурационные файлы тут: ‘deploy/makesite_templates/uwsgi/deploy/’ Итого у нас имеется  главный makesite. ini (его пока держим в сторонке, его нужно будет положить на сервер) дополнительный makesite. ini, который лежит в корне проекта если нужно, то папка deploy/makesite_templates/uwsgi с измененными настройками nginx, uwsgi, supervisor. Эта папка так же лежит в проекте. Не забываем положить под git все это добро и сделать push на внешний репозиторий. Настройка сервера: Все команды проверял для дистрибутива Ubuntu 12. 04. В качестве базы данных будет использовать postgresql. Поехали. Заходим на сервер под root’ом. Проверяем версию python: python -VДалее я предполагаю, что python версии 2. 7. # Мы зашли под root'ом# Первым делом обновим системуapt-get updateapt-get upgrade# Утилиты для сборкиapt-get install build-essentialapt-get install libpq-dev# Конфигурирование локалейlocale-gen ru_RU. UTF-8locale-gen en_US. UTF-8dpkg-reconfigure locales# Установка gitapt-get install git# Устанавливаем setup-toolsapt-get install python-setuptools python-pip# Исходники python2. 7 для сборки библиотекapt-get install python2. 7-dev# На всякий случайapt-get install python-dev# Устоновка postgresapt-get install postgresql# правим файл# /etc/postgresql/{postgres_version}/main/pg_hba. conf# строку#  local  all       postgres                peer # Меняем на#  local  all       postgres                md5 # Настройка postgressudo -u postgres psql template1# Установка пароля для юзера postgres\password postgres# !!! Вводим пароль и запоминаем его# нажимаем ctrl+d чтобы выйти# Полезности для postgresapt-get install pgagentapt-get install pgadmin3# Если нужно изменить какие-либо настройки, обновляем конфиг (только если знаем, что делаем):# /etc/postgresql/{postgres_version}/main/postgresql. conf# где {postgres_version} = версия postgres. # Перезапусаем postgres/etc/init. d/postgresql restart# Установка uwsgipip install uwsgi# Установка nginx, virtualenv, supervisoreasy_install elementtreeapt-get install python-virtualenv nginx supervisor# Скорее всего это понадобиться для работы с изображениямиapt-get install libjpeg-devpip install -I Pillow# Создаем отдельный ssh ключ на сервере, который будет использовать makesitecdtest -d . ssh || mkdir . sshcd ~/. sshssh-keygen# На вопрос  Enter file in which to save the key  пишем makesite# На остальные просто enter, т. е. как-то так: Generating public/private rsa key pair.  Enter file in which to save the key (/home/ubuntu/. ssh/id_rsa): makesite Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in makesite.  Your public key has been saved in makesite. pub. # Копируем ключ в id_rsacp makesite id_rsacp makesite. pub id_rsa. pub# ! Важно# Берем публичный ключ makesite. pub и добавляем его в список доступа нашего репозитория (который на github или bitbucket или на собственном сервере)# Описание с github: https://help. github. com/articles/generating-ssh-keys#step-3-add-your-ssh-key-to-github# Скорее всего команда 'clip' (из github описания) не будет найдена, поэтому просто копируем содержимое файла makesite. pub  ручками # Создаем директорию для хранения наших будущих сайтовmkdir /var/www# Кладем наш  главный  makesite. ini в /var/www# Устанавливаем makesitepip install makesite# Выводим и проверяем настройки bashmakesite shell -p /var/www# Записываем их в ваш ~/. bashrccat &gt;&gt; ~/. bashrc# копируем и вставляем вывод предыдущей команды# он примерно такой:# Makesite integration# ====================export MAKESITE_HOME=/var/wwwsource /usr/local/lib/python2. 7/dist-packages/makesite/shell. sh# enter# ctrl+d для выхода# Загружаем настройки для текущего сеансаsource ~/. bashrc# Если вдруг у вас есть зависимости в requirements. txt# для которых нужно скачать репозиторий с помощью mercurial# например что-то такое:# -e hg+https://bitbucket. org/psam/django-postman/@92ede1fd0c32f4e5acc6c78b25804dc047267d3e#egg=django_postman-dev# то нужно установить mercurialapt-get install mercurial# Устанавливаем проект (занимает время, может минут 5)# предварительно изменив &lt;project_name&gt; на название нашего проекта# Это название будем использовать позднее при обновлении проектаmakesite install &lt;project_name&gt;# Если вывелась строчка# OPERATION SUCCESSFUL# то все ок, если нет - смотрим ошибку# На всякий случай проверим, установились ли библиотеки из requirements. txt:cd /var/www/{project_name}/mastersource . virtualenv/bin/activatepip freeze# смотрим, установились ли зависимости из requirements. txt# Так же можно проверить, создались ли таблицы и применились ли миграции. # Убедитесь, что STATIC_ROOT и MEDIA_ROOT имеют верные значения# соответствующие папки должны быть тут:# /var/www/{project_name}/master/# если каких-то папок не хватает, то можно создать их вручную# указав владельца и группу www-data# обновим статику (предполагается, что virtualenv активирована):cd /var/www/{project_name}/master/source python manage. py collectstatic --noinput# На всякий случай перезапустим nginx/etc/init. d/nginx restartВсе, проект запущен и работает. Деплой проекта: Как уже говорилось, для обновления проекта действия такие:  делаем push в наш репозиторий заходим на сервер по ssh и вводим makesite update &lt;project_name&gt;"
    }, {
    "id": 22,
    "url": "https://st4lk.github.io/blog/2013/09/26/django-logging-settings.html",
    "title": "Настройки логов для django",
    "body": "2013/09/26 -   Gist Рассмотрим дефолтные настройки логов в django и попробуем их сделать максимально удобными. Вот что есть в settings. py после команды django-admin. py startproject project_name (django 1. 5): # A sample logging configuration. The only tangible logging# performed by this configuration is to send an email to# the site admins on every HTTP 500 error when DEBUG=False. # See http://docs. djangoproject. com/en/dev/topics/logging for# more details on how to customize your logging configuration. LOGGING = {  'version': 1,  'disable_existing_loggers': False,  'filters': {    'require_debug_false': {      '()': 'django. utils. log. RequireDebugFalse'    }  },  'handlers': {    'mail_admins': {      'level': 'ERROR',      'filters': ['require_debug_false'],      'class': 'django. utils. log. AdminEmailHandler'    }  },  'loggers': {    'django. request': {      'handlers': ['mail_admins'],      'level': 'ERROR',      'propagate': True,    },  }}Как и написано в коментарии, здесь определяется логгер, который будет отсылать сообщения на email всем админам при возникновении ошибки HTTP 500 (по сути это любое неперехваченное исключение - exception), при условии, что settings. DEBUG = False. Список email’ов определен в settings. ADMINS. Однако, это не все. Есть еще дефолтные настройки, они определены в django. utils. log. DEFAULT_LOGGING. Так они выглядят для версии 1. 5. 4 (актуальную версию можно посмотреть на github): DEFAULT_LOGGING = {  'version': 1,  'disable_existing_loggers': False,  'filters': {    'require_debug_false': {      '()': 'django. utils. log. RequireDebugFalse',    },    'require_debug_true': {      '()': 'django. utils. log. RequireDebugTrue',    },  },  'handlers': {    'console':{      'level': 'INFO',      'filters': ['require_debug_true'],      'class': 'logging. StreamHandler',    },    'null': {      'class': 'django. utils. log. NullHandler',    },    'mail_admins': {      'level': 'ERROR',      'filters': ['require_debug_false'],      'class': 'django. utils. log. AdminEmailHandler'    }  },  'loggers': {    'django': {      'handlers': ['console'],    },    'django. request': {      'handlers': ['mail_admins'],      'level': 'ERROR',      'propagate': False,    },    'py. warnings': {      'handlers': ['console'],    },  }}Что они означают?: Логгер ‘django’ пишет все логи с дочерних логгеров в консоль с уровнем сообщений WARNING и выше (‘level’ для логгера ‘django’ не задан, а по умолчанию он равен WARNING). Дочерние логгеры такие: ‘django. db. backends’, ‘django. contrib. gis’ и т. д. Но кроме ‘django. request’, у которого стоит ‘propagate’: False. Логгер ‘py. warnings’ так же пишет сообщения от модуля warnings в консоль. Например DeprecationWarnings. Пример удобного (на мой взгляд) конфига: Для простоты можно определить один корневой логгер, который будет собирать все сообщения со всех модулей. Если settings. DEBUG = True, то он будет писать в консоль и в специальный отладочный лог-файл все сообщения. Если же settings. DEBUG = False, то сообщения будут писаться только в продакшн лог-файл, только с уровнем INFO и выше. При settings. DEBUG = True будут выводиться в консоль и писаться в лог-файл все SQL запросы, что очень удобно. При желании можно создать отдельный логгер ‘django. db’ с ‘propagate’: False и задать ему нужные настройки. Т. к. тут я определил корневой логгер, остальным логгерам ставлю null handler, чтобы сообщения не дублировались. Код на gist. github Настройки будут работать для django 1. 5+: Для более ранних версий django можно создать файл log. py: import loggingfrom django. conf import settingstry:  from logging import NullHandlerexcept ImportError:  class NullHandler(logging. Handler):    def emit(self, record):      passclass RequireDebugTrue(logging. Filter):  def filter(self, record):    return settings. DEBUGclass RequireDebugFalse(logging. Filter):  def filter(self, record):    return not settings. DEBUGИ использовать пути для NullHandler, RequireDebugTrue, RequireDebugFalse из этого файла, вместо django. utils. log. . . . . Теперь, в любом файле можно сделать так: import logginglogger = logging. getLogger(__name__)logger. debug( some message )logger. warning( oops, it is a warning )logger. error( bad, very bad )try:  # do somethingexcept ValueError:  logger. exception( I know it could happen )и все ваши логи попадут в нужное место, в зависимости от DEBUG. "
    }, {
    "id": 23,
    "url": "https://st4lk.github.io/blog/2013/08/20/python-logging-every-day.html",
    "title": "Python logging на каждый день",
    "body": "2013/08/20 -   Gist В процессе написания программы, скрипта, часто бывает нужно вывести какую-либо отладочную информацию или сообщить о каком-то событии. Известно, что для этих целей есть встроенный модуль logging. Однако обычно у меня бывает так: времени в обрез, а все эти настройки логов (handlers, loggers, formatters и пр. ) никак не могу запомнить, и на скорую руку вставляю просто print. Потом, если скрипт используется часто или его надо отдать заказчику - хочется чтобы все эти сообщения записывались в файл для последующего анализа. И приходится все переделывать с использованием logging. Так вот, чтобы не держать в голове все настройки логирования, пишу этот пост с необходимыми для работы параметрами. Требования к логированию будут такие::  Все логи пишутся в файл и выводятся в консоль (дублируются) Файл логов ротируется (не превышает указанный размер) Все логи используемых библиотек так же обрабатываются (в логах видно, что это сообщения из библиотеки, а не из скрипта) Работает на python 2. 5+ 3. 0+ В записи лога есть: само сообщение, имя логгера, имя файла, номер строки, дата, уровень сообщения (DEBUG/INFO/WARNING и т. д. ) В лог можно записывать unicode строкиКак пользоваться этими настройками: В сниппете приведены три типа настроек: путем определения logging классов, используя fileConfig и используя dictConfig. Выберите какой вам больше нравится. Самый простой - первый, который использует классы. Так же он работает на большинстве версий python’a: 2. 5+, 3. 0+. Вставьте настройки в ваш скрипт - и у вас настроено логгирование. Теперь все лог-сообщения будут выводиться на консоль и в файл. В коде задаются параметры для корневого (root) логгера. : Он стоит на вершине иерархии логгеров и соответственно к нему попадают сообщения со всех других логгеров (для полного понимания модуля logging лучше почитать статьи в интернете. Например, есть очень хороший туториал прямо в документации python’a: http://docs. python. org/2/howto/logging. html#logging-basic-tutorial). Пример: Возьмем простейший скрипт: import logginglogging. info('started')logging. info('finished')Если запустим скрипт в таком виде, то ничего никуда не напечатается. Добавим теперь настройки из моего сниппета (используя классы): ################################################### LOGGING CLASS SETTINGS (py25+, py30+) ####################################################### also will work with py23, py24 without 'encoding' argimport loggingimport logging. handlersf = logging. Formatter(fmt='%(levelname)s:%(name)s: %(message)s '  '(%(asctime)s; %(filename)s:%(lineno)d)',  datefmt= %Y-%m-%d %H:%M:%S )handlers = [  logging. handlers. RotatingFileHandler('rotated. log', encoding='utf8',    maxBytes=100000, backupCount=1),  logging. StreamHandler()]root_logger = logging. getLogger()root_logger. setLevel(logging. DEBUG)for h in handlers:  h. setFormatter(f)  h. setLevel(logging. DEBUG)  root_logger. addHandler(h)################################## END LOGGING SETTINGS ##################################logging. info('started')logging. info('finished')Запустим скрипт. В консоль (и в файл rotated. log) напечатались сообщения: INFO:root: started (2013-08-21 01:52:31; test. py:21)INFO:root: finished (2013-08-21 01:52:31; test. py:22)Проверим, выводятся ли сообщения из сторонних библиотек. Для простоты создадим игрушечную библиотеку thirdpartylib с кодом: import logginglogger = logging. getLogger(__name__)def do_something():  logger. debug('something is done in thirdpartylib')Теперь в нашем скрипте вызовем do_something: import thirdpartyliblogging. info('started')thirdpartylib. do_something()logging. info('finished')Вывод будет таким: INFO:root: started (2013-08-21 01:57:27; test. py:135)DEBUG:thirdpartylib: something is done in thirdpartylib (2013-08-21 01:57:27; __init__. py:5)INFO:root: finished (2013-08-21 01:57:27; test. py:137)Сам сниппет: "
    }, {
    "id": 24,
    "url": "https://st4lk.github.io/blog/2013/07/11/django-querysetcount-cache.html",
    "title": "Кеширование queryset.count в django",
    "body": "2013/07/11 -  Как-то обнаружил, что у меня идут несколько одинаковых запросов вида SELECT COUNT(*) . . . . Оказалось (да, для меня это было новостью :) ), что метод queryset. count() в джанго кешируется по особому. Но лучше начать рассказ издалека. Как известно, объекты queryset у ORM django являются “ленивыми”, а так же кешируются. Т. е. , преподолжим у нас такая модель: class Item(models. Model):  name = models. CharField(max_length=50)Тогда при создании запроса фактически обращения к БД не происходит (отсюда название lazy - “ленивый”): items = Item. objects. all()Оно происходит, когда мы непосредственно обращаемся к объектам из запроса, например в цикле: for item in items:   print item. nameПри исполнении инструкции for item in items: был такой запрос к БД: SELECT  main_item .  id ,  main_item .  name  FROM  main_item ;При следующем обращении к объектам уже запроса к БД не будет, т. к. все объекты уже были “потроганы” и они попали в кэш. Т. е. этот код сделает только одно обращение к БД: for item in items: # hit the database   print item. namefor item in items: # cache   print item. nameТем не менее, есть некоторые нюансы, когда может произойти второй запрос к БД. Не буду дублировать документацию, чтобы не загромождать статью. Можно почитать здесь: https://docs. djangoproject. com/en/dev/topics/db/queries/#caching-and-querysets. Теперь непосредственно про count: Зная, что queryset кешируется, мне казалось, что и . count() тоже кешируется. Но нет (точнее не всегда). Если вызываем метод count() до того, как исходный queryset попал в кеш, будет обращение к БД при каждом вызове count (данное обращение не ленивое, ведь count() возвращает число, а не другой queryset, как это делают all, filter, exclude): items = Item. objects. all() # not hit DBitems. count() # hit DBitems. count() # hit DBitems. count() # hit DBfor item in items: # hit DB and put into cache   print item. nameОднако, если исходный queryset попал в кеш, то count уже не будет трогать БД: items = Item. objects. all() # no DB hitfor item in items: # hit DB and put to cache   print item. nameitems. count() # cacheitems. count() # cacheitems. count() # cacheСоответственно все это относится и к шаблонам django. В коде, который делал несколько одинаковых запросов SELECT COUNT(*) . . . , как раз были проверки вида: {% if items. count %}и просто вывод количества: {{ items. count }}При этом до этих строк не было обращения к самим объектам items. В итоге на каждой из этих строк шел запрос к БД. Опять же, если до этого где-то был цикл, например такой: {% for item in items %}  {{item. name}}{% endfor %}то {{ items. count }} уже не обращался к БД. Итак, варианты для избежания лишних запросов.    Если мы знаем, что где-то дальше будет перебор всех элементов из queryset, то вполне уместно использовать len.   Python код:   len(items) # DB len(items) # cache for item in items: # cache   # . . .     или наоборот, что тоже верно:   for item in items: # DB   # . . . len(items) # cache len(items) # cache    Шаблон django:   {{ items|length }} # DB {{ items|length }} # cache {% if items|length %} # cache {% for item in items %} # cache    или наоборот:   {% for item in items %} # DB {{ items|length }} # cache {{ items|length }} # cache {% if items|length %} # cache      Если нужно только подсчитать количество, либо queryset, для которого нужно количество не совпадает с тем, который будет использоваться для доступа к элементам, то надо использовать count(). Но вызывать его лучше только единожды   Если в шаблоне нужно обратиться к count более одного раза, то вместо этого:   {{ items. count }} {{ items. count }}    надо либо во view, который генерит этот шаблон, добавить переменную items_count в контекст и в шаблоне использовать ее:   # views. py context['items_count'] = items. count() # template {{ items_count }} {{ items_count }}    либо можно использовать {% with items. count as items_count %} (не добавляя в контекст новых переменных из views. py):   # template {% with items. count as items_count %}   {{ items_count }}   {{ items_count }} {% endwith %}   Конечно, в этой статье под словом “кеш” имеется в виду внутренний кеш queryset. Он никак не связан с кешированием. "
    }, {
    "id": 25,
    "url": "https://st4lk.github.io/blog/2013/06/20/unicode-string-formatting.html",
    "title": "Форматирование unicode строк",
    "body": "2013/06/20 - Вы знали, что если одно из значений строкового выражения с оператором % - unicode, то вся результирующая строка будет тоже unicode? &gt;&gt;&gt;  Hello, %s  % u Alex u'Hello, Alex'&gt;&gt;&gt;  Hello, %s  % u Алексей u'Hello,  0410 043b 0435 043a 0441 0435 0439'Я привык пользоваться методом . format для форматирования строк и его поведение мне больше нравится: тип исходной строки всегда сохраняется, а если в качестве параметра даются не ascii символы, то возбуждается исключение UnicodeEncodeError. &gt;&gt;&gt;  Hello, {0} . format(u Alex )'Hello, Alex'&gt;&gt;&gt;  Hello, {0} . format(u Алексей )Traceback (most recent call last): File  &lt;stdin&gt; , line 1, in &lt;module&gt;UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-6: ordinal not in range(128)&gt;&gt;&gt; u Hello, {0} . format(u Алексей )u'Hello,  0410 043b 0435 043a 0441 0435 0439'А большая ли разница, что вернется - обычная строка или строка unicode? Иногда да, например в случае с urlparse. parse_qs, тип строки имеет значение. Т. е. нужно иметь в виду, что код вида: &gt;&gt;&gt;  Hello, %s  % valueвполне может вернуть unicode строку. Ссылки:  Документация % Документация . format"
    }, {
    "id": 26,
    "url": "https://st4lk.github.io/blog/2013/05/22/parse-url-which-chontains-unicode-query-using-urlp.html",
    "title": "Парсинг url'а, содержащего unicode параметры, используя urlparse.parse_qs",
    "body": "2013/05/22 - Задача: получить словарь параметров URL’a. Например, имеем адрес: http://example. com/?key=value&amp;a=bи нужно получить такой словарь: {'key': ['value'], 'a': ['b']}Тут значения являются списками, т. к. у одного ключа может быть несколько значений: In: http://example. com/?key=value&amp;a=b&amp;a=cOut: {'key': ['value'], 'a': ['b', 'c']}В python’е для этих целей есть функция urlparse. parse_qs, которая делает следующее: &gt;&gt;&gt; import urlparse&gt;&gt;&gt; query =  key=value&amp;a=b &gt;&gt;&gt; urlparse. parse_qs(query){'a': ['b'], 'key': ['value']}Т. е. на вход функции parse_qs нужно давать сами параметры, без “http://exapmle. com/?”. Для отделения параметров от остального адреса, можно воспользоваться функцией urlparse. urlparse: &gt;&gt;&gt; import urlparse&gt;&gt;&gt; url =  http://example. com/?key=value&amp;a=b &gt;&gt;&gt; query = urlparse. urlparse(url). query&gt;&gt;&gt; query'key=value&amp;a=b'&gt;&gt;&gt; params = urlparse. parse_qs(query)&gt;&gt;&gt; params{'a': ['b'], 'key': ['value']}Попробуем восстановить исходные параметры url’а. Воспользуемся функцией urllib. urlencode: &gt;&gt;&gt; import urllib&gt;&gt;&gt; urllib. urlencode(params, doseq=True)'a=b&amp;key=value'Порядок параметров значения не имеет, так что все ок. URL с unicode параметром: По RFC3986, URL может содержать только ограниченный набор символов из набора US-ASCII, состоящий из цифр, букв и нескольких графических символов. Причем некоторые графические символы являются зарезервированными ( : ,  / ,  ? ,  # ,  [ ,  ] ,  @ ,  ! ,  $ ,  &amp; ,  ' ,  ( ,  ) ,  * ,  + ,  , ,  ; ,  = ). Если в URL нужно передать непечатные или зарезервированные символы (например как значение параметра), то их нужно кодировать по правилам Percent-Encoding: %HH, где HH - это шестнадцатеричный код. Предположим нужно передать u”значение”. В python’е строка u значение  содержит unicode коды, нам нужно получить байты. Для этого закодируем строку например кодировкой utf8: &gt;&gt;&gt; value = u'значение'&gt;&gt;&gt; value_utf8 = value. encode('utf8')&gt;&gt;&gt; value_utf8' d0 b7 d0 bd d0 b0 d1 87 d0 b5 d0 bd d0 b8 d0 b5'Теперь уже надо закодировать эти байты, используя Percent-Encoding (%HH), для передачи в url: &gt;&gt;&gt; value_url = urllib. quote(value_utf8)&gt;&gt;&gt; value_url'%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5'Построим полный адрес: &gt;&gt;&gt; url =  http://example. com/?key=%s&amp;a=b  % value_url&gt;&gt;&gt; url'http://example. com/?key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'И опять попробуем получить словарь параметров: &gt;&gt;&gt; query = urlparse. urlparse(url). query&gt;&gt;&gt; query'key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'&gt;&gt;&gt; params = urlparse. parse_qs(query)&gt;&gt;&gt; params{'a': ['b'], 'key': [' d0 b7 d0 bd d0 b0 d1 87 d0 b5 d0 bd d0 b8 d0 b5']}Видим, что parse_qs раскодировал значение из кодировки Percent-Encoding и вернул нам байты. Можно теперь получить unicode, ведь мы помним, что кодировали строку с помощью utf8: &gt;&gt;&gt; params['key'][0]. decode('utf8')u' 0437 043d 0430 0447 0435 043d 0438 0435'&gt;&gt;&gt; print params['key'][0]. decode('utf8')значениеОк. Восстановим исходные параметры из полученного словаря: &gt;&gt;&gt; urllib. urlencode(params, doseq=True)'a=b&amp;key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5'Получили те же параметры, которые и задавали вначале. Проделаем тоже самое для URL, которая возвращается в django при вызове request. get_full_path(). request. get_full_path() почему-то возвращает не строку (str), а unicode (пробовал на django 1. 4, 1. 5): &gt;&gt;&gt; request. get_full_path()u'/?key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'Повторим те же шаги c этой url: &gt;&gt;&gt; url = request. get_full_path()&gt;&gt;&gt; query = urlparse. urlparse(url). query&gt;&gt;&gt; queryu'key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'&gt;&gt;&gt; params = urlparse. parse_qs(query)&gt;&gt;&gt; params{u'a': [u'b'], u'key': [u' d0 b7 d0 bd d0 b0 d1 87 d0 b5 d0 bd d0 b8 d0 b5']}Интересно, что значение для u’key’ представляет из себя unicode строку, которая содержит байты! Конечно же, раскодировать ее уже не получится: &gt;&gt;&gt; params['key'][0]. decode('utf8')Traceback (most recent call last): File  &lt;stdin&gt; , line 1, in &lt;module&gt; File  C:\Python27\lib\encodings tf_8. py , line 16, in decode  return codecs. utf_8_decode(input, errors, True)UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-15: ordinal not in range(128)То же самое получим и с urlencode: &gt;&gt;&gt; urllib. urlencode(params, doseq=True)Traceback (most recent call last): File  &lt;stdin&gt; , line 1, in &lt;module&gt; File  C:\Python27\lib rllib. py , line 1337, in urlencode  l. append(k + '=' + quote_plus(str(elt)))UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-15: ordinal not in range(128)Тут для меня были две неожиданности:  django вернул url как unicode (зачем? почему не обычная строка str, ведь в url’е не могут быть не-ASCII символы) parse_qs вернул строку unicode, которая содержит байты. Решение простое, на вход parse_qs нужно давать только строку str: &gt;&gt;&gt; url = request. get_full_path()&gt;&gt;&gt; url = url. encode('ascii')&gt;&gt;&gt; url'/?key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'Либо так, что тоже самое: &gt;&gt;&gt; url = request. get_full_path()&gt;&gt;&gt; url = str(url)&gt;&gt;&gt; url'/?key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'Ссылки:  Вопрос на эту тему на stackoverflow Отличная презентация про кодировки строк на python’e: http://nedbatchelder. com/text/unipain. html"
    }, {
    "id": 27,
    "url": "https://st4lk.github.io/blog/2013/04/12/django-admin-site-optimisation.html",
    "title": "Оптимизация админки django",
    "body": "2013/04/12 -  Как известно, чем меньше запросов к базе данных делает сайт, тем лучше производительность. Обычно админка - часть сайта с меньшим трафиком, но все же хорошо, если от туда не идут лишние запросы. Это и приятней в пользовании, т. к. страница отдается быстрее, и все-таки разгружает сервер. В этом посте я рассмотрю некоторые способы уменьшения количества запросов из админки к БД при использовании __unicode__, содержащего поля связанного объекта (ForeignKey). Думаю, глядя на примеры станет понятнее. 1. __unicode__ at admin change form page: Рассмотрим пример, в котором есть модели станции метро (SubwayStation), линии метро (SubwayLine), города (City). Линия - это ForeignKey для станции, а City - ForeignKey для линии. models. py: from django. db import modelsclass City(models. Model):  name = models. CharField(max_length=50)  def __unicode__(self):    return self. nameclass SubwayLine(models. Model):  name = models. CharField(max_length=50)  city = models. ForeignKey(City, related_name='lines')  def __unicode__(self):    return u {0}, {1} . format(self. city, self. name)class SubwayStation(models. Model):  name = models. CharField(max_length=50)  line = models. ForeignKey(SubwayLine, related_name='stations')  def __unicode__(self):    return self. nameadmin. py: from django. contrib import adminfrom . models import SubwayLine, SubwayStation, Cityclass SubwayLineAdmin(admin. ModelAdmin):  list_display = 'name',class SubwayStationAdmin(admin. ModelAdmin):  list_display = 'name', 'line'class CityAdmin(admin. ModelAdmin):  list_display = 'name',admin. site. register(SubwayLine, SubwayLineAdmin)admin. site. register(SubwayStation, SubwayStationAdmin)admin. site. register(City, CityAdmin)Зайдем на страницу изменения станции Как видим, __unicode__ для линии состоит из двух полей: поля связанного объекта City. name и поля SubwayLine. name. Посмотрим теперь, какие запросы были к БД (запросы к таблицам django_session, auth_user, django_content_type в расчет не берем): SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id  FROM  main_subwaystation  WHERE  main_subwaystation .  id  = 1 ;SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwayline ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;Идут такие запросы:  первый - получить запись станции второй - получить все варианты линий остальные - получить для каждой линии имя городаПолучается, что количество запросов = 2 + количество линий. Это много. Чтобы исправить ситуацию есть два варианта:    Использовать select_related в запросе   Для этого переопределим форму админки для SubwayStation:   admin. py:    from django. contrib import admin from django import forms from . models import SubwayLine, SubwayStation, City class SubwayLineAdmin(admin. ModelAdmin):   list_display = 'name', class SubwayStationForm(forms. ModelForm):   def __init__(self, *args, **kwargs):     super(SubwayStationForm, self). __init__(*args, **kwargs)     self. fields['line']. queryset = SubwayLine. objects. all()\       . select_related('city')   class Meta:     model = SubwayStation class SubwayStationAdmin(admin. ModelAdmin):   list_display = 'name',   form = SubwayStationForm class CityAdmin(admin. ModelAdmin):   list_display = 'name', admin. site. register(SubwayLine, SubwayLineAdmin) admin. site. register(SubwayStation, SubwayStationAdmin) admin. site. register(City, CityAdmin)    И теперь при отображении страницы станции осталось только два запроса (благодаря INNER JOIN):    SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id  FROM  main_subwaystation  WHERE  main_subwaystation .  id  = 1 ; SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_subwayline  INNER JOIN  main_city  ON ( main_subwayline .  city_id  =  main_city .  id );      Использовать raw_id_fields (полезно, когда записей так много, что неудобно и дорого их выводить в выпадающем списке)   Задаем raw_id_fields у SubwayStationAdmin:   admin. py:    class SubwayStationAdmin(admin. ModelAdmin):   list_display = 'name',   raw_id_fields = 'line',    Страница теперь выглядит так:     Смотрим запросы:    SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id  FROM  main_subwaystation  WHERE  main_subwaystation .  id  = 1 ; SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwayline  WHERE  main_subwayline .  id  = 1 ; SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)    Три запроса, где третий - это запрос города для отображения __unicode__ линии. Хорошо. При желании можно пойти еще дальше и избавиться от этого последнего запроса. Для этого придется переопределить виджет ForeignKeyRawIdWidget:   admin. py:    from django. contrib import admin from django. contrib. admin. widgets import ForeignKeyRawIdWidget from django. contrib. admin. sites import site from django. utils. text import Truncator from django. utils. html import escape from django import forms from . models import SubwayLine, SubwayStation, City class SubwayLineAdmin(admin. ModelAdmin):   list_display = 'name', class StationForeignKeyRawIdWidget(ForeignKeyRawIdWidget):   def label_for_value(self, value):     key = self. rel. get_related_field(). name     try:       obj = self. rel. to. _default_manager. select_related('city'). using(self. db). get(**{key: value})       return '&amp;nbsp;&lt;strong&gt;%s&lt;/strong&gt;' % escape(Truncator(obj). words(14, truncate='. . . '))     except (ValueError, self. rel. to. DoesNotExist):       return '' class SubwayStationForm(forms. ModelForm):   def __init__(self, *args, **kwargs):     super(SubwayStationForm, self). __init__(*args, **kwargs)     self. fields['line']. widget = StationForeignKeyRawIdWidget(       SubwayStation. _meta. get_field( line ). rel, site)   class Meta:     model = SubwayStation class SubwayStationAdmin(admin. ModelAdmin):   list_display = 'name',   raw_id_fields = 'line',   form = SubwayStationForm class CityAdmin(admin. ModelAdmin):   list_display = 'name', admin. site. register(SubwayLine, SubwayLineAdmin) admin. site. register(SubwayStation, SubwayStationAdmin) admin. site. register(City, CityAdmin)    И запросов осталось только два:    SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id  FROM  main_subwaystation  WHERE  main_subwaystation .  id  = 1 ; SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_subwayline  INNER JOIN  main_city  ON ( main_subwayline .  city_id  =  main_city .  id ) WHERE  main_subwayline .  id  = 1 ;    Приведенный вариант с переопределением ForeignKeyRawIdWidget будет работать на django 1. 4, 1. 5. Для более ранних версий метод label_for_value отличается, так что его нужно скопировать из класса ForeignKeyRawIdWidget модуля django/contrib/admin/widgets. py и добавить к нему ``. select_related(‘city’)`.  2. __unicode__ в inline формах: Показанные выше методы можно применять и в inline формах. Допустим у нас появилась модель района (District), на которую ссылается станция по ForeignKey: models. py: # . . . class District(models. Model):  name = models. CharField(max_length=50)  city = models. ForeignKey(City, related_name='districts')  def __unicode__(self):    return u {0}, {1} . format(self. city, self. name)class SubwayStation(models. Model):  name = models. CharField(max_length=50)  line = models. ForeignKey(SubwayLine, related_name='stations')  district = models. ForeignKey(District, related_name='stations')  def __unicode__(self):    return self. name# . . . Как видим, __unicode__ у District включает поле связанной модели City. Теперь добавим станции как inline к линии метро: admin. py: # . . . class SubwayStationInline(admin. TabularInline):  model = SubwayStation  extra = 0class SubwayLineAdmin(admin. ModelAdmin):  list_display = 'name',  inlines = SubwayStationInline,# . . . Откроем страницу редактирования линии И посмотрим, какие запросы к БД были при отображении страницы (для простоты я сделал только две станции у данной линии): SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwayline  WHERE  main_subwayline .  id  = 1 ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwaystation .  district_id  FROM  main_subwaystation  WHERE  main_subwaystation .  line_id  = 1 ORDER BY  main_subwaystation .  id  ASC;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city ;SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id  FROM  main_district ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id  FROM  main_district ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id  FROM  main_district ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)Получается, что на каждую вложенную станцию идет запрос на получение всех районов + города на каждый район. Тут можно избавиться от дополнительных запросов для получения города района, сделав select_related('city') в форме для Inline модели: admin. py: # . . . from django import formsfrom . models import SubwayLine, SubwayStation, City, Districtclass SubwayStationForm(forms. ModelForm):  def __init__(self, *args, **kwargs):    super(SubwayStationForm, self). __init__(*args, **kwargs)    self. fields['district']. queryset = District. objects. all()\      . select_related('city')  class Meta:    model = SubwayStationclass SubwayStationInline(admin. TabularInline):  model = SubwayStation  form = SubwayStationForm  extra = 0class SubwayLineAdmin(admin. ModelAdmin):  list_display = 'name',  inlines = SubwayStationInline,# . . . Теперь уже нет отдельных запросов городов: SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwayline  WHERE  main_subwayline .  id  = 1 ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwaystation .  district_id  FROM  main_subwaystation  WHERE  main_subwaystation .  line_id  = 1 ORDER BY  main_subwaystation .  id  ASC;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city ;SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_district  INNER JOIN  main_city  ON ( main_district .  city_id  =  main_city .  id );SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_district  INNER JOIN  main_city  ON ( main_district .  city_id  =  main_city .  id );SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_district  INNER JOIN  main_city  ON ( main_district .  city_id  =  main_city .  id );3. __unicode__ на странице списка объектов (change list page): Рассмотрим страницу списка станций. Напомню код модели и админки: models. py: # . . . class SubwayLine(models. Model):  name = models. CharField(max_length=50)  city = models. ForeignKey(City, related_name='lines')  def __unicode__(self):    return u {0}, {1} . format(self. city, self. name)class SubwayStation(models. Model):  name = models. CharField(max_length=50)  line = models. ForeignKey(SubwayLine, related_name='stations')  def __unicode__(self):    return self. name# . . . admin. py: # . . . class SubwayStationAdmin(admin. ModelAdmin):  list_display = 'name', 'line'# . . . Откроем страницу: Видим, что на каждую станцию выводиться линия. Причем __unicode__ у линии содержит еще и город Посмотрим на запросы к БД: SELECT COUNT(*) FROM  main_subwaystation ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_subwaystation  INNER JOIN  main_subwayline  ON ( main_subwaystation .  line_id  =  main_subwayline .  id ) INNER JOIN  main_city  ON ( main_subwayline .  city_id  =  main_city .  id ) ORDER BY  main_subwaystation .  id  DESC;Все выглядит здорово. Django сам сделал select_related на странице списка объектов. Но здесь есть небольшая ловушка. Читаем документацию о select_related внимательно, где написано  Note that, by default, select_related() does not follow foreign keys that have null=True Т. е. в нашем случае select_related() сработал, но только потому, что у поля line нет null=True. Попробуем сделать теперь так: models. py: # . . . class SubwayStation(models. Model):  name = models. CharField(max_length=50)  line = models. ForeignKey(SubwayLine, related_name='stations', null=True, blank=True)  def __unicode__(self):    return self. name# . . . Открываем страницу списка станций и смотрим запросы: SELECT COUNT(*) FROM  main_subwaystation ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwaystation  INNER JOIN  main_subwayline  ON ( main_subwaystation .  line_id  =  main_subwayline .  id ) ORDER BY  main_subwaystation  SC;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;Опа. Так уже для отображения __unicode__ каждой линии идет запрос города. Чтобы исправить ситуацию, нужно переопределить queryset для страницы списка объектов, указав явно в select_related нужные поля: # . . . class SubwayStationAdmin(admin. ModelAdmin):  list_display = 'name', 'line'  def queryset(self, request):    qs = super(SubwayStationAdmin, self). queryset(request)    qs = qs. select_related('line__city')    return qs# . . . Теперь с запросами все в порядке: SELECT COUNT(*) FROM  main_subwaystation ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_subwaystation  INNER JOIN  main_subwayline  ON ( main_subwaystation .  line_id  =  main_subwayline .  id ) LEFT OUTER JOIN  main_city  ON ( main_subwayline .  city_id  =  main_city .  id ) ORDER BY  main_subwaystation .  id  DESC;"
    }, {
    "id": 28,
    "url": "https://st4lk.github.io/blog/2013/03/05/python-mutable-default-arguments.html",
    "title": "Функции с изменяемыми значениями по умолчанию в python'e",
    "body": "2013/03/05 -  В python’е значения функции по умолчанию создаются в момент выполнения инструкции def, а не при каждом вызове функции. Если значение - неизменяемый объект (например строка, целое число, кортеж), то никаких подводных камней здесь нет. А вот если объект изменяемый (например список), то есть возможность попасть в ловушку. Вот пример: def foo(l=[]):  l. append('x')  return lКазалось бы, при каждом вызове foo() будет возвращаться список [‘x’]. Но: &gt;&gt;&gt; foo()['x']&gt;&gt;&gt; foo()['x', 'x']&gt;&gt;&gt; foo()['x', 'x', 'x']Поэтому, если нужно, чтобы при каждом вызове создавался новый пустой список, следует делать так: def bar(l=None):  if l is None:    l = []  l. append('x')  return lОднако, иногда этот эффект может быть полезен. Вот способ узнать сколько раз функция была вызвана: from itertools import countdef bar(call_count=count()):  return next(call_count)&gt;&gt;&gt; bar()0&gt;&gt;&gt; bar()1&gt;&gt;&gt; bar()2&gt;&gt;&gt; bar()3"
    }, {
    "id": 29,
    "url": "https://st4lk.github.io/blog/2013/02/26/multilanguage-site-django-without-redirects.html",
    "title": "Многоязычный сайт на django без редиректов",
    "body": "2013/02/26 - Star Начиная с django 1. 4, можно задать префикс для url для каждого включенного языка. К примеру, мы хотим сайт, который будет иметь русскую и английскую версию. Для этого добавляем в settings. py: # default language, it will be used, if django can't recognize user's languageLANGUAGE_CODE = 'ru'# list of activated languagesLANGUAGES = (  ('ru', 'Russian'),  ('en', 'English'),)# enable django’s translation systemUSE_I18N = True# specify path for translation filesLOCALE_PATHS = (  os. path. join(PROJECT_DIR, 'locale'),)# add LocaleMiddlewareMIDDLEWARE_CLASSES = (  # . . .  'django. middleware. locale. LocaleMiddleware',  # . . . )И в urls. py используем i18n_patterns вместо patterns: from django. conf. urls import urlfrom django. conf. urls. i18n import i18n_patternsurlpatterns = i18n_patterns('',  url(r'^about/$', 'about. view', name='about'),)В этом случае, когда мы обратимся по адресу /about/, django попытается определить предпочитаемый нами язык и сделает редирект на страницу с префиксом языка. Допустим django определил, что наш язык - русский, тогда произойдет редирект на /ru/about/. При этом, если мы сами пройдем по адресу /en/about/, то уже никаких редиректов не будет, а отобразится английская версия сайта, т. к. в url уже присутствует префикс /en/. При такой схеме могут возникнуть проблемы с индексированием сайта некоторыми поисковыми системами. Например yandex. ru очень долго отказывался индексировать сайт, т. к. не получал http кода 200 с корневого url. C Google’ом таких проблем не было, но все же и он не рекомендует использовать редиректы. Из http://support. google. com/webmasters/bin/answer. py?hl=ru&amp;answer=182192:  Избегайте автоматического перенаправления по языку пользователя. Это может привести к тому, что пользователи и поисковые системы не смогут просмотреть все версии вашего сайта. Поэтому я решил создать пакет, который позволял бы работать без редиректов. Схема работы будет такая:  Если в url нет языкового префикса, то используется язык по умолчанию (settings. LANGUAGE_CODE) Если префикс есть, то используется язык, соответствующий префиксу (/en/ = англ), но при этом префикса для языка по умолчанию нет. Благо для этого нужно совсем немного кода, нужно лишь изменить LocaleMiddleware и i18n_patterns, что я и сделал в репозитории https://github. com/st4lk/django-solid-i18n-urls. Установка:    Устанавливаем django-solid-i18n-urls, например с помощью pip:   pip install solid_i18n      Изменяем LocaleMiddleware на SolidLocaleMiddleware:   MIDDLEWARE_CLASSES = (  # . . .   # remove 'django. middleware. locale. LocaleMiddleware',  'solid_i18n. middleware. SolidLocaleMiddleware',   # . . . )      Вместо i18n_patterns используем solid_i18n_patterns:   from django. conf. urls import url from solid_i18n. urls import solid_i18n_patterns  urlpatterns = solid_i18n_patterns('',   url(r'^about/$', 'about. view', name='about'), )   Репозиторий на github: https://github. com/st4lk/django-solid-i18n-urls. UPDATED: Добавлена опция settings. SOLID_I18N_USE_REDIRECTS (по умолчанию False). Если она равна True, то будут использованы редиректы по следующим правилам:  При обращении по url без языкового префикса, например '/', язык будет определен из предпочтений пользователя. Если этот язык не равен языку по умолчанию (settings. LANGUAGE_CODE), то будет перенаправление на url с соответствующем префиксом. Если равен, то отображается url без префикса (который и был запрошен).  При обращении по url с языковым префиксом поведение не меняется, т. е. всегда используется язык из префикса. Привет: # settings. py: LANGUAGE_CODE = 'ru'SOLID_I18N_USE_REDIRECTS = TrueДопустим предпочитаемый язык пользователя - английский. Тогда при обращении к '/' будет редирект на '/en/'. А если предпочитаемый язык - русский, то редиректа не будет, т. е. при обращении к '/' оторбразиться '/'. Замечание: При таком подходе возможна следующая ситуация. Предпочитаемый язык браузера - английский. Но пользователь хочет увидеть русскую версию, которая отображается без префикса. Если для переключения языков использовать простые ссылки, т. е. &lt;a href= {{ specific language url}} &gt;, то пользователь будет постоянно перенаправляться на английскую версию. Поэтому при переключении языков надо записывать в cookie пользователя выбранный им язык. Это можно делать с помощью специального встроенного в django set_language view. "
    }, {
    "id": 30,
    "url": "https://st4lk.github.io/blog/2013/02/04/script-downloading-music-vkcom-vkontakteru.html",
    "title": "Скрипт для скачивания музыки вконтакте",
    "body": "2013/02/04 -   Gist Беглый поиск соответствующего скрипта на python’е не дал результов. В статье на хабре ссылка не работает. Решил написать свой велосипед, он доступен здесь. Запуск (нужен установленный python интерпретатор): python vkcom_audio_download. pyПроверял на python 2. 6 и 2. 7. Никаких дополнительных библиотек не требуется. Принцип работы: Скрипт проверяет сохраненный access_token. Если его нет или срок истек, то открывается страница в браузере с запросом на доступ к данным аккаунта (аудио записям). После подтверждения идет редирект на https://oauth. vk. com/blank. htm#… . Нужно скопировать весь url, на который вас редиректнуло и вставить его в консоль скрипта. Далее будут скачиваться все ваши аудиозаписи. Если аудиозапись уже есть на диске - то скачивания не происходит. Будут запрошены ваши данные приложением с app_id = 3358129. Можно создать свое Standalone-приложение с доступом к аудио здесь: http://vk. com/editapp?act=create. И заменить APP_ID на ваше. Ссылка на скрипт: https://gist. github. com/4708673. Код скрипта: "
    }, {
    "id": 31,
    "url": "https://st4lk.github.io/blog/2013/01/03/sublime-text-and-github-gists.html",
    "title": "Sublime text и github gists",
    "body": "2013/01/03 -  В Sublime text есть огромное количество полезных инструментов, помогающих быстро писать код. Пока я изучил лишь небольшую их часть, в том числе пытаюсь привыкнуть к Vintage mode (управление курсором в стиле vim). Но сейчас хочу рассказать о другом - об интеграции github gists с sublime text. Если вы не знаете, github gists позволяет сохранять скрипты, снипеты в виде отдельного файла, чтобы не создавать для этого целый репозиторий. При этом поддерживаются многие функции репозитория - версии, возможность форка. Что мы получим в итоге: Сохраняем снипет прямо из sublime, даем ему описание в виде ключевых слов, затем опять-таки из sublime ищем наш снипет по ключевым словам и видим его в редакторе. Все снипеты сохраняются на github, т. е. они доступны и с другого компьютера. Не стоит забывать, что в самом sublime есть свой настраиваемый функционал снипетов. Однако он больше подходит для маленьких авто-заполнений, например при наборе def для питон кода вставлять шаблон для написания функции: def function():  passСнипеты в github gist подходят для чего-то большего - какая-либо готовая функция, которая делает конкретную задачу. Настроим этот функционал в нашем редакторе: Установим плагин для работы с github gist: Проще всего это сделать с помощью пакетного менеджера sublime. Здесь есть инструкция для его установки. В sublime нажимаем ctrl + shift + p, вводим install, и далее gist:  Теперь дадим плагину доступ к нашему github аккаунту: Нажимаем Preferences-&gt;Package settigns-&gt;Gist-&gt;Settings User.  Можно либо указать логин+пароль, либо токен. Если нужных настроек в Settings User нет, то их можно скопировать из Settings Default. Только default settings лучше не менять. Чтобы получить токен, нужно в командной строке выполнить такую команду (должен быть установлен curl): curl -v -u USERNAME -X POST https://api. github. com/authorizations --data  {\ scopes\ :[\ gist\ ]} Где USERNAME - ваш логин на github Создадим gist: Пишем код нашего снипета в новой вкладке sublime. Я написал код для получения содержимого csv файла в виде списка списков. Нажимаем ctrl + shift + p, вводим gist create public и enter. Здесь работает fuzzy поиск, так что я набираю просто public.  Даем нашему снипету описание. Важно включить значащие слова, т. к. по ним потом будет идти поиск. Я напишу так “Python: Get csv lines”.  Будет еще запрос на название файла, можно просто нажать enter. Найдем только что созданный снипет: Вызываем строку ввода команд ctrl + shift + p и пишем gist open.  Далее пишем ключевые слова “python csv” И видим код снипета Этот снипет так же создался на github: https://gist. github. com/3931305. Ссылки:  Редактор кода sublime text Репозиторий плагина Gist Видео про sublime и github gist на tutsplus. com (на английском) Видео курс по sublime text на tutsplus. com (на английском)"
    }, {
    "id": 32,
    "url": "https://st4lk.github.io/blog/2012/11/29/mobileesp-easily-detect-mobile-web-site-visitors.html",
    "title": "MobileESP: Скрипт определения мобильного устройства посетителя",
    "body": "2012/11/29 - Скрипт полезен, если вы хотите показывать разные версии сайта для обычных компьютеров и мобильных устройств. Большое количество методов для определения вида девайса. Доступен на разных языках программирования, включая python. Собственно порт на python был написан мной по просьбе freelance заказчика. Так его можно использовать в django проекте: from mobileesp import mdetectuser_agent = request. META. get( HTTP_USER_AGENT )http_accept = request. META. get( HTTP_ACCEPT )if user_agent and http_accept:  agent = mdetect. UAgentInfo(userAgent=user_agent, httpAccept=http_accept)  #Do first! For iPhone, Android, Windows Phone 7, etc.   if agent. detectTierIphone():    HttpResponseRedirect('/myapp/i/')  #Then catch all other mobile devices  if agent. detectMobileQuick():    HttpResponseRedirect('/myapp/m/')#For traditional computers and tablets (iPad, Android, etc. )return HttpResponseRedirect('/myapp/d/')Сам скрипт на code. google. com, ко всем методам есть комментарий-описание. Описание на сайте проекта. "
    }, {
    "id": 33,
    "url": "https://st4lk.github.io/blog/2012/11/18/debug-django-project-embedded-python-debugger-pdb.html",
    "title": "Отладка django проекта с помощью встроенного python отладчика pdb",
    "body": "2012/11/18 - Я использую sublime-text в качестве редактора python кода. В нем нет встроенного отладчика, поэтому для отладки django проектов я в основном делал так: print var_nameи в консоле локального сервера смотрел вывод команды. Я использую этот метод и сейчас, но иногда хочется пройтись по коду по шагам, посмотреть все переменные. Это можно сделать с помощью встроенного python отладчика pdb: import pdb; pdb. set_trace()Т. е. мы вставляем эту строку в то место в коде, где мы хотим остановиться. Это брейкпоинт. Теперь обновим страницу проекта в браузере. Когда код проекта дойдет до этой строки, браузер замрет, а в консоле локального сервера появиться: (Pdb)Мы попали в отладчик и теперь можем вводить комманды, например такие:  l - посмотреть, где мы находимся n - (step next) сделать шаг вперед, не входя внутрь функции s - (step in) сделать шаг внутрь, т. е. если стоим на вызове функции, войдем внутрь r - (step out) продолжить выполнение до конца текущего блока. Например, мы стоим внутри цикла, вводим r и попадаем на первую после цикла строку.  c - продолжить выполнение до следующего брейкпоинта, т. е. до pdb. set_trace() p - выполнить питон код, или просто показать переменную: p var_nameПример: Допустим у нас есть такой view: Вставим import pdb; pdb. set_trace() в нужное место и запустим локальный сервер, если не запущен: В браузере обратимся к странице, которая вызывает этот view. Страница замерла: В консоле видим (Pdb): Посмотрим, где мы, командой l: Сделаем два шага веред командой n: Посмотрим значение переменных about и about. content: Продолжим выполнение командой c: Страница отобразилась в браузере: "
    }, {
    "id": 34,
    "url": "https://st4lk.github.io/blog/2012/10/08/cloud-service-openshift.html",
    "title": "Облачный сервис Openshift",
    "body": "2012/10/08 -  Я знаю немного хостингов с бесплатным тарифом и с поддержкой python. Это Google App Engine и Alwaysdata. И вот недавно узнал про замечательный проект Openshift от RedHat, на котором и работает этот блог. Для начала небольшой обзор упомянутых хостингов. Google app engine: Основной недостаток google app engine - ограниченный набор библиотек, которые можно использовать. Конечно, чисто python библиотеки можно ставить, но те, которые требуют C компиляции - нет. К примеру, не получится использовать pycurl. Так же на GAE используются особые базы данные, так что и библиотеки для работы с ними - тоже особые. Отсюда запуск приложения на django не так прост, ведь djangо работает только с SQL базами данных. Плюс django можно использовать на текущий момент только версий 1. 2, 1. 3 (а уже есть 1. 4). На всякий случай, полезные линки: список поддерживаемых библиотек, проект django-rocket-engine. Alwaysdata: На бесплатном тарифе от alwaysdata опять-таки нельзя устанавливать библиотеки, требующие C компиляцию. Но здесь имеются привычные базы данных - mysql, postgres, mongodb. Так что обычное django приложение можно запустить без проблем. Я часто использую этот хостинг для демонстрации несложных проектов. Теперь непосредственно про openshift. Openshift: Openshift - это PaaS, т. е. платформа как сервис. Мы не получаем root доступ к операционной системе. Систему нам предоставляет сервис в работающем виде, мы же можем делать только некоторые дозволенные действия. Похоже на упомянутый app engine, но здесь гораздо больше свободы. В первую очередь мне понравилось, что там можно установить многое вручную. Какую хочешь версию python, django. Желаешь SQL/Postgres/mongodb в качестве базы данных - пожалуйста. Можно устанавливать библиотеки, компилировать их. Есть дополнительные плюшки: cron, статистика, phpmyadmin и прочее. Здесь я упомянул django, но можно сделать приложение и на другом фреймворке, просто это то, что я пробовал. Удобный способ загрузки приложения на сервер. Просто делаешь git push special_application_url из своего git-репозитория, и все! Файлы автоматически обновляются на сервере, сервер перезапускается. Процессом перезапуска можно управлять с помощью специальных скриптов. Например, можно указать в скрипте установить нужные библиотеки из requirements. txt, собрать статические файлы (manage. py collectstatic) и т. д. Этот скрипт будет выполняться каждый раз при обновлении сервера. В бесплатном режиме предоставляется 1 Gb дискового пространства и 3 малых ‘gear’. Насколько я понял, gear - это некое изолированное окружение со своим объемом RAM. Для малой gear размер RAM равен 512. Если приложению не хватает мощности одной gear, то подключается вторая, третья и т. д. Если проще, то вот нагрузка, которую должен выдержить типовой сайт на DLE на бесплатном тарифе (из описания на openshift): 15 страниц в секунду, сотни страниц на сайте, 50к посетителей в месяц. Для простого сайта вполне достаточно. На один аккаунт можно создать 3 приложения (по крайне мере бесплатно). Конечно, есть вероятность, что эту халяву прикроют через какое-то время. Но существующий код всегда можно будет запустить на другом хостинге - здесь нет особенностей, как на google app engine. Ну или купить платный аккаунт на openshift. В принципе, на openshift подробно написано, что нужно сделать для создания и запуска приложения. Опишу свой опыт. Шаги для запуска приложения на python 2. 7. 3 + django:  регистрируемся устанавливаем git (если еще нет) устанавливаем особую программу “rhc” (можно обойтись и без нее, но с ней удобнее), описание в разделе get-started следуем инструкциям https://github. com/ehazlett/openshift-diy-py27-django (простое приложение без базы данных) либо следуем инструкциям https://github. com/st4lk/lexev (приложение с mysql, собственно код этого блога)Все! Приложение доступно по адресу http://&lt;app_name&gt;-&lt;namespace&gt;. rhcloud. com/ Привязка к домену: Но нет, не совсем все. Хотелось бы иметь нормальный адрес … Допустим, у нас уже есть свой домен (например lexev. org). Давайте теперь привяжем наше приложение к этому домену. У openshift нет dns серверов, которые можно было бы прописать для домена. Вместо этого они предлагают сделать привязку к домену с помощью CNAME. Это можно сделать в панели управления домена. Эх, все бы хорошо, но я покупал домен на nic. ru. Там, чтобы сделать CNAME привязку нужен дополнительный платный доступ… Но, как оказалось, есть выход! Воспользуемся бесплатным сервисом freedns. afraid. org. Итак,  регистрируемся на freedns. afraid. org   указываем DNS сервера к нашему домену:    ns1. afraid. org ns2. afraid. org ns3. afraid. org ns4. afraid. org      добавляем домен на afraid. org: http://freedns. afraid. org/domain/add. php, в моем случае это lexev. org. Не забудем выбрать Shared State: Private, а то к нашему домену сможет привязаться любой пользователь afraid. org     для нашего вновь добавленного домена добавляем subdomen с типом CNAME, как на картинке (конечно заменяя lexev. org на нужный домен и указывая верный url приложения в поле destination):     на странице subdomen’ов видим две записи: одну с CNAME и одну без (она скорее всего сверху) нажимаем на ту, которая без CNAME ничего не меняя, нажимаем на “Forward to a URL”   вводим как на картинке (опять-таки заменяя lexev. org на нужный домен):       в итоге мы должны получить такие subdomen’ы:    Вот теперь кажется все. Какое-то время понадобиться, чтобы новые dns серверы заработали для домена. По обращению lexev. org будет идти редирект на www. lexev. org. Ну а www. lexev. org указывает на приложение openshift. Напоследок скажу, что openshift так же поддерживает PHP, Ruby, Java, Node. js, Perl ! "
    }, {
    "id": 35,
    "url": "https://st4lk.github.io/blog/2012/08/31/aphorism-messenger.html",
    "title": "Мессенджер афоризмов",
    "body": "2012/08/31 -  У меня есть интересный проект, которым хотел бы поделиться. Идея создать его возникла во время изучения Java. Прочел я несколько книжек, сделал маленькие программки-задания и захотелось сделать нечто большее. Суть проекта: Есть desktop-программа (клиент), которая сидит в трее, и периодически показывает афоризмы. Афоризмы она берет с web-сервиса (сервер), таким образом база цитат находится в одном месте. Т. е. база не привязана к клиенту, я могу обновить ее на сервере и все клиенты это заметят. У афоризмов есть рейтинг - количество лайков. При появлении высказывания пользователь видит рейтинг и может добавить/убрать лайк. Вместе с афоризмом отображается его автор, если нажать по автору появляется его краткая биография. Можно просматривать афоризмы, помеченные как любимые. Также можно смотреть список самых популярных афоризмов. Получилось так (скачать можно здесь): Весь проект (и клиент и сервер) написан на Java. Сервер расположен на google app engine, ресурсы предоставляет в формате xml. Он представляет из себя REST веб-сервис, так что его можно использовать и отдельно (например на веб-сайте). Спецификация есть здесь. Вот пример ресурса: показать случайный афоризм. Ссылки:  подробное описание проекта программа-клиент сервер код клиента код сервера видео обзор"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}
</script>

<style>
    #lunrsearchresults {padding-top: 0.2rem;}
    .lunrsearchresult {padding-bottom: 1rem;}
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>

<form onsubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <p><input type="text" class="form-control" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Поиск (только на англ.)" /></p>
</form>
<div id="lunrsearchresults">
    <ul></ul>
</div>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name"></li><li><a class="u-email" href="mailto:alexevseev@gmail.com">alexevseev@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"></div>

    </div>

  </div>

</footer>
<script id="dsq-count-scr" src="//lexev-dev.disqus.com/count.js" async></script></body>

</html>
