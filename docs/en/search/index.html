<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1"><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Search | Alexey Evseev</title>
<meta name="generator" content="Jekyll v3.9.2" />
<meta property="og:title" content="Search" />
<meta name="author" content="Alexey Evseev" />
<meta property="og:locale" content="en" />
<link rel="canonical" href="https://st4lk.github.io/en/search/" />
<meta property="og:url" content="https://st4lk.github.io/en/search/" />
<meta property="og:site_name" content="Alexey Evseev" />
<meta property="og:type" content="website" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Search" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"WebPage","author":{"@type":"Person","name":"Alexey Evseev"},"headline":"Search","url":"https://st4lk.github.io/en/search/"}</script>
<!-- End Jekyll SEO tag -->
<link rel="shortcut icon" type="image/png" href="/favicon.png">
  <link rel="stylesheet" href="/en/assets/main.css">
  <link rel="stylesheet" href="/en/assets/css/styles.css"><link type="application/atom+xml" rel="alternate" href="https://st4lk.github.io/en/feed.xml" title="Alexey Evseev" /><link type="application/atom+xml" rel="alternate" href="https://st4lk.github.io/en/feed.xml" title="Alexey Evseev" /><!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-8ZHLL9G1CF"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-8ZHLL9G1CF');
</script>
<script async defer src="https://buttons.github.io/buttons.js"></script>
</head>
<body><header class="site-header" role="banner">

  <div class="wrapper"><a class="site-title" rel="author" href="/en/">Alexey Evseev</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger">
          <a class="page-link" href="/en/blog/">Blog</a>
          <a class="page-link" href="/en/contact/">Contact</a>
          <a class="page-link" style="margin-right: 0px" href="/search/" ><img src="/en/assets/images/ru.png" /></a>
          <a class="page-link" href="/en/search/" ><img src="/en/assets/images/en.png" /></a>
          <a class="page-link" href="/en/search/" ><img src="/en/assets/images/search.svg" /></a>
        </div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post">

  <div class="post-content">
    <script src="/assets/js/lunr.js"></script>

<script>

var documents = [{
    "id": 0,
    "url": "https://st4lk.github.io/en/blog/2019/02/19/sublime-text-and-language-server-protocol-lsp.html",
    "title": "Sublime Text and Language Server Protocol",
    "body": "2019/02/19 -  Language Server Protocol (LSP): LSP - protocol for interactions between IDE and language server. The latter provides such means like autocompletion, goto implementation and etc. When IDE needs to show autocomplete choices on, for example, python language - it sends a request to the special server. And it responds with the necessary data. The cool part here is that it is an initiative of a big company - Microsoft. But what is the point, in almost all popular IDEs such features are already available out of the box? I think the main point is unification of implementation. We can have a single server with many IDEs. No need to develop standard means in each IDE separately. The only thing that must be done - is to implement protocol for communication with the language server. There is also an advantage from the perspective of IDE user. When some feature will be added to the language server, you’ll receive it for free and immediately. It doesn’t matter what exact IDE you are using, it is independent now. The main part that must be supported by your code editor - is language server protocol. We can assume that it will be stable in near future since it is supported by such giant as Microsoft. Some useful links:  Official website of LSP: https://microsoft. github. io/language-server-protocol/ The list of server implementations: https://microsoft. github. io/language-server-protocol/implementors/servers/ Community-driven website: https://langserver. org/Python Server: Let’s try to connect IDE (Sublime Text will be used in this article) with python language server. The first thing that we’ll need - is the server itself, it will respond to LSP requests. At the time this post was written, the most active project implemented in python was: https://github. com/palantir/python-language-server. There is a server from Microsoft as well: https://github. com/Microsoft/python-language-server/. And it even has the short instruction how to setup Sublime Text for it. This server requires dotnet to be installed in your system. I tried to launch it, but it didn’t provide all the features that I needed. Maybe I didn’t set it up correctly. And I rarely (well, never :)) use C# in my work, so it is hard to understand the details of implementation. This is not required to be able to use, but sometimes I like to do small tweaks and it will be hard to do in C# for me. So let’s return to the server written in python. I had such a plan in my mind:  We have a separate virtualenv, that was created exclusively for LSP server We keep clean the virtualenv of our working project, i. e. there is no LSP server installed in it During communication with the server, our IDE will tell which virtualenv to use (to indicate environment of current project)In total, we can install LSP server only once and use it for different projects. It wasn’t obvious how to do it at the time this post was written. At least I didn’t find a definite guide on how to do it. In the documentation for the python server it was implicitly assumed that server will be installed in the same virtualenv as the project. To my mind this is not useful, so I created a PR. Also, this LSP didn’t use my linter settings. For example, in the root folder of the project I have setup. cfg file with such params: ignore = D202,D205,D211max-line-length = 99To make LSP use it I have to create another PR, which in turn migrated to this one. There were other problems and sometimes they required PRs as well. I hope, that all of them will be included in release soon. Meanwhile, there is release in my fork, it contains mentioned fixes. To be precise:  Create a separate virtualenv for LSP server. Better to use python3. 6+.    Install server there (the release that I mentioned above ^):    pip install https://github. com/st4lk/python-language-server/archive/0. 22. 0a1. zip   If you want to use requirements. txt - then just add a link to zip file there as is, no need to use any prefix/suffix. It is possible, that at the moment when you read this post, all the fixes are already included in official version &gt; 0. 22 (or they were fixed by other commits). It worth checking the changelog of the current version. Sublime Text: Let’s move to IDE setup. I’ll describe Sublime Text in this article. I like this code editor, although it is not an IDE strictly speaking. I like its speed, vintage mode, multi-cursors and many other features. So, how to connect Sublime Text with LSP server?First of all we’ll need to install a plugin for LSP: https://github. com/tomv564/LSP. The most important thing - to setup it properly. Go to plugin settings (on MacOS it is Sublime Text -&gt; Preferences -&gt; Package Settings -&gt; LSP -&gt; Settings). In user’s settings type something like: {  clients : {   pyls :  {    enabled : true,    command : [     ~/. virtualenvs/python-language-server/bin/pyls    ],    settings : {     pyls : {      configurationSources : [ flake8 ],      plugins : {       jedi_definition : {        follow_imports : true      }     }    }   },    scopes : [ source. python ],    syntaxes : [     Packages/Python/Python. sublime-syntax ,     Packages/Djaneiro/Syntaxes/Python Django. tmLanguage    ],    languageId :  python   } }}Let’s speak about the most important parameters. “command” - path to executable file of pyls server. As you can see, on my computer it is installed in virtualenv ~/. virtualenvs/python-language-server/ (I use virtualenvwrapper). This is not a virtualenv of your project! Here we have only pyls and nothing else. “settings” : “pyls” : “configurationSources”. Here we can specify the source of linter settings. By default it is “pycodestyle”, but I usually use flake8. So pyls will search following files: '. flake8', 'setup. cfg' (section [flake8]), 'tox. ini'“follow_imports”: true. An important setting, without it goto will show only the import place, but not the source code. To my mind, such behavior is not very useful, it is more interesting to look at actual implementation, instead of just import path. To make it work I have to tweak the server a little bit: https://github. com/palantir/python-language-server/pull/404/. Another valuable thing: “syntaxes” setting. If you are using plugin like Djaneiro (it adds some cool autocompletion, snippets for Django project), then Sublime Text will treat syntax of python files as Djaneiro, not as the default Python. So need to specify it for our LSP server. Otherwise, editor will not launch it. We have defined global settings that will be used for all python fileds. But how to define settings specific for concrete project? Surprisingly, you can specify them in project settings :). Go to Project -&gt; Edit project and add the following: {  folders : [   {      path :  /Path/to/my/project    }, ],  settings : {   LSP :  {    pyls :   {     env :    {      VIRTUAL_ENV :  /path/to/my/project/virtualenv/     }   }  } }}By using “VIRTUAL_ENV” we can point to project’s virtualenv, so pyls will know where to search installed packages for autocompletion, goto end etc. Ok, let’s launch all of it. Imagine that we have such django project: project├── . . . ├── my_app│  ├── . . . │  ├── forms. py│  └── models. py├── manage. py└── setup. cfgThe simplest models. py file: from django. db import modelsclass Book(models. Model):       This is docstring of Book model.   Example for LSP demostration.        title = models. CharField(max_length=50)and forms. py: from django. forms import ModelFormfrom . models import Bookclass BookForm(ModelForm):  class Meta:    model = Book    fields = '__all__'In setup. cfg we have [flake8] section: [flake8]ignore = D202,D205,D211max-line-length = 99What do we get now?Server will provide features of jedi out of the box, for example, Definitions (goto), References, autocomplete. On hover you’ll see the doscstring of the object, as well as available actions: If you’ll click on Definition you will jump to class implementation. On click on References - all usages will be shown: Autocomplete is also working:  But syntax errors and pep8 rules violation will not be shown. To make it work, need to install following packages into virtualenv where your LSP server is running: pip install mccabepip install pyflakespip install pycodestyleVoila, now editor will show warnings:  Remember, that linter settings will be taken from setup. cfg automatically (according to our pyls settings). So rules D202,D205,D211 will be ignored and max line length will be 99 instead of default 80. We can setup mypy check as well. A corresponding plugin will be needed for that. But it will be better to describe it in a separate article if I can make it. Some non-obvious things exist there as well. In total, we have a working Python Language Server in Sublime Text 3. As you can see, the experience wasn’t very smooth, but I keep using this code editor, it is my habit. Yes, I use Pycharm sometimes, it has tons of nice features out of the box. Visual Studio Code editor is good as well, but Sublime looks so native for me. What conclusion can I make?  the server doesn’t look fully ready (at least implementation in python), have to create PRs and they accepted slowly Sublime LSP doesn’t work in the right window (View -&gt; Layout -&gt; Columns: 2), probably need to fix by submitting further PRs it is worth checking the server implemented by Microsoft, I think it may be the mainstream. Meanwhile, my setup is working and it helps me a lot in my daily work. Thanks to Grigory Petrov for his talk at Moscow Python. That was the starting point for me to investigate the LSP world. "
    }, {
    "id": 1,
    "url": "https://st4lk.github.io/en/blog/2017/01/14/count-filtered-related-objects-django.html",
    "title": "Trap in counting related objects in Django",
    "body": "2017/01/14 -  Task: for every object count number of related objects satisfying some conditions. Example: class Category(models. Model):  title = models. CharField(max_length=50)class Article(models. Model):  title = models. CharField(max_length=50)  category = models. ForeignKey(Category)  approved_at = models. DateTimeField(blank=True, null=True)Pay attention at field Article. approved_at, it contains article approval time and it can be null. Create test data: from django. utils import timezonec1 = Category. objects. create(title='c1')c2 = Category. objects. create(title='c2')a1 = Article. objects. create(category=c1, title='a1')a2 = Article. objects. create(category=c1, title='a2', approved_at=timezone. now())So we have two categories. The first one has one approved and one not approved article. The second category doesn’t have any article. Let’s begin with counting all articles for each category: from django. db. models import Count&gt;&gt;&gt; Category. objects. annotate(. . .   article_count=Count('article'). . . ). values('title', 'article_count')&lt;QuerySet [{'article_count': 2, 'title': u'c1'}, {'article_count': 0, 'title': u'c2'}]&gt;Django ORM built following expected SQL query: SELECT  main_category .  title , COUNT( main_article .  id ) AS  article_count   FROM  main_category   LEFT OUTER JOIN  main_article  ON ( main_category .  id  =  main_article .  category_id )  GROUP BY  main_category .  id ,  main_category .  title ;Ok, now we want to count only approved articles. Using SQL language, we can just update JOIN part: SELECT  main_category .  title , COUNT( main_article .  id ) AS  article_count   FROM  main_category   LEFT OUTER JOIN  main_article      ON ( main_category .  id  =  main_article .  category_id  AND       main_article .  approved_at  IS NOT NULL)  GROUP BY  main_category .  id ,  main_category .  title ;Unfortunately, Django ORM doesn’t allow to apply any filter for Count (at least in v1. 10). But starting from v1. 8 we have conditional expressions, so let’s use it: from django. db. models import Count, Case, When&gt;&gt;&gt; Category. objects. annotate(. . .   article_count=Count(. . .     Case(When(article__approved_at__isnull=False, then=1)). . .   ). . . ). values('title', 'article_count')&lt;QuerySet [{'article_count': 1, 'title': u'c1'}, {'article_count': 0, 'title': u'c2'}]&gt;The returned values are valid. Corresponding SQL query: SELECT  main_category .  title , COUNT(  CASE WHEN  main_article .  approved_at  IS NOT NULL THEN 1 ELSE NULL END) AS  article_count FROM  main_category LEFT OUTER JOIN  main_article  ON ( main_category .  id  =  main_article .  category_id )GROUP BY  main_category .  id ,  main_category .  title ;The Trap: Here is an interesting question, how we can count number of not approved articles? The first thought that comes to the mind is to change False to True in the query: &gt;&gt;&gt; Category. objects. annotate(. . .   article_count=Count(. . .     Case(When(article__approved_at__isnull=True, then=1)). . .   ). . . ). values('title', 'article_count')But query returns not valid values: &lt;QuerySet [{'article_count': 1, 'title': u'c1'}, {'article_count': 1, 'title': u'c2'}]&gt;The second category has mysterious unapproved article. Check the SQL: SELECT  main_category .  title , COUNT(  CASE WHEN  main_article .  approved_at  IS NULL THEN 1 ELSE NULL END) AS  article_count FROM  main_category LEFT OUTER JOIN  main_article  ON ( main_category .  id  =  main_article .  category_id )GROUP BY  main_category .  id ,  main_category .  title ;The condition CASE WHEN  main_article .  approved_at  IS NULL THEN 1will be true even if category doesn’t have any article at all! One approach to fix is the following: &gt;&gt;&gt; Category. objects. annotate(. . .   article_count=Count(. . .     Case(. . .       When(. . .         article__id__isnull=False,. . .         article__approved_at__isnull=True,. . .         then=1. . .       ). . .     ). . .   ). . . ). values('title', 'article_count')&lt;QuerySet [{'article_count': 1, 'title': u'c1'}, {'article_count': 0, 'title': u'c2'}]&gt;The moral of the story: When condition like IS NULL happens, double check every edge case! "
    }, {
    "id": 2,
    "url": "https://st4lk.github.io/en/blog/2016/08/01/django-signal-or-model-method.html",
    "title": "Django: signal or model method?",
    "body": "2016/08/01 -  When I needed to implement some functionality on model saving, I always asked a question to myself - where to place it. In signal or in model method save()? Let’s see, what and when is more applicable. When use model methods save(), delete()?: To my mind class methods are more usable, if logic concerns exclusively current model. For example, fill some field on model saving according to data from other fields. Some people say, that signals are better because it is easy to reuse them. It seems strange to me because we can define a function or a mixin class and reuse it in method save() as well. Generally, we can always use signals, so why I favor method save? Simple answer - it is more readable. When you are going through the model, you can easily understand, that something will happen on saving. In case of signals, especially if there is no rule where they are defined, the logic often come out of sight. Keep in mind, that delete signals pre_delete, post_delete have some advantage over delete() method: they are called even on cascading delete and deleting a queryset. This is not happening with model method. In this situation make a decision according to context, maybe cascading delete is not so important. On bulk creating and updating no code is executed: nor signal nor save(). So here they are equal. And don’t forget to call parent’s method save() or delete() if you override them. When use signals?: Signals are more applicable when you implement reusable applications. The users of your app can easily connect signals to their models without modifying the code of these models. We can define a function or a mixin class for the same purpose. But agree, that attaching logic from some foreign app is more comfortable by using signals. Besides, if you decide to stop using the app, you will need to modify very small part of the project’s code. The same is true when there are two (or more) apps within one project and you need to do something with one model when another model from the different application is being saved. Imagine two applications, users and reports. When we create a user we need to create automatically a report for that user. In this case, I prefer to create a signal in reports application, since logic corresponds to it. Why? Firstly, we keep logic in the place where it belongs. Secondly, if for some reason we decide to delete the reports app entirely, we don’t even touch the users application. Where define signals and were connect them?: As django docs (section “Where should this code live?”) suggests, define signals in separate submodule signals and not in models. py and __init__. py. This will save you from import problems. To be sure the signals are connected, we need to execute the code that connects them on project launch. When we define them in models. py, we already get this. But now they are living in other place and it won’t run until we import it somewhere. Let’s do it in ready() method of application config class. In general, I follow recommendations from this stackoverflow answer. Here is an example for users and reports applications, that I talked about earlier. We need to create a report on user creation.    Create submodule signals and place handlers. py in it   reports/signals/__init__. py reports/signals/handlers. py      Define signals in that file handlers. py   from django. db. models. signals import post_save from django. dispatch import receiver from django. contrib. auth import get_user_model from reports. models import Report User = get_user_model() @receiver(post_save, sender=User) def create_user_report(sender, instance, created, **kwargs):   if created:     Report. objects. create(user=instance)      Create application config class   reports/apps. py    with code:   from django. apps import AppConfig class ReportsConfig(AppConfig):   name = 'reports'   verbose_name = 'Reports'   def ready(self):     import reports. signals. handlers # noqa    And now our signal is connected. In this example I used decorator @receiver, so just import is enough. We also could call connect method of the signal explicitly here. It is a matter of taste.   Don’t forget to define our ReportsConfig class as config of the application. To do it, place this code in reports/__init__. py:   default_app_config = 'reports. apps. ReportsConfig'    Or place ReportsConfig in your settings. INSTALLED_APPS. Look django docs for details.  If follow these rules we will always know where to find signal handlers. And consequently no need to search the models module to find them. "
    }, {
    "id": 3,
    "url": "https://st4lk.github.io/en/blog/2015/09/30/trying-json-combo-django-and-postgresql.html",
    "title": "Trying JSON in Django and PostgreSQL (and compare with MongoDB)",
    "body": "2015/09/30 -  New JSONField will be added in Django 1. 9, it can be used with PostgreSQL &gt;= 9. 4. Let’s try to work with it and find out, in what situations it can be useful. Currently django 1. 9 alpha is available, final version is scheduled on December 2015. Alpha can be installed with pip: pip install --pre djangoNow imagine that we have an e-commerce site, where we offer products of different types. For example, laptops and t-shirts. Obviously, such goods will have different attributes: t-shirts will have size, color and laptops - screen size, CPU frequency, hard drive and so on. One of the approaches to design such data in SQL is Entity–attribute–value model (EAV). But now we have JSON, so let’s try to organise data using this type. Create simple model for products: from django. db import modelsfrom django. contrib. postgres. fields import JSONFieldclass Category(models. Model):  name = models. CharField(max_length=100)class Product(models. Model):  name = models. CharField(max_length=100)  category = models. ForeignKey(Category)  price = models. IntegerField()  attributes = JSONField()  def __str__(self):    return self. nameAs we can see, there are several common fields for all products (name, category, price) plus specific to particular product attributes (as JSON field). Create objects: tshirt = Category. objects. create(name='tshirts')notebook = Category. objects. create(name='notebook')# TshirtsProduct. objects. create(name='Silk tshirt', category=tshirt, price=100, attributes={  'colors': ['red', 'black'],  'sizes': ['S', 'M'],  'model': 'polo',  'material': 'silk',})Product. objects. create(name='Bamboo tshirt', category=tshirt, price=120, attributes={  'colors': ['white', 'yellow'],  'sizes': ['M', 'L', 'XL'],  'model': 'poet',  'material': 'bamboo',})# NotebooksProduct. objects. create(name='MacBook Pro', category=notebook, price=2000, attributes={  'brand': 'Apple',  'screen': 15. 0,  'speed': 2200,  'hd': 256,})Product. objects. create(name='ATIV Book 9', category=notebook, price=1200, attributes={  'brand': 'Samsung',  'screen': 12. 2,  'speed': 2400,  'hd': 128,})Queries: Let’s see, what queries we can make.    Get t-shirts with both ‘M’ and ‘L’ sizes:   &gt;&gt;&gt; Product. objects. filter(category=tshirt, attributes__contains={'sizes': ['M', 'L']}) [&lt;Product: Bamboo tshirt&gt;]      Get t-shirts with both ‘M’ and ‘L’ sizes, both white and yellow colors, with poetry on it (model=poet):   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'sizes': ['M', 'L'], 'colors': ['white', 'yellow'],   'model': 'poet'}) [&lt;Product: Bamboo tshirt&gt;]      Get laptops with CPU frequency 2400 and screen size 12. 2   &gt;&gt;&gt; Product. objects. filter(category=notebook,   attributes__contains={'speed': 2400, 'screen': 12. 2}) [&lt;Product: ATIV Book 9&gt;]      Get t-shirts with red color, model polo and with size ‘M’ or ‘L’   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'colors': ['red'], 'model': 'polo'},   attributes__sizes__has_any_keys=['M', 'L']) [&lt;Product: Silk tshirt&gt;]      Get laptops with CPU greater that 2000 and screen larger that 13   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__gt=2000,   attributes__screen__gt=13) [&lt;Product: MacBook Pro&gt;]      Get laptops with CPU frequency equal to 2200 or 2400   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__in=(2200, 2400)) [&lt;Product: ATIV Book 9&gt;, &lt;Product: MacBook Pro&gt;]    or like this:   &gt;&gt;&gt; from django. db. models import Q &gt;&gt;&gt; Product. objects. filter(category=notebook). filter(   Q(attributes__contains={'speed': 2200}) | Q(attributes__contains={'speed': 2400}))   Indexes: Let’s find out, how effective can be our queries. PostgreSQL supports different indexes for JSON types:    GIN   this index in its turn can support different operators:      jsonb_ops (default), supports operators @&gt;, ?, ?&amp;, ?|   jsonb_path_ops, supports only @&gt;, but works faster and requires less space      btree   can be useful in searching exact json document     hash   same as btree, can be useful in searching exact json document  Сorrespondence of some Django operations and PostgreSQL operators: Django    Postgres----------------------contains   @&gt;contained_by &lt;@has_key    ?has_any_keys ?|has_keys   ?&amp;In our case the most interesting operator is @&gt;. Django will transform operation contains to it for json fields. If we simply add db_index=True, btree index will be created: class Product(models. Model):  . . .   attributes = JSONField(db_index=True)For our queries GIN index will be more applicable. To create it we’ll use RunSQL operation. First create empty migration. In current example app with products has name ‘catalogue_simple’ python manage. py makemigrations --empty catalogue_simpleIn created file (in my case it is 0002_auto_20150928_1610. py) add couple imports and commands to create and discard index: from catalogue_simple. models import Productfrom psycopg2. extensions import AsIsclass Migration(migrations. Migration):  # . . .   operations = [    migrations. RunSQL(      [( CREATE INDEX catalogue_product_attrs_gin ON %s USING gin          (attributes jsonb_path_ops); , [AsIs(Product. _meta. db_table)])],      [('DROP INDEX catalogue_product_attrs_gin;', None)],    )  ]Here catalogue_product_attrs_gin - index name (we can choose any), attributes - name of JSON field, Product - product model. We are creating jsonb_path_ops index, as it will cover the most common operation in our queries - contains. Extension AsIs is used to not wrap %s param with single quotes. We don’t need btree index, so don’t add it: class Product(models. Model):  . . .   attributes = JSONField()Test data: I’ve generated 1 000 000 products of 4 different categoires, 250 000 in each. Every product category has its own attributes, from 4 to 7 keys. Some values are scalar (t-shirt material), some - lists (t-shirt sizes). Queries and indexes:    Get t-shirts with both ‘M’ and ‘L’ sizes:   &gt;&gt;&gt; Product. objects. filter(category=tshirt, attributes__contains={'sizes': ['M', 'L']})    Corresponding SQL (enumeration of all field names is replaced with * for brevity):   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 5   AND  catalogue_simple_product .  attributes  @&gt; '{ sizes : [ M ,  L ]}');    Without GIN index on attributes query time is 292 ms, EXPLAIN ANALYSE.   Same query with GIN index - 250 ms, EXPLAIN ANALYSE.   In this case we don’t gain much performance (292 ms vs 250 ms), because result contains a lot of rows: 66412. It is called “low selectivity”. Selectivity - ratio of filtered rows to the total rows. If this ratio tends to 1, we say “low selectivity”, to 0 - “high selectivity”. This metric helps us to estimate index effectiveness. With low selectivity index will not gain much performance.     Get t-shirts with both ‘M’ and ‘L’ sizes, both white and yellow colors, with poetry on it (model=poet):   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'sizes': ['M', 'L'], 'colors': ['white', 'yellow'],   'model': 'poet'})    Corresponding SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 5   AND  catalogue_simple_product .  attributes  @&gt; '{    colors : [ white ,  yellow ],    model :  poet ,    sizes : [ M ,  L ] }');    Without GIN index - 240 ms, EXPLAIN ANALYSE.   With GIN index - 49 ms, EXPLAIN ANALYSE.   Query became faster: 240 ms vs 49 ms. Result contains 3737 rows, higher selectivity than in previous request.     Get laptops with CPU frequency 2400 and screen size 12. 2   &gt;&gt;&gt; Product. objects. filter(category=notebook,   attributes__contains={'speed': 2400, 'screen': 12. 2})    Corresponding SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 7   AND  catalogue_simple_product .  attributes  @&gt; '{ screen : 12. 2,  speed : 2400}');    Without GIN index - 222 ms, EXPLAIN ANALYSE.   With GIN index - 34 ms, EXPLAIN ANALYSE.   222ms vs 34ms. Result contains 10389 rows.     Get t-shirts with red color, model polo and with size ‘M’ or ‘L’   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'colors': ['red'], 'model': 'polo'},   attributes__sizes__has_any_keys=['M', 'L'])    Corresponding SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 5   AND  catalogue_simple_product .  attributes  @&gt; '{ colors : [ red ],  model :  polo }'   AND  catalogue_simple_product .  attributes  -&gt; 'sizes' ?| ARRAY['M', 'L']);    Without GIN index - 253 ms, EXPLAIN ANALYSE.   With GIN index - 78 ms, EXPLAIN ANALYSE.   253 ms vs 78 ms. Result contains 18428 rows. In this query has_any_keys can’t use index, as we declared jsonb_path_ops index. But index jsonb_ops also will not work, because we are looking for array elements and not for first level keys. If such query is common and it has high selectivity, we can create index on particular JSON key:   CREATE INDEX gin_sizes ON catalogue_simple_product USING gin ((attributes -&gt; 'sizes'));    But in current example this don’t make sense, as filter  attributes  -&gt; 'sizes' ?| ARRAY['M', 'L'] has low selectivity:   &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'colors': ['red'], 'model': 'polo'},   attributes__sizes__has_any_keys=['M', 'L']). count() 18428 &gt;&gt;&gt; Product. objects. filter(category=tshirt,   attributes__contains={'colors': ['red'], 'model': 'polo'}). count() 25162    Only ~25% of objects are filtered by sizes in this query.     Get laptops with CPU greater that 2000 and screen larger that 13   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__gt=2000,   attributes__screen__gt=13)    Corresponding SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 7   AND  catalogue_simple_product .  attributes  -&gt; 'screen' &gt; '13'   AND  catalogue_simple_product .  attributes  -&gt; 'speed' &gt; '2000')    GIN index will not help here. If such request is common, we can create btree index on needed JSON keys:   CREATE INDEX attrs_screen_speed ON catalogue_simple_product ((attributes -&gt; 'screen'), (attributes -&gt; 'speed'));    Result contains 10536 rows.   Without btree index query time is 352 ms, EXPLAIN ANALYSE.   With btree index - 46 ms, EXPLAIN ANALYSE.     Get laptops with CPU frequency equal to 2200 or 2400   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__in=(2200, 2400))    Corresponding SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 7   AND  catalogue_simple_product .  attributes  -&gt; 'speed' IN ('2200', '2400'))    This query is not covered by GIN index. Query time ~ 389 ms, EXPLAIN ANALYSE.   Let’s try to rewrite this query to use existing GIN index   &gt;&gt;&gt; from django. db. models import Q &gt;&gt;&gt; Product. objects. filter(category=notebook). filter(Q(attributes__contains={'speed': 2200}) | Q(attributes__contains={'speed': 2400}))    SQL:   SELECT * FROM  catalogue_simple_product  WHERE ( catalogue_simple_product .  category_id  = 7   AND ( catalogue_simple_product .  attributes  @&gt; '{ speed : 2200}'     OR  catalogue_simple_product .  attributes  @&gt; '{ speed : 2400}'));    Now GIN index can be used, query time ~ 337 ms EXPLAIN ANALYSE.   As we see, there is no much difference. But let’s check the selectivity of this query. Result contains 124 995 rows from 250 000 possible for current category, we have very low selectivity.   Create 100 laptops with CPU frequency 3200 and 100 laptops with 3500. There are no other laptops with such frequencies in database.   No check the performance:   Query doesn’t use GIN index:   &gt;&gt;&gt; Product. objects. filter(category=notebook, attributes__speed__in=(3200, 3500))    we get the same time ~ 391 ms EXPLAIN ANALYSE.   Query does use GIN index:   &gt;&gt;&gt; Product. objects. filter(category=notebook). filter(Q(attributes__contains={'speed': 3200}) | Q(attributes__contains={'speed': 3500}))    Now we have query time only 0. 773 ms! EXPLAIN ANALYSE.  Resume of indexes: We can use single GIN index (jsonb_path_ops) to query by several attributes, and not just by one! Of course, it is not a silver bullet. We must always take into account, what data is we working with. And consequently choose right design. NoSQL database (MongoDB): Can we store same data and make similar queries in MongoDB (v3. 0. 6)? To use only one single index in queries by unknown fields in advance, we need to use different structure in MongoDB. Field attributes will be a list of embedded documents, that have name and value: &gt; db. catalogue_simple. find(). pretty(){   _id  : ObjectId( 560ab1970a0a88fe77d00f02 ),   category  :  tshirts ,   name  :  Silk tshirt ,   price  : 100,   attributes  : [    {       name  :  colors ,       value  :  red     },    {       name  :  colors ,       value  :  black     },    {       name  :  sizes ,       value  :  S     },    {       name  :  sizes ,       value  :  M     },    {       name  :  model ,       value  :  polo     },    {       name  :  material ,       value  :  silk     }  ]}{   _id  : ObjectId( 560ab1dd0a0a88fe77d00f03 ),   category  :  tshirts ,   name  :  Bamboo tshirt ,   price  : 120,   attributes  : [    {       name  :  colors ,       value  :  white     },    {       name  :  colors ,       value  :  yellow     },    {       name  :  sizes ,       value  :  M     },    {       name  :  sizes ,       value  :  L     },    {       name  :  sizes ,       value  :  XL     },    {       name  :  model ,       value  :  poet     },    {       name  :  material ,       value  :  bamboo     }  ]}{   _id  : ObjectId( 560ab2cb0a0a88fe77d00f04 ),   category  :  notebook ,   name  :  MacBook Pro ,   price  : 2000,   attributes  : [    {       name  :  brand ,       value  :  Apple     },    {       name  :  screen ,       value  : 15    },    {       name  :  speed ,       value  : 2200    },    {       name  :  hd ,       value  : 256    }  ]}{   _id  : ObjectId( 560ab2ec0a0a88fe77d00f05 ),   category  :  notebook ,   name  :  ATIV Book 9 ,   price  : 1200,   attributes  : [    {       name  :  brand ,       value  :  Samsung     },    {       name  :  screen ,       value  : 12. 2    },    {       name  :  speed ,       value  : 2400    },    {       name  :  hd ,       value  : 128    }  ]}Queries (MongoDB):    Get t-shirts with both ‘M’ and ‘L’ sizes:   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  sizes ,  value :  M }},   { $elemMatch : { name :  sizes ,  value :  L }} ]}, category: 'tshirts'}) { name  :  Bamboo tshirt , /* . . . */}      Get t-shirts with both ‘M’ and ‘L’ sizes, both white and yellow colors, with poetry on it (model=poet):   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  sizes ,  value :  M }},   { $elemMatch : { name :  sizes ,  value :  L }},   { $elemMatch : { name :  colors ,  value :  white }},   { $elemMatch : { name :  colors ,  value :  yellow }},   { $elemMatch : { name :  model ,  value :  poet }} ]}, category: 'tshirts'}) { name  :  Bamboo tshirt , /* . . . */}      Get laptops with CPU frequency 2400 and screen size 12. 2   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  speed ,  value : 2400}},   { $elemMatch : { name :  screen ,  value : 12. 2}} ]}, category: 'notebook'}) { name  :  ATIV Book 9 , /* . . . */}      Get t-shirts with red color, model polo and with size ‘M’ or ‘L’   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  colors ,  value :  red }},   { $elemMatch : { name :  model ,  value :  polo }},   { $elemMatch : { name :  sizes ,  value : { $in : [ M ,  L ]}}} ]}, category: 'tshirts'}) { name  :  Silk tshirt , /* . . . */}      Get laptops with CPU greater that 2000 and screen larger that 13   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  speed ,  value : { $gt : 2000}}},   { $elemMatch : { name :  screen ,  value : { $gt : 13}}} ]}, category: 'notebook'}) { name  :  MacBook Pro , /* . . . */}      Get laptops with CPU frequency equal to 2200 or 2400   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  speed ,  value : { $in : [2200, 2400]}}}, ]}, category: 'notebook'}) { name  :  MacBook Pro , /* . . . */}, { name  :  ATIV Book 9 , /* . . . */}   Indexes (MongoDB): We can create multikey index: &gt; db. catalogue_simple. ensureIndex({ attributes. name  : 1,  attributes. value  : 1})For fairness, create index on category (django automatically creates it for ForeignKey fields) &gt; db. catalogue_simple. ensureIndex({ category : 1})Keep in mind, that MongoDB will use index only for first filter from $all operator. Although MongoDB docs tells about index intersection, looks like it can’t be used in our case. Illustrative example. Imagine, that in database there is only 1 product with size “XXXS” and a lot of products with size “M”. We want products, that have both sizes “XXXS” and “M”. Check out the query: &gt; db. catalogue_simple. find({attributes: {$all: [  { $elemMatch : { name :  sizes ,  value :  M }},  { $elemMatch : { name :  sizes ,  value :  XXXS }},]}, category: 'tshirts'})MongoDB will apply index only to “M” value. As a result, many documents will be scanned:  nReturned  : 1, executionTimeMillis  : 1902, totalKeysExamined  : 249934, totalDocsExamined  : 249934,But if we place size “XXXS” to the first position: &gt; db. catalogue_simple. find({attributes: {$all: [  { $elemMatch : { name :  sizes ,  value :  XXXS }},  { $elemMatch : { name :  sizes ,  value :  M }},]}, category: 'tshirts'})only one document will be scanned:  nReturned  : 1, executionTimeMillis  : 0, totalKeysExamined  : 1, totalDocsExamined  : 1,The moral is, put filter with highest selectivity to the first place. Unfortunately, we don’t always have such information. Test data (MongoDB): Test data is exactly the same as in PostgreSQL (only structure is different): 4 categories, 250 000 products in each category, 1 000 000 in total. Queries and indexes (MongoDB):    Get t-shirts with both ‘M’ and ‘L’ sizes:   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  sizes ,  value :  M }},   { $elemMatch : { name :  sizes ,  value :  L }} ]}, category: 'tshirts'})    Without multikey index on attributes query time is 706 ms, category index is used.   Output of . explain('executionStats'):    winningPlan  : {   // . . .    indexName  :  category_1 , }  executionStats  : {    nReturned  : 66412,    executionTimeMillis  : 706,    totalKeysExamined  : 250001,    totalDocsExamined  : 250001, }    After creation of multikey index on attributes field nothing changes, as MongoDB optimizer decides, that category index is better (i suppose because of low selectivity of attributes query).     Get t-shirts with both ‘M’ and ‘L’ sizes, both white and yellow colors, with poetry on it (model=poet):   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  sizes ,  value :  M }},   { $elemMatch : { name :  sizes ,  value :  L }},   { $elemMatch : { name :  colors ,  value :  white }},   { $elemMatch : { name :  colors ,  value :  yellow }},   { $elemMatch : { name :  model ,  value :  poet }}, ]}, category: 'tshirts'})    Behaviour is similar to first query.   Behaviour can change, if we put higher selective filter to the first place. In test data color is more selective, that size. So, make it first:   &gt; db. catalogue_simple. find({attributes: {$all: [     { $elemMatch : { name :  colors ,  value :  white }},     { $elemMatch : { name :  sizes ,  value :  M }},     { $elemMatch : { name :  sizes ,  value :  L }},     { $elemMatch : { name :  colors ,  value :  yellow }},     { $elemMatch : { name :  model ,  value :  poet }},   ]}, category: 'tshirts'}). explain('executionStats')    Explain:    winningPlan  : {   // . . .    indexName  :  attributes. name_1_attributes. value_1 , }  executionStats  : {    nReturned  : 3737,    executionTimeMillis  : 658,    totalKeysExamined  : 124902,    totalDocsExamined  : 124902, }    Query time - 658 ms.     Get laptops with CPU frequency 2400 and screen size 12. 2   Same as points 1 and 2.     Get t-shirts with red color, model polo and with size ‘M’ or ‘L’   Same as points 1 and 2.     Get laptops with CPU greater that 2000 and screen larger that 13   &gt; db. catalogue_simple. find({attributes: {$all: [   { $elemMatch : { name :  speed ,  value : { $gt : 2000}}},   { $elemMatch : { name :  screen ,  value : { $gt : 13}}} ]}, category: 'notebook'}). explain('executionStats')    Remember, PostgreSQL can’t use GIN index in such a query. But multikey index in current data structure can work.   We have good selectivity for speed in this request:    winningPlan  : {   // . . .    indexName  :  attributes. name_1_attributes. value_1 , }  executionStats  : {    nReturned  : 10536,    executionTimeMillis  : 160,    totalKeysExamined  : 62472,    totalDocsExamined  : 62472, }      Get laptops with CPU frequency equal to 2200 or 2400   Same as previous points.  Resume: PostgreSQL 9. 4 introduced new type jsonb. It can be used effectively in queries. And we can use single index to query on different json keys. Not all operations are available (for example greater than/less than, we need a separate index on particular key), but anyway this is incredibly useful for a wide range of tasks. And starting from Django 1. 9 this functionality is available out of the box. MongoDB doesn’t have analogue to @&gt; operator. We can adopt the structure of data to make similar queries, using one index. But it is less effective, than in PostgreSQL. Because index is applied only to one key. On the other hand MongoDB support higher range of operations when filtering on any one key, using single index. I really like JSON in PostgreSQL, it can be used in many tasks. And we have all advantages of SQL: joins and transactions, that are not presented in MongoDB. And now it is supported by Django ORM. Useful links:  Django JSONField docs PostgreSQL JSON type docs PostgreSQL JSON functions and operations docs Christophe Pettus - PostgreSQL Proficiency for Python People - PyCon 2015 (video) PostgreSQL and JSON: 2015. Christophe Pettus. PGConf US 2015 (slides) Asya Kamsky, Yandex 2014 MongoDB meetup"
    }, {
    "id": 4,
    "url": "https://st4lk.github.io/en/blog/2015/07/18/oauth-and-django-rest-framework.html",
    "title": "OAuth and django rest framework",
    "body": "2015/07/18 - Star This is a well known topic, but i can’t find the existing solution that will fully satisfy me. So i write it by myself :). Assume we have a “single page” web site, that is talking with backend via REST API. Client side can be written with ember, angularjs or some like this. Backend - django rest framework (DRF). We’ve got a task - add social login (OAuth protocol). How it will look like in case of simple “old-school” site? User press login, social network page is opening. Person confirms the access, resource redirects back to our web page, providing special code in redirected url. Using this code we do the login. For such process there are several existing libraries, my favourite is python-social-auth. In case of single page site we can do the same. But, often development of frontend and backend are separated. Moreover, API can run on different subdomen, so backend can’t handle redirected url. So instead following scheme looks the most optimal. User press login, new popup window is opened with social network confirmation. User says yes, popup gets parameters from social resource, propagates them back to parent window (our frontend application). Finally, frontend sends these parameters to backend to finish login process. Thus backend developer must implement API resource, that will take parameters from OAuth provider as input, and respond with user details as output, including session information (session id in cookies or token). Frontend will call this resource after receiving corresponding data from social network (OAuth provider). The question is, what data it is better to send to that resource? Let’s take OAuth 2. 0. We have two choices: request token or access token. In first case server will need to exchange request token to access token by itself. In second case access token is already given (it was acquired by frontend). At first sight, the second approach is easier. But it has several disadvantages. First, access token acquired by frontend have very short lifetime (couple of hours). We could save the access token in database for later usage (for example write something on user’s wall). With short lifetime it will be not trivial. Second, access token will be sent by frontend to our server. It is a potential security issue. If our API is not running on HTTPS, then access token can be easily eavesdropped. This token is enough to make valid requests. After searching for existing solutions, i’ve found django-rest-auth. It suggest resource, that takes access token as input. But there is no resource, that will take request token as input and do the rest of the work by itself. Also this package is built on top of django-allauth, which is not my favourite. After summing all the things being said, i decided to write my own tool that will link django-rest-framework, python-social-auth and login resource with request token as input. Here it is: django-rest-social-auth. Details can be found in readme. This package is very small but useful (to my mind of course). All the customisation from python-social-auth is still available. Live working example - site woobie. ru, where this package is used. "
    }, {
    "id": 5,
    "url": "https://st4lk.github.io/en/blog/2015/06/05/tornado-and-pgettext.html",
    "title": "Tornado and pgettext",
    "body": "2015/06/05 -  Recently (26. 05. 2015) new tornado 4. 2 was released. It contains different updates, the most valuable i suppose are modules tornado. locks and tornado. queues. They migrated from package Toro, look detailed explanation in Jesse Jiryu Davis post. Here i want to tell about another helpful function, that was added with my help - pgettext. It can be useful, when you are creating the translation for ambiguous strings. Let’s say we have word “bat” and we want to show it either in english, either in russian, depending on user’s preferred language. Special translate functions can be used to implement it. For example in html template: &lt;div&gt;{{_( Bat )}}&lt;/div&gt;Next we’ll use util xgettext to create translation file, that will contain something like this (details of i18n process can be found here) msgid  Bat msgstr   So now in place of empty string we need to put a translation. But what does the word “Bat” mean? Is it a mammal or a club? It will be very hard for translator to understand exact meaning without knowing the context. That’s where function pgettext will help, it accept context as the first argument: &lt;div&gt;{{ pgettext( mammal ,  Bat ) }}&lt;/div&gt;To generate translations add following options to xgettext: –keyword=pgettext:1c,2 –keyword=pgettext:1c,2,3 After that translation will look like this: msgctxt  mammal msgid  Bat msgstr   Now it is clear what was meant and we can translate definitely: msgctxt  mammal msgid  Bat msgstr  Летучая мышь Plural forms are also supported: &lt;div&gt;{{ pgettext( mammal ,  Bat ,  Bats , 2) }}&lt;/div&gt;Python code example: class HomeHandler(tornado. web. RequestHandler):  def get(self):    self. render( home. html , text=self. locale. pgettext( mammal ,  Bat ,  Bats , 2))Plural form translations: msgctxt  mammal msgid  Bat msgid_plural  Bats msgstr[0]  Летучая мышь msgstr[1]  Летучие мыши msgstr[2]  Летучих мышей "
    }, {
    "id": 6,
    "url": "https://st4lk.github.io/en/blog/2015/05/16/oauth-step-step.html",
    "title": "OAuth step by step",
    "body": "2015/05/16 -   Gist OAuth protocol has two versions: 1. 0 and 2. 0. Most of services today use version 2. 0, i suppose because it is easier to implement. Also, 2. 0 can be realized in standalone applications (those, that don’t have a server). To understand the protocols very useful to have a look at their realisation. Here i’ll show several scripts that talk to OAuth providers of different versions. Scripts will implement client application functionality. Only standard python libraries are used. This help to overview the OAuth protocol - everything is on single screen and familiar. Of course, for production application we must use third party oauth libs, they handle many special cases and so on. Purpose of these scripts is just understanding of the protocol and nothing else. It is often hard to keep the protocol flow in production-ready library, because it is splitted in many modules, some other packages are used. And the full vision is slipping out of sight. Let’s refresh in mind some theory first. For sure you know, that there are two objectives: authentication and authoriztion. They look very similar, but a little different in fact. So, just to remember:    authentication - process of identifying someone.   We need to know, does this person really an owner of google account with exact id? This information is enough for us. Just login him and that’s it, we don’t need additional information or do something on behalf of the owner of account.   Such work is done for example by OpenID protocol (although google suggest to to use another means, OpenID - deprecated).     authorization - process of acquiring rights to do something.   Authorization already includes authentication, but gives more power.  For example, not just identify some person, but also get his email and maybe post a message on his wall.   That is what OAuth protocol for.  To remember i use word “author”. If objective contains “author” - we are talking about permissions (authority). Otherwise just an identification. OAuth 1. 0: Specification: http://tools. ietf. org/html/rfc5849 The main part to remember about OAuth 1. 0 - it signs all requests with secret key. Secret key must be kept in safe place, the only one is server. Protocol provides fully security, even if https is not used. By saying “security” i mean the following: once the request was eavesdropped, the hacker can’t create new valid request. Of course he can get the data being transmitted, to hide the data we still need the https. OAuth 1. 0 less-legged (2-legged, 1-legged, 0-legged): It is a modification of OAuth 1. 0, were user is not interacted in process. Formally speaking, it is not an OAuth anymore, as specification doesn’t describe such modification. Just same means are used. In this case client application is acting as a user. It can request either public resources, either his own resources (even private). OAuth 2. 0 with server: Specification: http://tools. ietf. org/html/rfc6749 Interesting, that in title of OAuth 2. 0 specification it is called “framework”. Whereas in title of OAuth 1. 0 specification it is called protocol. To get full security with OAuth 2. 0, https is mandatory (service provider must use https, for example facebook). Once the access_token is acquired, secret key is not needed. So if someone will steal the access_token, he can make a valid request. That is why https is needed, to hide access_token. Also, during acquiring the access_token, secret key is transmitted by HTTP as is. access_token always have limited time to live. Because of all said above (and something else), one of creators of OAuth 1. 0 protocol has left the OAuth 2. 0 team, as 2. 0 is very easy to implement insecurely. For details follow this link. Here is a sequence of steps to get OAuth 2. 0 access_token, if you have a server. Server use secret key to get an access_token. Pay attention, no crypto library is used. OAuth 2. 0 without server: OAuth 2. 0 client can be implemented without server. In that case we also get an access_token, but we don’t need to know the secret key at all! Usually such access_token have a small time to live (1-2 hours), whereas time to live of access_token, acquired by server, is longer (can be several tens of days). "
    }, {
    "id": 7,
    "url": "https://st4lk.github.io/en/blog/2015/04/30/base-python-tips-tricks.html",
    "title": "Python tips & tricks",
    "body": "2015/04/30 -  Recently i’ve read the book Learning Python, 5th Edition by Mark Lutz. Here is a list of most interesting insights for me.    set generation:    {x for x in [1,2]} set(x for x in [1,2]) assert set(x for x in [1,2]) == {x for x in [1,2]}      dict generation:    {x:x**2 for x in [1,2]} dict((x, x**2) for x in [1,2]) assert {x:x**2 for x in [1,2]} == dict((x, x**2) for x in [1,2])      division of integers   In python 3 division of integers returns float    &gt;&gt;&gt; 1 / 2 0. 5 &gt;&gt;&gt; - 1 / 2 -0. 5    In python 2 division of integers - round to floor, it is not truncate    &gt;&gt;&gt; 1 / 2  # 0. 5 round floor -&gt; 0 0 &gt;&gt;&gt; - 1 / 2 # -0. 5 round floor -&gt; -1 (not 0) -1    In python 2 and 3 round to floor integer division    &gt;&gt;&gt; 1 // 2 0 &gt;&gt;&gt; - 1 // 2 -1 &gt;&gt;&gt; 13 // 2. 0 6. 0      is - check, that variables point to the same address, == - check, that variables have same values     python 3: [1, 'spam']. sort() raises exception (different types)     python 3: dict(). keys() returns iterator (view object, linked to dict). It is set-like object, we can apply set operations to it (union and so on)    &gt;&gt;&gt; dict(a=1, b=2). keys() dict_keys(['b', 'a']) &gt;&gt;&gt; dict(a=1, b=2). keys() | {'c', 'd'} {'b', 'd', 'a', 'c'}      frozenset - immutable set, hashable, can be used as key in dict    &gt;&gt;&gt; fz = frozenset([1,2]) &gt;&gt;&gt; fz. add(3) AttributeError: 'frozenset' object has no attribute 'add' &gt;&gt;&gt; {fz: 5} {frozenset([1, 2]): 5}      list support compare operators: ==, &lt;, &gt;, &lt;=, &gt;=. List compare is similiar to string compare. In py3 all objects must be the same type    &gt;&gt;&gt; [1, 2] == [1, 2] True &gt;&gt;&gt; [2, 2] &gt; [1, 2] True &gt;&gt;&gt; [1] &gt; ['sh'] # python2 False &gt;&gt;&gt; [1] &gt; ['sh'] # python3 TypeError: unorderable types: int() &gt; str()      dict compare   python 2 and 3    &gt;&gt;&gt; dict(a=1) == dict(a=1) True    python 2 only    &gt;&gt;&gt; dict(a=3) &gt; dict(a=2) True &gt;&gt;&gt; dict(a=3) &gt; dict(a=2, b=1) False      list + string, list + tuple is forbidden, but list += string is allowed    &gt;&gt;&gt; L = [] &gt;&gt;&gt; L + 'spam' TypeError: can only concatenate list (not  str ) to list &gt;&gt;&gt; L = [] &gt;&gt;&gt; L += 'spam' &gt;&gt;&gt; L ['s', 'p', 'a', 'm']      L += a is faster than L = L + a.     L += [1,2] is in place modification! (new list is not created)    &gt;&gt;&gt; L = [] &gt;&gt;&gt; id(L) 4368997048 &gt;&gt;&gt; L += [1,2] &gt;&gt;&gt; id(L) 4368997048 &gt;&gt;&gt; L = L + [1,2] &gt;&gt;&gt; id(L) 4368996976      ‘spam’[0][0][0] can last forever, every time we’ll get single-char-string ‘s’     variables unpack in python 3    &gt;&gt;&gt; a, *b = 'spam' &gt;&gt;&gt; a 's' &gt;&gt;&gt; b ['p', 'a', 'm'] &gt;&gt;&gt; *a, b = 'spam' &gt;&gt;&gt; a ['s', 'p', 'a'] &gt;&gt;&gt; b 'm' &gt;&gt;&gt; a, *b, c = 'spam' &gt;&gt;&gt; a 's' &gt;&gt;&gt; b ['p', 'a'] &gt;&gt;&gt; c 'm'      python 2: True = 0, but not in python 3   python 2    &gt;&gt;&gt; True = 0 &gt;&gt;&gt; True 0    python 3    &gt;&gt;&gt; True = 0 SyntaxError: can't assign to keyword      sys. stdout = open(‘temp. txt’, ‘w’) - all prints goes to file temp. txt     and, or returns object, not True/False     while has else     python 3: . . . is the same as pass     reversed works with lists, not generator    &gt;&gt;&gt; reversed([1,2,3]) &lt;list_reverseiterator object at 0x10127c550&gt; &gt;&gt;&gt; reversed((x for x in [1,2,3])) TypeError: argument to reversed() must be a sequence      zip iterates until the smallest sequence    &gt;&gt;&gt; [x for x in zip([1,2,3], [4,5])] [(1, 4), (2, 5)]      python 2: map(None, s1, s2) is the same as zip, but iterates until longest sequence. Insert None for elements without pair.   python 2    &gt;&gt;&gt; map(None, [1,2,3], [4,5]) [(1, 4), (2, 5), (3, None)] &gt;&gt;&gt; map(None, [1,2], [4,5,6]) [(1, 4), (2, 5), (None, 6)]    python 3    &gt;&gt;&gt; list(map(None, [1,2,3], [4,5])) TypeError: 'NoneType' object is not callable      map can take more than one iterators (similiar to zip)   python 2    &gt;&gt;&gt; map(lambda x, y: (x, y), [1,2], [3,4]) [(1, 3), (2, 4)] &gt;&gt;&gt; map(lambda x, y: (x, y), [1,2], [3,4,5]) [(1, 3), (2, 4), (None, 5)]    python 3    &gt;&gt;&gt; list(map(lambda x, y: (x, y), [1,2], [3,4])) [(1, 3), (2, 4)] &gt;&gt;&gt; list(map(lambda x, y: (x, y), [1,2], [3,4,5])) [(1, 3), (2, 4)]      nested list comprehensions    &gt;&gt;&gt; [x+y for x in 'abc' for y in 'lmn'] ['al', 'am', 'an', 'bl', 'bm', 'bn', 'cl', 'cm', 'cn'] # flat list of lists &gt;&gt;&gt; csv = [[1, 2, 3], [4, 5, 6], [7, 8, 9]] &gt;&gt;&gt; [col for row in csv for col in row] [1, 2, 3, 4, 5, 6, 7, 8, 9]      sorted returns list (not generator) in py2 and py3    &gt;&gt;&gt; sorted(x for x in [2,1,3]) [1, 2, 3]      *args accept any iterator, not only list     unzip: zip(*zip(a,b))    &gt;&gt;&gt; zip(*zip([1,2],[3,4])) [(1, 2), (3, 4)]      py3: map returns generator, it can be iterated only once    &gt;&gt;&gt; m = map(lambda x: x, [1,2,3]) &gt;&gt;&gt; [x for x in m] [1, 2, 3] &gt;&gt;&gt; [x for x in m] []      py3: range is not a simple generator, it support len() and index access    &gt;&gt;&gt; r = range(10) &gt;&gt;&gt; r range(0, 10) &gt;&gt;&gt; len(r) 10 &gt;&gt;&gt; r[3] 3      generator allows only single scan     cycle import works! But only for import without from     try has else, will be execution when no exception happened     with similar to finally     except (name1, name2) - orders from top to bottom, from left to right     except Exception: vs except: - first doesn’t catch system errors (KeyboardInterrupt, SystemExit, GeneratorExit например)     set(). remove(x) - removes x or KeyError, set(). discard(x) - removes x or nothing     py3. 3+ accept u””, U”” for backwards compatibility with py2     default encoding is in sys module sys. getdefaultencoding()   python 2    &gt;&gt;&gt; sys. getdefaultencoding() 'ascii'    python 3    &gt;&gt;&gt; sys. getdefaultencoding() 'utf-8'      [c for c in sorted([1,2,3], key=lambda c: -c)] - variable c will not conflict here     in py2 variable inside comprehension can change outer variables and also accessible after, in py3 - not   python 2    &gt;&gt;&gt; x = 1 &gt;&gt;&gt; [x for x in range(3)] [0, 1, 2] &gt;&gt;&gt; x 2 # creates new var &gt;&gt;&gt; [y for y in range(3)] [0, 1, 2] &gt;&gt;&gt; y 2    python 3    &gt;&gt;&gt; x = 1 &gt;&gt;&gt; [x for x in range(3)] &gt;&gt;&gt; x 1 # no new var &gt;&gt;&gt; [y for y in range(3)] [0, 1, 2] &gt;&gt;&gt; y NameError: name 'y' is not defined      py3 has nonlocal statement. It is used to reference the variable in outer def block (in py2 it is not possible to access such variable)    def f():   x = 2 # local for f   def g():     nonlocal x # python3 only     x = 3 # local for g   g()   print(x) &gt;&gt;&gt; f() # python3 only 3 &gt;&gt;&gt; f() # with commented nonlocal 2      LEGB rule (local, enclosing, global, builtin) or LNGB (N=nonlocal) - order of variable search in python     py3 exception variable as name is removed after block execution (even if variable was declared before try block)   python 2    &gt;&gt;&gt; x = 1 &gt;&gt;&gt; try: . . .   1/0 . . . except Exception as x: . . .   pass &gt;&gt;&gt; x ZeroDivisionError('integer division or modulo by zero',)    python 3    &gt;&gt;&gt; x = 1 &gt;&gt;&gt; try: . . .   1/0 . . . except Exception as x: . . .   pass &gt;&gt;&gt; x NameError: name 'x' is not defined      override builtin and undo override    &gt;&gt;&gt; open = 99 &gt;&gt;&gt; open 99 &gt;&gt;&gt; del open &gt;&gt;&gt; open &lt;built-in function open&gt;      py2 fun: __builtins__. True = False     lambda can take default arguments     nonlocal functionality can be replaced by mutable object or function attribute    def f():   x = [1]   def g():     print x[0]     x. append(2)   g()   print x &gt;&gt;&gt; f() 1 [1, 2] def f():   x = 1   def g():     print g. x     g. x = 2   g. x = x   g()   print g. x &gt;&gt;&gt; f() 1 2      py3 keyword only arguments    def f(*args, name):   print( args , args)   print( name , name) &gt;&gt;&gt; f(1, 2) TypeError: f() missing 1 required keyword-only argument: 'name' &gt;&gt;&gt; f(1, 2, name=3) args (1, 2) name 3 def f(*args, name=3):   print( args , args)   print( name , name) &gt;&gt;&gt; f(1, 2) args (1, 2) name 3      in py3 there is unpack of variables, it returns list. And arguments unpack in function call returns tuple   python 2 and 3    def f(a, *b):   print(b) &gt;&gt;&gt; f(1, *[2, 3]) (2, 3)    python 3    &gt;&gt;&gt; a, *b = [1, 2, 3] &gt;&gt;&gt; print(b) [2, 3] &gt;&gt;&gt; a, *b = (1, 2, 3) &gt;&gt;&gt; print(b) [2, 3]      add list to the beginning of existing list: L[:0] = [1, 2, 3]     get and set maximum recursion limit    &gt;&gt;&gt; sys. getrecursionlimit() # 1000 &gt;&gt;&gt; sys. setrecursionlimit(10000) &gt;&gt;&gt; help(sys. setrecursionlimit)      function arguments    &gt;&gt;&gt; def f(a): . . .   b = 1 . . .  &gt;&gt;&gt; f. __name__ 'f' &gt;&gt;&gt; f. __code__. co_varnames ('a', 'b') &gt;&gt;&gt; f. __code__. co_argcount 1      in py3 we can add annotations to function arguments. This information is saved in func. __annotations__. Nothing is done automatically with annotations, but we can work with them manually (for example for checking type and range of argument from decorator)    &gt;&gt;&gt; def func(a: 'spam', b: (1, 10), c: float): . . .   return a + b + c &gt;&gt;&gt; func. __annotations__ {'b': (1, 10), 'c': &lt;class 'float'&gt;, 'a': 'spam'} # default values &gt;&gt;&gt; def func(a: 'spam'=4, b: (1, 10)=5, c: float=0. 1): . . .   return a + b + c      it is impossible to use = in lambda, but it is possible to use setattr, __dict__     operator module in std lib    import operator as op reduce(op. add, [2, 4, 6]) # same as reduce(lambda x, y: x+y, [2, 4, 6])      KISS: Keep It Simple [Sir/Stupid]     comprehension vs map in general (better test on your system)   map(lambda x: x . . ) slower than [x for x . . ]   [ord(x) for x . . ] slower than map(ord for x . . )   map(lambda x: L. append(x+10), range(10)) even slower than for x in range(10): L. append(x+10)     unpacking in lambda differs in py2 and py3   python 2    &gt;&gt;&gt; map(lambda (a, b, c): a, [(0,1,2), (3,4,5)]) [0, 3]    python 3    &gt;&gt;&gt; list(map(lambda (a, b, c): a, [(0,1,2), (3,4,5)])) SyntaxError: invalid syntax &gt;&gt;&gt; list(map(lambda a, b, c: a, [(0,1,2), (3,4,5)])) TypeError: &lt;lambda&gt;() missing 2 required positional arguments: 'b' and 'c' &gt;&gt;&gt; list(map(lambda row: row[0], [(0,1,2), (3,4,5)])) [0, 3]      many builtin functions accept generators, no additional parenthesis are needed    &gt;&gt;&gt;   . join(str(x) for x in [1, 2]) '12' &gt;&gt;&gt; sorted(str(x) for x in [1, 2]) ['1', '2']    but for args () is needed    &gt;&gt;&gt; sorted(str(x) for x in [1, 2], reverse=True) SyntaxError: Generator expression must be parenthesized if not sole argument &gt;&gt;&gt; sorted((str(x) for x in [1, 2]), reverse=True) ['2', '1']      py3: yield from iterator (following functions are the same)    def f():   for i in range(5):     yield i def g():   yield from range(5)      put last list element to the beginning    L = L[1:] + L[:1]      zip for single list    &gt;&gt;&gt; zip([1,2,3]) [(1,), (2,), (3,)]      map and zip are similiar    map(lambda x,y: (x,y), S1, S2) == zip(S1, S2)      python -m script_name - runs module (module is a . py file), that can be found from current search path. Module can be placed somewhere in site-packages folder, but is run as main (__name__ = '__main__'). If script_name is a package (folder with __init__. py), then file __main__. py will be launched. If no such file, then error. Some modules are smart and accepts arguments from command line, for example timeit: python -m timeit ' - . join(str(n) for n in range(100))'     there is no direct way to use global and local variable with same name simultaneously. We can play with __main__. my_global_var    # OK X = 99 def f():   print(X) &gt;&gt;&gt; f() 99 # ERROR def f():   print(X) # &lt;- error   X = 99 &gt;&gt;&gt; f() UnboundLocalError: local variable 'X' referenced before assignment # global everywhere def f():   global X   print(X)   X = 88 # hack with main def f():   import __main__   print(__main__. X)   X = 88      square root performance    math. sqrt(x) # fastest x ** . 5 # fast pow(x, . 5) # slow      py3. 2+ creates folder __pycache__ for saving bytecode of different python versions there and to reuse them in future. There are no *. pyc files outside this folder now.     . pyc for main script (__name__ = '__main__') is not created, only for import     import search order (look at sys. path):      home of program (+ in some versions current dir, from where program is launched, i. e. current dir)   PYTHONPATH   std lib dir   content of any . pth file (if exists)   site-packages dir      sys. path can be changed at runtime, this will impact all program     python -O creates a little bit optimized bytecode . pyo instead of . pyc, it ~5% faster. Also this flag removes all asserts from code. And changes value of var __debug__    # main. py print __debug__ assert True == False # python main. py True AssertionError # python -O main. py False      in py2 in function we can do from some_module import *, but with warning. In py3 - error    # python 2 def f():   from urllib import *   print('after import') &gt;&gt;&gt; f() SyntaxWarning: import * only allowed at module level after import # python 3 &gt;&gt;&gt; f() SyntaxError: import * only allowed at module level      reload doesn’t update objects, that are loaded with from: from x import y. y will not be reloaded after reload(x)     reload doesn’t update c modules     py3: in package there is no package folder in sys. path. If module in package needs to import another module from the same package, relative import must be used: from . import smth. However, if module is launched as main program (__main__), then package folder is in sys. path.     py2: from __future__ import absolute_import makes import in py2 the same as in py3. It allows to import module string from standard library in following case very easy:    mypkg ├── __init__. py ├── main. py # import string from std here? └── string. py      relative import is forbidden outside the package    # test. py from . import a # python 2 python test. py ValueError: Attempted relative import in non-package # python 3 python test. py SystemError: Parent module '' not loaded, cannot perform relative import      cons of relative import:      module with relative imports can’t be used as script (__main__). Solution: use absolute import with package name at the beginning   derives from previous point: we can’t launch tests, that are executed when running module as main program      in py3. 3+ there are namespace packages. They don’t have __init__. py. Two (or more) namespace packages with same name can be placed in different locations in sys. path. Modules from those packages will be aggregated under same package name. If modules have same name - first found in sys. path will be taken. Namespace package always has lower priority under regular package (with __init__. py). When regular package is found - all found namespace packages with that name are discarded, normal package is used instead. Namespace package import process is slow.    # collect modules in namespace package current_dir └── mypkg   └── mymod1. py site-packages └── mypkg   └── mymod2. py &gt;&gt;&gt; import mypkg. mymod1 &gt;&gt;&gt; import mypkg. mymod2 # redefine module in namespace package current_dir └── mypkg   └── mymod1. py   └── mymod2. py site-packages └── mypkg   └── mymod2. py &gt;&gt;&gt; import mypkg. mymod1 &gt;&gt;&gt; import mypkg. mymod2 # current_dir. mypkg. mymod2 # regular package is used current_dir └── mypkg   └── mymod1. py site-packages └── mypkg   └── mymod2. py another-packages └── mypkg   └── mymod1. py &gt;&gt;&gt; import sys &gt;&gt;&gt; sys. append('another-packages') &gt;&gt;&gt; import mypkg. mymod1 # another-packages. mypkg. mymod1 &gt;&gt;&gt; import mypkg. mymod2 ImportError: No module named 'mypkg. mymod2'      In py2 and py3 new-style classes (inherent from object), when operator is applied, corresponding magic methods are searched in class, ignoring instance (__getattr__, __getattribute__ are not invoked). But on direct call of magic method instance is not ignored (__getattr__, __getattribute__ are invoked).    class A(object):   def __repr__(self):     return  class level repr    def normal_method(self):     return  class level normal method  def instance_repr():   return  instance level repr  def instance_normal_method():   return  instance level normal method  a = A() print(a) # class level repr print(a. normal_method()) # class level normal method a. __repr__ = instance_repr a. normal_method = instance_normal_method print(a) # class level repr print(a. normal_method()) # instance level normal method print(a. __repr__()) # instance level repr      ZODB - object database for python objects, support ACID-compatible transactions (including savepoints)     slice object:    L[2:4] == L[slice(2,4)]      iteration context (for, while, …) will try      __iter__       __getitem__     class Gen(object):   def __getitem__(self, index):     if index &gt; 5:       raise StopIteration()     return index for x in Gen():   print x, # output 0 1 2 3 4 5             for calls __iter__(). Then calls method returned_object. __next__() (in py2 . next()), until StopIteration. It is possible to use yield __item__(): yield smth, then no need to define __next__.     __call__ is invoked, when parentheses () are applied to instance, not to class    class A(object):   def __call__(self):     print( call ) a = A() # nothing a() # print call      __eq__ = True doesn’t mean, that __ne__ = False     boolean context:      __bool__ (__nonzero__ in py2)   __len__   True      OOP patterns      inheritance - “is a”   composition - “has a” (container stores other objects)   delegation - special case of composition, when only one object is stored. Wrapper implement same interface, but add some intermediate steps.       class attributes (including methods), that start with double underscores __, but don’t end with them, have special behaviour. They do not overlap with same named attributes in child classes. In __dict__ they are stored as _ClassName__attrname.    class A(object):   __x = 1   def show_a(self):     print self. __x class B(A):   def show_b(self):     print self. __x &gt;&gt;&gt; a = A() &gt;&gt;&gt; a. show_a() 1 &gt;&gt;&gt; b = B() &gt;&gt;&gt; b. show_a() 1 &gt;&gt;&gt; b. show_b() AttributeError: 'B' object has no attribute '_B__x' class B(A):   __x = 2   def show_b(self):     print self. __x &gt;&gt;&gt; b = B() &gt;&gt;&gt; b. show_a() 1 &gt;&gt;&gt; b. show_b() 2      in py3 in class method we can suppress self argument and use that method only from class (not from instance) - it will behave as static method. But not in py2.    class A(object):   def f():     print( f ) # python 2 &gt;&gt;&gt; A. f() TypeError: unbound method f() must be called with A instance as first argument (got nothing instead) # python 3 &gt;&gt;&gt; A. f() f &gt;&gt;&gt; a = A() &gt;&gt;&gt; a. f() TypeError: f() takes 0 positional arguments but 1 was given      bound function:    class A(object):   def f(self):     pass a = A() print(a. f. __self__) # that is where self is saved      attribute search in classic (old-style) and new-style classes:      classic. DFLR: Depth First, Left to Right   new-style. Diamond pattern, L-R, D-F; MRO (more complex, that just LRDF)    MRO guards class, from which &gt;= 2 other classes are subclassed, from being search twice. So class will be searched only once.    # python 2 old-style class A: attr = 1 class B(A): pass class C(A): attr = 2 class D(B,C): pass &gt;&gt;&gt; x = D() &gt;&gt;&gt; print(x. attr) # x, D, B, A 1 # python 2 new-style class A(object): attr = 1 class B(A): pass class C(A): attr = 2 class D(B,C): pass &gt;&gt;&gt; x = D() &gt;&gt;&gt; print(x. attr) # x, D, B, C 2 # scheme A   A |   | B   C \   /   |   D   |   X    Check search order in new-style (mro algorithm):    &gt;&gt;&gt; D. __mro__ (&lt;class '__main__. D'&gt;, &lt;class '__main__. B'&gt;, &lt;class '__main__. C'&gt;, &lt;class '__main__. A'&gt;, &lt;type 'object'&gt;) &gt;&gt;&gt; D. mro() # same as list(D. __mro__) [&lt;class '__main__. D'&gt;, &lt;class '__main__. B'&gt;, &lt;class '__main__. C'&gt;, &lt;class '__main__. A'&gt;, &lt;type 'object'&gt;]      format() calls method __format__. If it is not exist, then TypeError in py2.   python 2    &gt;&gt;&gt; print('{0}'. format(object)) &lt;type 'object'&gt; &gt;&gt;&gt; print('{0}'. format(object. __reduce__)) TypeError: Type method_descriptor doesn't define __format__ # call __str__ explictly &gt;&gt;&gt; print('{0!s}'. format(object. __reduce__)) &lt;method '__reduce__' of 'object' objects&gt;    python 3. 4    &gt;&gt;&gt; print('{0}'. format(object. __reduce__)) &lt;method '__reduce__' of 'object' objects&gt;    python 2 &amp; 3    class A(object):   def __format__(self, *args):     return  A. __format__    def __str__(self):     return  A. __str__  &gt;&gt;&gt; a = A() &gt;&gt;&gt;  {0} . format(a) 'A. __format__' &gt;&gt;&gt; print(a) A. __str__ &gt;&gt;&gt; '%s' % a 'A. __str__'      __dict__ doesn’t contain “virtual” attributes:      new-style properties (@property)   slots   descriptors   dynamic attrs computed with tools like __getattr__      MRO - method resolution order     diamond pattern - special case of ‘multi inheritance’, when 2 or more class can be child of the same class (object). This pattern is used in python.     proxy object, returned by super(), doesn’t work with operators:   python 3    class A(list):   def get_some(self):     return super()[0] &gt;&gt;&gt; a = A([1, 2]) &gt;&gt;&gt; a. get_some() TypeError: 'super' object is not subscriptable class A(list):   def get_some(self):     return super(). __getitem__(0) &gt;&gt;&gt; a = A([1,2]) &gt;&gt;&gt; a. get_some() 1    python 2    class A(list):   def get_some(self):     return super(A, self)[0] &gt;&gt;&gt; a = A([1,2]) &gt;&gt;&gt; a. get_some() TypeError: 'super' object has no attribute '__getitem__' class A(list):   def get_some(self):     return super(A, self). __getitem__(0) &gt;&gt;&gt; a = A([1,2]) &gt;&gt;&gt; a. get_some() 1      super()          super() pros:          if superclass need to be changed in runtime, we can’t do it without super: C. __bases__ = (Y, )           calls sequence of inherited methods in multi inheritance class, in MRO order.       If we’ll try to do it without super, we can call method of some class twice.        class A(object):   def __init__(self):     print( A ) class B(A):   def __init__(self):     print( B )     super(B, self). __init__() class C(A):   def __init__(self):     print( C )     super(C, self). __init__() class D(B, C):   pass &gt;&gt;&gt; d = D() B C A # A only once &gt;&gt;&gt; B. mro() [&lt;class '__main__. B'&gt;, &lt;class '__main__. A'&gt;, &lt;type 'object'&gt;] &gt;&gt;&gt; D. mro() [&lt;class '__main__. D'&gt;, &lt;class '__main__. B'&gt;, &lt;class '__main__. C'&gt;, &lt;class '__main__. A'&gt;, &lt;type 'object'&gt;]            Sequence of methods        class B(object):   def __init__(self):     print( B )     # for B super is C here, by MRO order     super(B, self). __init__() class C(object):   def __init__(self):     print( C )     # it is ok here to call super(). __init__     # because object also has __init__     super(C, self). __init__() class D(B, C):   pass &gt;&gt;&gt; d = D() B C                super will search attribute in MRO hierarchy. It will search all classes. So, for example hierarchy for super is the following: A, C. A doesn’t have attribute, whereas C has, then C. method will be used without error.               super() cons (or features):          when super is used, all methods in sequence must accept same arguments     super(). m - all classes must have method m and call super(). m, except last one, that must not call super.              inherit method from exact class:    class A(B, C):   other = C. other # not B other      finally block will be called even if exception was happened in except or else block     exception - always instance, even if raise ExceptionClass (without ()). Instance will be created automatically (without arguments):    raise Exception # == raise Exception() raise # reraise caught exception      py2, look for builtin exceptions:    import exceptions help(exceptions)      the downside of reading bytes from file and further manual decoding: if we’ll read by chunks, then some nasty case can happen, when one byte of one symbol will fall in first chunk, and another byte of same symbol - in second chunk. So it is better to use codecs. open in py2.     When file name is given in unicode, python will automatically decode and encode from/to bytes. When file name is in bytes, then no encoding is happen. Default encoding for file names:    &gt;&gt;&gt; sys. getfilesystemencoding() 'utf-8'      descriptor - class, that implement one of the following methods      __get__   __set__   __delete__      If descriptor doesn’t implement __set__, it doesn’t mean, that corresponding attribute is read-only. Attribute will be simply rewritten. To avoid it, implement __set__ with exception.     decorators can be combined, they will be called from bottom to top:    @A @B @C def f(): pass # same as f = A(B(C(f)))      decorator can accept arguments    @dec(a, b) def f(): pass # same as f = dec(a, b)(f) # implementation: def dec(a, b):   def actual_dec(f):     return f   return actual_dec    So decorator can include 3 levels of callables:      callable to accept decorator args   callable to serve as decorator   callable to handle calls to the original function      during class creation, two methods of class type are called:    type. __new__(type_class, class_name, super_classes, attr_dict) type. __init__(class, class_name, super_classes, attr_dict) # python 3 class Eggs: pass class Spam(Eggs):   data = 1   def method(self, arg): pass # same as Eggs = type('Eggs', (), . . . ) # in () object will be added automatically in python 3 Spam = type('Spam', (Eggs, ), {'data': 1, 'method': method, '__module__': '__main__'})      Set metaclass   python 2    class Spam(object):   __metaclass__ = Meta    Inherit from object is not mandatory, but if it is not present, and __metaclass__ is used, then result will be new-style anyway, and in __bases__ object will be present.  But better to use object explicitly, as there can be problems, for example with inheritance.   python 3    class Spam(Eggs, metaclass=Meta):   pass    attribute __metaclass__ is just ignored     Metaclass can not be a class itself. It just must return class. Function also can be a metaclass:    def meta_func(class_name, bases, attr_dict):   return type(class_name, bases, attr_dict) # python 2 class Spam(object):   __metaclass__ = meta_func      Regular classes also have method __new__. But it doesn’t create class, it is invoked at instance creation (takes class as input argument). This method calls __init__.     Magic methods of metaclass and class:    class Meta(type): pass    on creation of class Class (class Class(metaclass=Meta): . . . ) following methods are called:    Meta. __new__ Meta. __init__    on creation of instance of class Class (instance = Class(. . . )) following methods are called:    Meta. __call__   calls Class. __new__     calls Class. __init__    on calling of instance of class Class (instance()) following method is called:    Class. __call__      It is not mandatory to subclass metaclass from type. We can use simple class with __new__ method as metaclass. But in that case methods __init__ and __call__ will not be called:    class MySimpleMetaClass(object):   def __new__(cls, *args, **kwargs):     new_class = type. __new__(type, *args, **kwargs)     return new_class   def __init__(new_class, *args, **kwargs):     print( __init__ won't be called. . .  )   def __call__(*args, **kwargs):     print( __call__ won't be called. . .  )      Metaclass of some class will be invoked for all subclasses. When __new__ of metaclass is called for parent class, bases will contain (&lt;type 'object'&gt;,), and for subclass - parent class.     Metaclass attributes are inherited by class, not by instances of class.   python 2 (python 3 has some syntax differences)    class MyMetaClass(type):   attr = 2   def __new__(*args, **kwargs):     return type. __new__(*args, **kwargs)   def toast(*args, **kwargs):     print(args, kwargs) class A(object):   __metaclass__ = MyMetaClass    Metaclass is included in search sequence of class attributes    &gt;&gt;&gt; A. toast() ((&lt;class '__main__. A'&gt;,), {})    Interesting, that method from metaclass is bound, although is called from class, not from instance. In fact class - is an instance of metaclass:    &gt;&gt;&gt; A. toast &lt;bound method MyMetaClass. toast of &lt;class '__main__. A'&gt;&gt;    But metaclass is not present in instance attribute search sequence    &gt;&gt;&gt; a = A() &gt;&gt;&gt; a. toast() AttributeError: 'A' object has no attribute 'toast'    If some superclass has attribute with same name, as in metaclass, it has higher priority (no matter how deep superclass is)    class B(object):   attr = 1 class C(B):   __metaclass__ = MyMetaClass &gt;&gt;&gt; C. attr 1 # MyMetaClass. attr = 2 is ignored    Instance attributes are searched in its __dict__, next in all __dict__ of __class__. __mro__ Class attributes are searched also in __class__. __mro__, it is different class, from instance it will be __class__. __class__. __mro__.    &gt;&gt;&gt; inst = C() &gt;&gt;&gt; inst. __class__ -&gt; &lt;class '__main__. C'&gt; &gt;&gt;&gt; C. __bases__  -&gt; (&lt;class '__main__. B'&gt;,) &gt;&gt;&gt; C. __class__  -&gt; &lt;class '__main__. MyMetaClass'&gt;    Instance inherit attributes from all superclasses. Class - from superclasses and metaclasses. Metaclasses - from super-metaclasses (and probably from meta-metaclasess).   Data descriptors (those, that define __set__) brings some changes in attribute search order for instances.  For class instance, data descriptor will have higher priority in search, even if they are declared in superclasess:    class DataDescriptor(object):   def __get__(self, instance, owner):     print( DataDescriptor. __get__ )     return 5   def __set__(self, instance, value):     print( DataDescriptor. __set__ , value) class B(object):   attr = DataDescriptor() class C(B):   pass &gt;&gt;&gt; c = C() &gt;&gt;&gt; c. __dict__['attr'] = 88 &gt;&gt;&gt; c. attr DataDescriptor. __get__ 5 &gt;&gt;&gt; c. attr = 8 ('DataDescriptor. __set__', 8)    Descriptor was called, in spite of attribute with same name is present in c. __dict__.  Attribute doesn’t hide descriptor of superclass.  Such behaviour will not happen in case of nondata descriptor:    class SimpleDescriptor(object):   def __get__(self, instance, owner):     print( SimpleDescriptor. __get__ )     return 5 class B(object):   attr = SimpleDescriptor() class C(B):   pass &gt;&gt;&gt; c = C() &gt;&gt;&gt; c. attr SimpleDescriptor. __get__ 5 &gt;&gt;&gt; c. __dict__['attr'] = 88 &gt;&gt;&gt; c. attr 88    Also, for builtin operators that call magic methods implicitly, the search order is special. It ignores instance. __dict__, the search goes to __dict__ of classes from __mro__.     magic methods, that are called implicitly by builtin operators, are searched in metaclasses, ignoring the class (and all its superclasses)   python 2 (in python 3 syntax differs a little bit)    class MyMetaClass(type):   def __new__(*args, **kwargs):     return type. __new__(*args, **kwargs)   def __str__(cls):     return  __str__ from meta  class A(object):   __metaclass__ = MyMetaClass   def __str__(self):     return  __str__ from class A     Method MyMetaClass. __str__ will be called, not A. __str__    &gt;&gt;&gt; print A __str__ from meta    And here method object. __str__ will be called:    &gt;&gt;&gt; print MyMetaClass &lt;class '__main__. MyMetaClass'&gt;      Author Mark Lutz is a little upset, that python become too complicated nowadays. It have more than one obvious way to do some things:      str. format and %   with and try/finally    It goes contrary with import this Zen.  "
    }, {
    "id": 8,
    "url": "https://st4lk.github.io/en/blog/2015/04/17/listen-wifi-with-wireshark.html",
    "title": "Listen wifi with wireshark",
    "body": "2015/04/17 -  I always knew, that it is possible to catch wifi network packets. But haven’t done it in practise (i was analysing network packets, but not in HTTP protocol). So i decided to do it, as this is interesting and useful. Such experience help to understand TCP-IP and HTTP protocols and also to pay more attention for web security. We’ll spy the network traffic with Wireshark program. There are a lot of tools for such purpose (ngrep, tcpdump, mitmproxy), but Wireshark looks the most popular and have a reach functionality. Lets try to solve following tasks:  listen network packets, that are sent/recieved inside local machine listen network packets, that are sent/recieved by local machine to/from outer world (internet) listen network packets, that are sent/recieved by other members of public wifi network listen network packets, that are sent/recieved by other members of private wifi networkAll actions i performed on laptop MacBook Pro with OS X Yosemite, so on other devices there can be some differences. Disclaimer: all actions are on your own responsibility. Do not apply described technics to make bad things. Localhost network packets: Install wireshark. Launch it, go to Capture -&gt; Intefaces.  My laptop is connected to wifi only (en0 interface). As i understand, awdl0 is a cable network. No cable is connected, so we don’t see any packets. And lo0 is a localhost interface, lets work with it now. Put checkbox at lo0 and press Start. To concentrate on HTTP protocol, set Display filter: http (this filter will be applied to already fetched and decoded packets, unlike Capture filter, which i’ll describe later): We are gonna catch packets, that will be sent from browser to django development server and back. There are some things about django server, that worth mentioning.  First, it respond HTTP 1. 0, not HTTP 1. 1 Second, the most important, response can not include neither Content-Length: &lt;response length&gt;, neither Transfer-Encoding: chunked. In that case, to determine the end of HTTP response we need to wait for server to close the connection. But it will not happen. HTTP data can be transmitted in several TCP segments and wireshark smart enough to group those segments and to show final HTTP response. But, as wireshark can’t understand exactly when response is completed, it will not group them in HTTP frame. Well, this is not very bad, as we can always look for response to particular request by clicking on it and apply Analyse -&gt; Follow TCP Stream. But, it will be great to see the HTTP response in frame list. To do it, prepend ConditionalGetMiddleware to the list of MIDDLEWARE_CLASSES: if DEBUG:  MIDDLEWARE_CLASSES = (    'django. middleware. http. ConditionalGetMiddleware',  ) + MIDDLEWARE_CLASSESMiddleware will set Content-Length header. It is not necessary, but useful in case of working with wireshark and django dev server. In other cases all work correctly: production servers usually set Transfer-Encoding: chunked and respond HTTP by chunks (probably it is done by proxy server (nginx, apache, etc)). Now start django project. The main page just shows the name of current user and login form. If current user is anonymous, then his name will be “AnonymousUser”. For the purity of the experiment clean cookies in browser for 127. 0. 0. 1. Open page http://127. 0. 0. 1:8000/. If we have not add ‘ConditionalGetMiddleware’, then we’ll probably see only request: The response still can be found, if we choose Analyse-&gt;Follow TCP Stream: And with ‘ConditionalGetMiddleware’ the response will be visible in list of frames: Content-Length is set: Well, it wasn’t very interesting. But lets try to login! Enter username and password and press Login. In wireshark we’ll see 4 new frames: POST request, redirect to main page (302 code), GET / request, response for GET: Look at fetched data more carefully. Frame with POST request, along with HTTP headers contain form data. Here how they look like: Login and password as plain text. Response to POST request was 302 redirect. In that response server ask client to store session id in cookies: Next goes GET request for main page, cookies now contain session id: So this is how we can spy the network data, that is sent from client to server and back. All these information will be transmitted in wifi network by the same way (if non secure HTTP protocol is used). If we login - login and password are sent in plain text. If we just make a requst - our session id is visible. With session id it is easy to access the site as the owner of that session. For simplicity, we can check that with console tool (curl, httpie). Example with httpie: $ http 127. 0. 0. 1:8000  Cookie: sessionid=tmpocxkz6zsir6xe6i03kspucvlqq385 HTTP/1. 0 200 OKContent-Length: 567Content-Type: text/html; charset=utf-8Date: Thu, 16 Apr 2015 13:06:58 GMTServer: WSGIServer/0. 1 Python/2. 7. 6Set-Cookie: csrftoken=3bUoLB28WyzcH7qG5GXreWPm0Pj11861; expires=Thu, 14-Apr-2016 13:06:58 GMT; Max-Age=31449600; Path=/Vary: CookieX-Frame-Options: SAMEORIGIN  &lt;html&gt;    &lt;body&gt;      Hello, alex      &lt;div&gt;        &lt;form action= /login/  method= POST &gt;          &lt;input type= text  name= username  /&gt;          &lt;input type= password  name= password  /&gt;          &lt;input type= text  hidden name= next  value= /  /&gt;          &lt;input type='hidden' name='csrfmiddlewaretoken' value='3bUoLB28WyzcH7qG5GXreWPm0Pj11861' /&gt;          &lt;input type= submit  name= Login  value= Login  /&gt;        &lt;/form&gt;      &lt;/div&gt;    &lt;/body&gt;  &lt;/html&gt;Hello, alex is shown, so the server treat us as alex. Network packets of local maching sent to outer world: Listen for network packets of our computer. In wireshark choose Capture -&gt; Intefaces, apply en0 interface and press Start: I access the admin page of this site (lexev. org). In wireshark set Display filter http. request. full_uri contains lexev. orgto see data flow only with host lexev. org. Here what we get: We have now a session id of admin user. Network packets of other members of public wifi network: So far we were listening just our request and responses. But lets try to listen for other people. Go to cafe with public wifi, launch wireshark. Choose corresponding interface in Capture -&gt; Intefaces and press Options (not Start). We’ll see something like this: Double click on interface and put Capture packets in monitor mode checkbox: Ok, Start. Now we are listening entire wifi network (excluding yourself). In public network fly a lot of packets, we can easily fetch more than Gb information in hour. Hard to work with such amount of data, that is where Capture filters can help. Packets that are discarded by capture filters will not be saved. Unlike display filters, they are applied to not yet processed and decrypted packets, therefore it is harder to construct the filter. Here is a capture filter for HTTP GET and POST requests only on 80 port: (port 80 and tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x47455420) or (tcp dst port 80 and (tcp[((tcp[12:1] &amp; 0xf0) &gt;&gt; 2):4] = 0x504f5354))Apply it at interface Options: Lets try to connect to wifi from another device (phone for example) and send GET request to lexev. org from it. For convenience add same display filter to show only requests to lexev. org (we can combine capture and display filters). Session id is visible again: Just for fun, lets try to login. Look for my password everyone :): Network packets of other members of private wifi network: For example we have a wifi with WPA protection. Choose corresponding interface, enable monitor mode for it (the same way as for public network, just without capture filter) and listen. We’ll see something like this: The data is encrypted. But, if we have a wifi password, we can decrypt it! Go to Edit -&gt; Preferences. Choose Protocol -&gt; IEEE 802. 11.  Press Edit for Decryption Keys. Press new. In popup enter: Key type: wpa-pwdKey: password:wifiname Apply, ok. Now wireshark will decrypt the packets and we can see HTTP data as before: Spy public wifi: I went to macdonalds with public wifi and for about an hour run wireshark (save only GET and POST HTTP requests). Saved fetched data into pcap file (File -&gt; Save as). The question is, how to analyse such big amount of frames? Sometimes it is useful to export interesting data into CSV. Use tshark tool for that. Save fields “frame number”, “HTTP method”, “full url” into results. csv: tshark -r macdak_pushkin_get_post_only. pcap -T fields -e frame. number -e http. request. method -e http. request. full_uri &gt; results. csvAlso i wrote small python script, that will count number of requests for each second-level domain. The first 20 results:       Domain   Number of requests         vk. com   6280       beeline. ru   5407       vk. me   2817       instagram. com   867       google. com   544       apple. com   536       yandex. ru   473       symcb. com   471       msftncsi. com   441       msn. com   304       yandex. net   302       trendmicro. com   292       co. uk   270       badoocdn. com   258       yadro. ru   188       marketgid. com   184       adfox. ru   183       mycdn. me   165       interfax. ru   154       scorecardresearch. com   137   And yes, i’ve got couple of interesting sessions. For example, for site mamba. ru, session is transmitted by insecure HTTP protocol. So i copy the session, paste it in chrome using EditThisCookie plugin and voila, i am Sergey. I can read messages, look preferences and so on. Well, i just didn’t want to harm Sergey, so i didn’t make any POST requests. How to protect: The only way to protect your site is to use TLS (HTTPS). To apply it well many things must be checked, but such discussion worth to make another post. HTTPS traffic in wireshark: All data is encrypted, we can’t read it. Summary:  Use HTTPS were possible, especially if you deal with important user data (if it includes bank/card data, you must use https) Being in public wifi network and accessing site by http, keep in mind, that it is very easy to spy for you. It applies to private network also, attacker just need to know the wifi password. Useful links:  Dan Callahan: Quick Wins for Better Website Security - PyCon 2014 Hynek Schlawack: The Sorry State of SSL - PyCon 2014 Benjamin Peterson - A Dive into TLS - PyCon 2015 Ashwini Oruganti, Christopher Armstrong - Introduction to HTTPS: A Comedy of Errors - PyCon 2015 Getting comfortable with web security: A hands-on session - PyCon 2015"
    }, {
    "id": 9,
    "url": "https://st4lk.github.io/en/blog/2015/03/04/debug-sql-django-test.html",
    "title": "Debug SQL in django test",
    "body": "2015/03/04 -  In django tests we can measure number of sql queries: def test_home(self):  with self. assertNumQueries(1):    response = self. client. get('/')  self. assertEqual(response. status_code, 200)If code in context of assertNumQueries will make other number of DB attempts than expected (here 1), test will throw error. But when test fails it is sometimes hard to understand, what unexpected query was made. To debug such case very useful to log SQL expressions to console. Below is description how to do it. By the way, in Django 1. 7+ when test fails all SQL queries will be also printed. So we don’t need to do anything else! If you use older version of django, then this article will be helpful. Logging settings for SQL output in console: Django will log SQL attempt only if one of the following condition is met: settings. DEBUG = Trueor connection. use_debug_cursor = TrueBy default tests always have DEBUG = False regardless of your settings. DEBUG. Testing is good with real environment. So we are left with connection. use_debug_cursor, that is None or False by default (depending on version). But context manager assertNumQueries set this to True by itself for corresponding code block! We just need to set logging correctly. Create file settings_test. py. We’ll run tests with settings from this file, i recommend to do so. Project structure: project├── project│  ├── __init__. py│  ├── settings. py│  ├── settings_test. py│  ├── urls. py│  └── wsgi. py│├── spam # some app│  ├── __init__. py│  ├── views. py│  ├── tests. py│  └── models. py│└── manage. pyModule tests. py contains tests of spam app. It is not the best way to organise the app, better to create separate folder for tests, but for our simple example it is ok. settings_test. py: from settings import *try:  from settings import LOGGINGexcept ImportError:  LOGGING = dict(version=1, disable_existing_loggers=False,    handlers={}, loggers={})# use database in memory to not lose your data!DATABASES = {  'default': {    'ENGINE': 'django. db. backends. sqlite3',    'NAME': ':memory:',    'USER': '',    'PASSWORD': '',    'TEST_CHARSET': 'utf8',  }}LOGGING['handlers']['console'] = {  'level': 'DEBUG',  'class': 'logging. StreamHandler',}LOGGING['loggers']['django. db. backends'] = {  'handlers': ['console'],  'level': 'DEBUG',  'propagate': False,}LOGGING['loggers']['django. db. backends. schema'] = {  'propagate': False, # don't log schema queries, django &gt;= 1. 7}tests. py: from django. test import TestCasefrom spam. models import Fooclass SpamTestCase(TestCase):  def setUp(self):    Foo. objects. create(title= Foo )  def test_home(self):    with self. assertNumQueries(1):      response = self. client. get('/')    self. assertEqual(response. status_code, 200)Let’s see what we get using different test runners. Django 1. 4+: $ python manage. py test (no SQL): Creating test database for alias 'default'. . . . ----------------------------------------------------------------------Ran 1 test in 0. 009sOKDestroying test database for alias 'default'. . . $ python manage. py test --settings=project. settings_test (SQL in console) Creating test database for alias 'default'. . . (0. 000) SELECT  spam_foo .  id ,  spam_foo .  title  FROM  spam_foo  LIMIT 21; args=(). ----------------------------------------------------------------------Ran 1 test in 0. 009sOKDestroying test database for alias 'default'. . . Django 1. 4+ and django-nose:  pip install django-nose in settings. py set TEST_RUNNER = ‘django_nose. NoseTestSuiteRunner’$ python manage. py test (no SQL): . . . $ python manage. py test --settings=project. settings_test (SQL в консоле) (0. 000) SELECT  spam_foo .  id ,  spam_foo .  title  FROM  spam_foo  LIMIT 21; args=(). . . Django 1. 4+ and pytest-django:  pip install pytest-django create file pytest. ini next to manage. py and put following lines in it:[pytest]DJANGO_SETTINGS_MODULE = project. settings rename tests. py to test_spam. py (default name pattern in py. test)$ py. test (no SQL): . . . $ py. test --ds=project. settings_test (no SQL, py. test capture entire output) . . . $ py. test --ds=project. settings_test -s (SQL in console) (0. 000) SELECT  spam_foo .  id ,  spam_foo .  title  FROM  spam_foo  LIMIT 21; args=(). . . Summary: As we can see from previous examples, SQL from block assertNumQueries are logged to console when we use settings_test. If we’ll always run tests with such settings we soon become tired from all this SQL being printed. To avoid this just comment line 'handlers': ['console'],. And when you need to see SQL - uncomment it. Error output from assertNumQueries and Django 1. 7+: If assertNumQueries registers not expected number of database attempts then we get a traceback with an error: Traceback (most recent call last): . . . AssertionError: 1 queries executed, 2 expectedBut in django 1. 7+ along with this we get SQL queries being made: Captured queries were:QUERY = u'SELECT  spam_foo .  id ,  spam_foo .  title  FROM  spam_foo  LIMIT 21' - PARAMS = ()Logging settings have to effect to this, very useful! Show SQL outside of assertNumQueries: So far we were talking about assertNumQueries, but what if we need to check queries outside of this manager? It is needed to manually set connection. use_debug_cursor = True before tests. It can be done in test runner or using hook in py. test. Show all SQL: Django 1. 4+: Create file test_runner. py, put it next to settings. py and insert code: try:  from django. test. runner import DiscoverRunner as DjangoTestSuiteRunnerexcept ImportError:  # django &lt; 1. 6  from django. test. simple import DjangoTestSuiteRunnerfrom django. db import connections, DEFAULT_DB_ALIASclass SqlDebugTestSuiteRunner(DjangoTestSuiteRunner):  def setup_test_environment(self, **kwargs):    super(SqlDebugTestSuiteRunner, self). setup_test_environment(**kwargs)    connections[DEFAULT_DB_ALIAS]. use_debug_cursor = TrueNow either in settings (or settings_test if you use it) set runner: TEST_RUNNER = 'project. test_runner. SqlDebugTestSuiteRunner'Launch python manage. py test --settings=project. settings_test and see all SQL attempts made during test run. Either not set TEST_RUNNER and use –testrunner argument: python manage. py test --settings=project. settings_test --testrunner=project. test_runner. SqlDebugTestSuiteRunner Result will be the same. Show all SQL: Django 1. 4+ and django-nose: For nose mostly everything is the same, but runner should sublcass NoseTestSuiteRunner. test_runner. py: from django_nose import NoseTestSuiteRunnerfrom django. db import connections, DEFAULT_DB_ALIASclass SqlDebugTestSuiteRunner(NoseTestSuiteRunner):  def setup_test_environment(self, **kwargs):    super(SqlDebugTestSuiteRunner, self). setup_test_environment(**kwargs)    connections[DEFAULT_DB_ALIAS]. use_debug_cursor = TrueAt the time when i write this post version of django-nose is 1. 3. In this version argument –testrunner is not supported. I’ve send pull request, it could fall in release already. Show all SQL: Django 1. 4+ and pytest-django: In py. test we must go another way, as it doesn’t use standard django runner. Instead we create a hook to set test environment. Create file plugin_debug_sql. py, put it next to settings. py, paste code: def pytest_runtest_setup(item):  from django. db import connections, DEFAULT_DB_ALIAS  connections[DEFAULT_DB_ALIAS]. use_debug_cursor = TrueLaunch by such command: PYTHONPATH=`pwd`:$PYTHONPATH py. test -s --ds=sql. settings_test -p project. plugin_debug_sql I have to put current path explicitly in PYTHONPATH here, as py. test won’t do it automatically for some reason. "
    }, {
    "id": 10,
    "url": "https://st4lk.github.io/en/blog/2015/01/31/tornado-internationalization-and-localization.html",
    "title": "Tornado i18n and l10n",
    "body": "2015/01/31 - Star Let’s talk about i18n, i10n and tornado implementation. This post have a lot of text, but i wanted to describe many things that i faced during realization of i18n in tornado project. The step by step instruction is placed in the second part of this article. Common definitions: i18n: i18n - shorthand from internationalization. It mean the process of multiple languages support in application. It is not the translation itself, but technical part of the project, that allows to show text in different languages depending on user preferences. Usually done by developer. l10n: l10n - shorthand from localization. It mean the translation process. Usually done by translator. Language tags: Language tags define the language. Tag format contains many nuances, all of them are described in rfc5646.  But the most common used format is following: en-USFirst part define language and second - region. In current example tag en-US means english language, that is used in USA. And for example en-GB will mean english language, that is used in Great Britain (i suppose they differs a little bit). In language tag only one subtag is mandatory - language. Therefore this is correct tag: enMoreover, if region subtag is not necessary - don’t use it. CLDR: CLDR - Common Locale Data Repository.  Contains commonly used data in different langauges:  format of date, price in different currencies, timezones country names, week days, months rules of writing numbers, plural forms, write direction, first week day and etc. gettext: gettext - library that realise i18n process. In this article working with this lib is described. Torando allows also working with CSV files, but this approach have much less features, so i’ll not describe it here. Short description of gettext workflow:  All text strings in code must be written in english.    Mark translatable strings.   Put all strings as argument of special function, implemented in gettext. In python function with name _ is usually used. Was:    Hello world!     Now:   _( Hello world! )      Create . pot template file.   It can be done with xgettext util. It parse specified files, find calls of function _ and generate file messages. pot. This file will contain strings to be translated, along with some preference headers:   msgid  Hello world!  msgstr       This file shall not be modified by hand. Strictly speaking, we don’t need it for translations, it just a helper. For example, django i18n scripts don’t save it anywhere. But let us keep it, as this file will be useful to update our translations. Also, we can give this file for translator to show him work amount.     Create translation file in exact language.   Command msginit can be used for this, it will take template messages. pot as input and produce new file messages. po as output. If fact, we’ll get a copy of our template, but some parts will be set according to chosen language.     Translate strings in messages. po manually:   msgid  Hello world!  msgstr  Привет, мир!       Compile translation by command msgfmt. We’ll get messages. mo as output.     If new strings are added somewhere in our application, just update messages. po, don’t create it from scratch.   Thus previous translations will not be lost. To achieve it:      again create template . pot (point 3). Yes, it will replace previous messages. pot. But it’s ok as we never modify it manually.    use command msgmerge, it will synchronise . po and . pot files.    repeat points 5 and 6.    That’s it. If function _ in our project works properly, then englishman will see “Hello world!”, whereas russian - “Привет, мир!”. It remains unclear, how function _ is implemented and where we can get it. Also, how we discover user preferred language, who is he - englishman or russian? Where we can get those utilities xgettext, msginit, msgmerge, msgfmt? Let’s go step by step. Function _: This function can be written manually, as python has builtin support of gettext. However, tornado and django already implemented it. We need just to assign name _ to it.  Django example: from django. utils. translation import ugettext as __( Hello world! )And tornado: class SomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    _( Hello world! )How function _ discover user preferred language?: In most simple case web application knows language from HTTP header Accept-Language (preferred language is set in browser settings). Django and tornado have it.  For more complicated logic, when for example user can specify locale in his profile on web site, both frameworks have corresponding means for it.  Unlike django, tornado can not determine language from url prefix out of the box. I mean that django can show russian content for url /ru/. . . / and english content for /en/. . . /. But tornado haven’t this functionality, we must implement it manually. Where to get utils xgettext, msginit, msgmerge, msgfmt: Install :). Some Ubuntu versions already have them. If not, run this command: sudo apt-get install gettextOSX: brew install gettextbrew link gettext --forceWindows binaries can be downloaded from here (haven’t tried): http://gnuwin32. sourceforge. net/packages/gettext. htm If you are having trouble with install, then you can use builtin python scripts: pygettext. py, msgfmt. py. But they have much less features, than xgettext. For example, no msgmerge script that is extremely useful. One solution is also available - babel package. It implements many xgettext utils in python, including msgmerge. So we don’t need xgettext to be installed. I’ll describe this approach later in this article. Differences between tornado and django: Before starting the step by step instruction of implementing i18n in tornado, i want to pay attention on one feature.  It is extremely important for understanding how tornado is working. The main differences between tornado and django is that tornado runs in single process whereas django - not. How it corresponds to text translation? In django we can mark strings as translatable anywhere in code, even in models. For such purpose django has ‘lazy’ function, for example ugettext_lazy: from django. db import modelsfrom django. utils. translation import ugettext_lazy as _class SomeModel(models. Model):  title = models. CharField(_( Title ), max_length=50)Function ugettext_lazy doesn’t return string immediately, only at direct access, when locale will be discovered. But how it knows this locale? Obviously, client must make a request, so we can determine some information about visitor from this request and discover his localisation. After that django will store found language in thread global variable (using function activate()). Remember, that django process every request in separate process.  That is why function ugettext_lazy can be used anywhere in code, it will translate text correctly. It doesn’t need the request instance as all needed data will be taken from global variable. In tornado such approach is not working because it always run in single thread. It is a feature of asynchronous applications.  But what can happen, if we’ll try to implement this “lazy” pattern? Let’s try. Take a look at this simple tornado project. For “lazy” realisation we’ll use speaklater package: import os. pathfrom threading import localimport tornado. webfrom speaklater import make_lazy_gettext_active = local()def activate(current_locale):  _active. value = current_localedef gettext(s):  return _active. value. translate(s)_ = make_lazy_gettext(lambda: gettext)class HiModel(object):  hello = _( Hello, world! )class DoneModel(object):  done = _( Fuh, done )class HomeHandler(tornado. web. RequestHandler):  def prepare(self):    super(HomeHandler, self). prepare()    activate(self. locale) # set globally locale from request  @tornado. gen. coroutine  def get(self):    hello_model = HiModel()    self. write(unicode(hello_model. hello))    self. write( &lt;br/&gt; )    done_model = DoneModel()    self. write(unicode(done_model. done))    self. write( &lt;br/&gt; )    self. finish()application = tornado. web. Application([  tornado. web. url(r / , HomeHandler, name='home'),])if __name__ ==  __main__ :  application. listen(8888)  project_folder = os. path. dirname(os. path. abspath(__file__))  tornado. locale. load_gettext_translations(os. path. join(project_folder, 'locale'), 'messages')  tornado. ioloop. IOLoop. instance(). start()Here HiModel and DoneModel represent some models. The most important part - they have translatable strings.  Translation in messages. po look like this: msgid  Hello, world! msgstr  Привет, мир! msgid  Fuh, done msgstr  Фух, вроде готово Run this small server.  Assume, that “browser_en” have english preferred language and “browser_ru” - russian. Open url http://127. 0. 0. 1:8888/ in browser_ru and see: Привет, мир!Фух, вроде готовоSame action in browser_en: Hello, world!Fuh, doneAll is working fine. But let’s try to add async task. For simplicity we’ll use async timer: @tornado. gen. coroutinedef get(self):  hello_model = HiModel()  self. write(unicode(hello_model. hello))  self. write( &lt;br/&gt; )  io_loop = tornado. ioloop. IOLoop. instance()  yield tornado. gen. Task(io_loop. add_timeout, timedelta(seconds=5))  done_model = DoneModel()  self. write(unicode(done_model. done))  self. write( &lt;br/&gt; )  self. finish()So we just added these lines:   io_loop = tornado. ioloop. IOLoop. instance()  yield tornado. gen. Task(io_loop. add_timeout, timedelta(seconds=5))between models HiModel and DoneModel.  Now open url first from browser_ru, then from browser_en, but before timer in browser_ru will be done. In browser_ru we’ll see: Привет, мир!Fuh, doneand in browser_en: Hello, world!Fuh, doneI hope you already understand, why browser_ru show one string in russian and one in english. Just in case let’s describe this situation in details. When browser_ru made a request, we have set russian locale globally.  Then async task was fired.  While async task was working, browser_en made a request. Now we have set english locale globally.  Async task, that was fired by browser_ru, was finished and handler continued to work. But language was modified from another request, so from this moment strings will be translated to english, not to russian. Therefore we can make a decision, that in tornado we can correctly discover language only from request handler instance. i18n in tornado using xgettext : In tornado docs realisation of i18n is described quite poorly, so i’ll try to make a step by step instruction here. To my mind, the best way to describe is to show on example. Therefore let’s create simple tornado project. Project structure: └── project  ├── app. py  ├── home. html  └── requirements. txtapp. py: import tornado. ioloopimport tornado. webclass HomeHandler(tornado. web. RequestHandler):  def get(self):    self. render( home. html , text= Hello, world! )application = tornado. web. Application([  tornado. web. url(r / , HomeHandler, name='home'),])if __name__ ==  __main__ :  application. listen(8888)  tornado. ioloop. IOLoop. instance(). start()home. html: &lt;html&gt;  &lt;head&gt;&lt;title&gt;Home page&lt;/title&gt;&lt;/head&gt;  &lt;body&gt;   &lt;div&gt;Home page&lt;/div&gt;   &lt;div&gt;{{text}}&lt;/div&gt;  &lt;/body&gt; &lt;/html&gt;requirements. txt: tornado==4. 0. 2Goal: Add support of two languages, english and russian, in tornado project. If browser have english as preferred language - show english content and if russian - show russian content correspondingly. 1. All text in code must be in english: We already have it, currently there are such strings:  Hello, world!  # app. py Home page   # home. html2. Mark text: First of all we need to mark translatable strings (point 2 from gettext section).  As we remember, it is done by function _. In request handler code we can get it like this: _ = self. locale. translateHandler code (file app. py): class HomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    self. render( home. html , text=_( Hello, world! ))In templates this function is available by default, it is defined in method get_template_namespace().  Template code (file home. html): &lt;html&gt;  &lt;head&gt;&lt;title&gt;{{_( Home page )}}&lt;/title&gt;&lt;/head&gt;  &lt;body&gt;   &lt;div&gt;{{_( Home page )}}&lt;/div&gt;   &lt;div&gt;{{text}}&lt;/div&gt;  &lt;/body&gt; &lt;/html&gt;Tornado template system execute python code, that is placed inside double braces {{ . . . }}. 3. Create file with translations: Create locale folder in the root of our project: mkdir localeNext create file makemessages. sh in root and put such bash code there: #!/bin/bash # get arguments and init variablesif [  $#  -lt 1 ]; then  echo  Usage: $0 &lt;locale&gt; [optional: &lt;domain_name&gt;]   exit 1filocale=$1domain= messages if [ ! -z  $2  ]; then  domain=$2filocale_dir= locale/${locale}/LC_MESSAGES pot_file= locale/${domain}. pot po_file= ${locale_dir}/${domain}. po # create folders if not existsmkdir -p $locale_dir# create . pot filefind . -iname  *. html  -o -iname  *. py  | xargs \  xgettext --output=${pot_file} --language=Python --from-code=UTF-8 \  --sort-by-file --keyword=_ --keyword=_:1,2 --no-wrap# init . po file, if it doesn't exist yetif [ ! -f $po_file ]; then  msginit --input=${pot_file} --output-file=${po_file} --no-wrap --locale=${locale}else  # update . po file  msgmerge --no-wrap --sort-by-file --output-file=${po_file} ${po_file} ${pot_file}fiAdd execution rights: chmod a+x makemessages. shTo launch it is needed to specify language tag. This is the language that we want to implement (in this example - russian): . /makemessages. sh ruDuring first launch it can ask you email.  After execution files messages. pot and messages. po will be created: project├── locale│  ├── ru│  │  └── LC_MESSAGES│  │    └── messages. po│  └── messages. pot├── app. py├── home. html├── requirements. txt└── makemessages. shContents of messages. po: # Russian translations for tornado_i18n package. # Copyright (C) 2015 THE tornado_i18n'S COPYRIGHT HOLDER# This file is distributed under the same license as the tornado_i18n package. # stalk &lt;alexevseev@gmail. com&gt;, 2015. #msgid   msgstr    Project-Id-Version: tornado_i18n\n  Report-Msgid-Bugs-To: \n  POT-Creation-Date: 2015-01-30 12:27+0300\n  PO-Revision-Date: 2015-01-30 12:27+0300\n  Last-Translator: stalk &lt;alexevseev@gmail. com&gt;\n  Language-Team: Russian\n  Language: ru\n  MIME-Version: 1. 0\n  Content-Type: text/plain; charset=ASCII\n  Content-Transfer-Encoding: 8bit\n  Plural-Forms: nplurals=3; plural=(n%10==1 &amp;&amp; n%100!=11 ? 0 : n%10&gt;=2 &amp;&amp; n%10&lt;=4 &amp;&amp; (n%100&lt;10 || n%100&gt;=20) ? 1 : 2);\n #: app. py:8msgid  Hello, world! msgstr   #: home. html:2 home. html:4msgid  Home page msgstr   We must change charset=ASCII to charset=UTF-8 here, other headers are optional:  Content-Type: text/plain; charset=UTF-8\n 4. Do translation: Fill empty strings like msgstr “” in messages. po (not . pot!): #: app. py:8msgid  Hello, world! msgstr  Привет, мир! #: home. html:2 home. html:4msgid  Home page msgstr  Домашняя страница 5. Compile translation: Create file compilemessages. sh with bash code: #!/bin/bash # get arguments and init variablesif [  $#  -lt 1 ]; then  echo  Usage: $0 &lt;locale&gt; [optional: &lt;domain_name&gt;]   exit 1filocale=$1domain= messages if [ ! -z  $2  ]; then  domain=$2filocale_dir= locale/${locale}/LC_MESSAGES po_file= ${locale_dir}/${domain}. po mo_file= ${locale_dir}/${domain}. mo # create . mo file from . pomsgfmt ${po_file} --output-file=${mo_file}Add execution rights: chmod a+x compilemessages. shTo launch specify same language tag, as in makemessages. sh: . /compilemessages. sh ruWe’ve got file locale/ru/LC_MESSAGES/messages. mo: project├── locale│  ├── ru│  │  └── LC_MESSAGES│  │    ├── messages. po│  │    └── messages. mo│  └── messages. pot├── app. py├── home. html├── requirements. txt├── compilemessages. sh└── makemessages. sh6. Link translations with our project: To achieve it call function load_gettext_translations(). app. py: if __name__ ==  __main__ :  tornado. locale. load_gettext_translations('locale', 'messages')  application. listen(8888)  tornado. ioloop. IOLoop. instance(). start()Let’s try to start the project. python app. pyOpen address http://127. 0. 0. 1:8888/ in browser_en: Home pageHello, world!and brower_ru: Домашняя страницаПривет, мир!That’s it, translation is done! 7. Update translation: Imagine, that in home. html we have new text, that must be translated: &lt;div&gt;{{_( Good buy! )}}&lt;/div&gt;We just execute our script: . /makemessages. sh rufill translations in updated messages. po: #: home. html:6msgid  Good buy! msgstr  До свидания! compile: . /compilemessages. sh ruand restart server python app. pyThat’s all! 8. Plural forms: gettext can handle plural forms, here is how it works: ngettext( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)ngettext - standard function name for plural forms that is used in gettext.  But tornado function self. locale. translate also accept plural forms arguments. So we can use our usual name _ instead of ngettext: _( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)Pay attention on xgettext options in your script above: --keyword=_ --keyword=_:1,2With this options we tell the parser, that function _ can accept both single string and plural form strings as arguments. Plural forms function will return either single form, either plural form, depending on provided count.  Example in english: _( {count} event is gonna happen ,  {count} events are gonna happen , 1). format(count=1)# output 1 event is gonna happen _( {count} event is gonna happen ,  {count} events are gonna happen , 2). format(count=2)# output 2 events are gonna happen As we can see, english language have only 1 plural form. I. e. when count &gt; 1 the output will be the same. However russian language have 3 plural forms (and some other languages even more). Here is an example: 1,21,31 событие должно случиться2,3,4,22 события должно случиться5,6,7,8,9,20,25 событий должно случитьсяIf we’ll do everything correctly, gettext will produce the right form. Let’s implement it in our example.  Add following line in home. html: &lt;div&gt;{{_( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)}}&lt;/div&gt;in handler from app. py: class HomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    count = int(self. get_argument('count', 1))    self. render( home. html , text=_( Hello, world! ), count=count)Update translations: . /makemessages. sh ruNow we can find such strings in locale/ru/LC_MESSAGES/messages. po: #: home. html:6#, python-brace-formatmsgid  {count} event is gonna happen msgid_plural  {count} events are gonna happen msgstr[0]   msgstr[1]   msgstr[2]   gettext knows, that russian langauge has 3 plural forms.  Look at header, that was created by msginit:  Plural-Forms: nplurals=3; plural=(n%10==1 &amp;&amp; n%100!=11 ? 0 : n%10&gt;=2 &amp;&amp; n%10&lt;=4 &amp;&amp; (n%100&lt;10 || n%100&gt;=20) ? 1 : 2);\n Conditions from this header define correct plural form depending on provided count.  Do the translation: #: home. html:6#, python-brace-formatmsgid  {count} event is gonna happen msgid_plural  {count} events are gonna happen msgstr[0]  {count} событие должно случиться msgstr[1]  {count} события должно случиться msgstr[2]  {count} событий должно случиться Compile: . /compilemessages. sh ruStart the server and open in browser_ru http://127. 0. 0. 1:8888/: 1 событие должно случитьсяhttp://127. 0. 0. 1:8888/?count=2 2 события должно случитьсяhttp://127. 0. 0. 1:8888/?count=5 5 событий должно случитьсяGreat! 9. Custom logic for discovering language: Language was determined from browser settings so far. But what if we want to let the user to choose his preferred locale in our web project, for example in his profile.  This logic can be easily customised and browser settings will not be the mandatory setting.  Just override handler’s method get_user_locale(): class HomeHandler(tornado. web. RequestHandler):def get_user_locale(self):  return tornado. locale. get( ru )def get(self):  _ = self. locale. translate  self. render( home. html , text=_( Hello, world! ))Now regardless of browser settings the russian content will always be shown. babel package: Go further. babel provides access to CLDR data from python. Also it implement xgettext commands in pure python, thus we don’t need xgettext installed. But now let’s look at CLDR more carefully. Suppose we want to show some product price is US dollars. But in different countries different currency format is used.  For example in Russia price is often written as: 99,00 $ whereas in USA: $99. 00 All this information is available in CLDR! We can easily use it thanks to babel.  Install babel: pip install babelModify app. py: import tornado. ioloopimport tornado. webfrom babel. numbers import format_currencyclass HomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    count = int(self. get_argument('count', 1))    format_usd = lambda p: format_currency(p, currency= USD ,      locale=self. locale. code)    self. render( home. html , text=_( Hello, world! ), count=count,      format_usd=format_usd)add price in home. html: &lt;div&gt;{{format_usd(99)}}&lt;/div&gt;That’s all we need to do.  browser_ru will show: 99,00 $browser_en: $99. 00This is only one of many features available in babel. It have translations of week days, months, date formats and many other. And we shall use it, no need to translate everything by hand. i18n in tornado using babel : As I said before, babel implement main xgettext commands in python. So we don’t need xgettext installed. Also, babel allows to declare custom syntax for parsing. It is useful when we have templates with some specific syntax in it. Take again our simple example. In theory we just need to change our scripts makemessages. sh and compilemessages. sh. For the purity of the experiment delete all files inside locale folder. Project structure now looks like this: project├── locale├── app. py├── home. html├── requirements. txt├── makemessages. sh└── compilemessages. shInstall babel, if not already: pip install babelFor tornado templates parsing we’ll need package tornado-babel: pip install tornado-babelFirst of all create file locale/babel. cfg with following code: [python: **. py][tornado: **. html]By this code we tell babel, what files must be used for parsing (as you remember, for xgettext we used find . -iname  *. html  -o -iname  *. py ). Modify our makemessages. sh, so babel will be used instead of xgettext: #!/bin/bash # get arguments and init variablesif [  $#  -lt 1 ]; then  echo  Usage: $0 &lt;locale&gt; [optional: &lt;domain_name&gt;]   exit 1filocale=$1domain= messages if [ ! -z  $2  ]; then  domain=$2fibase_dir= locale locale_dir= ${base_dir}/${locale}/LC_MESSAGES pot_file= ${base_dir}/${domain}. pot po_file= ${locale_dir}/${domain}. po babel_config= ${base_dir}/babel. cfg # create pot templatepybabel extract . / --output=${pot_file} \  --charset=UTF-8 --no-wrap --sort-by-file \  --keyword=_ --mapping=${babel_config}# init . po file, if it doesn't exist yetif [ ! -f $po_file ]; then  pybabel init --input-file=${pot_file} --output-dir=${base_dir} \    --domain=${domain} --locale=${locale} --no-wrapelse  # update . po file  pybabel update --domain=${domain} --input-file=${pot_file} \  --output-dir=${base_dir} --locale=${locale} --no-wrapfiAnd compilemessages. sh: #!/bin/bash # get arguments and init variablesif [  $#  -lt 1 ]; then  echo  Usage: $0 &lt;locale&gt; [optional: &lt;domain_name&gt;]   exit 1filocale=$1domain= messages if [ ! -z  $2  ]; then  domain=$2filocale_dir= locale/${locale}/LC_MESSAGES po_file= ${locale_dir}/${domain}. po mo_file= ${locale_dir}/${domain}. mo # create . mo file from . popybabel compile --locale=${locale} --domain=${domain} --directory=locale/Maybe you have noticed, that we use only --keyword=_ option, without --keyword=_:1,2. Why? The thing is that in babel==1. 3, that is currently avaliable from pypi, multiple arguments for same function are not supported.  What impact it have in our case? We have to use ngettext function name for plural forms instead of _.  Modify code in app. py a little: class HomeHandler(tornado. web. RequestHandler):  def get(self):    ngettext = _ = self. locale. translate    # . . .     self. render( home. html , text=_( Hello, world! ), count=count,      ngettext=ngettext)and template: &lt;div&gt;{{ngettext( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)}}&lt;/div&gt;Other code remains the same. Create translation files: . /makemessages. sh ruAgain do translation in locale/ru/LC_MESSAGES/messages. po. Compile: . /compilemessages. sh ruDuring execution of this script we can see something like catalog 'locale/ru/LC_MESSAGES/messages. po' is marked as fuzzy, skippingIn that case delete special untrusted translation marks in messages. po: #, fuzzyand compile again. Now all works similarly. But without xgettext! Fix babel: Agree, that uncomfortable to have two functions _ and ngettext. Let’s fix it! :) I’ve send pull-requests into babel repo and in tornado-babel repo. Maybe they are already accepted.  However i’ve created fixed versions to not wait for acceptance. Uninstall current babel and tornado-babel packages: pip uninstall babelpip uninstall tornado-babelInstall fixed versions: pip install https://github. com/st4lk/babel/archive/2. 1. 2-draft. tar. gzpip install https://github. com/st4lk/tornado-babel/archive/0. 3b. tar. gzAdd --keyword=_1,2 option in makemessages. sh. Was: --keyword=_ --mapping=${babel_config}Now: --keyword=_ --keyword=_:1,2 --mapping=${babel_config}Remove unnecessary ngettext function. app. py: class HomeHandler(tornado. web. RequestHandler):  def get(self):    _ = self. locale. translate    # . . .     self. render( home. html , text=_( Hello, world! ), count=count)home. html: &lt;div&gt;{{_( {count} event is gonna happen ,  {count} events are gonna happen , count). format(count=count)}}&lt;/div&gt;Great, now we use babel and code is the same, as with xgettext! The code of example is available on github: https://github. com/st4lk/tornado_i18n_example "
    }, {
    "id": 11,
    "url": "https://st4lk.github.io/en/blog/2015/01/18/timestamp-objectid-mongodb.html",
    "title": "Timestamp and ObjectId in mongoDB",
    "body": "2015/01/18 -  Every record in mongoDB has field _id, that must be unique inside collection. By default type of this field is ObjectId,and it is assigned automatically if field is not set. Lets look at ObjectId more carefully. It is 12 bytes that includes:  a 4-byte value representing the seconds since the Unix epoch, a 3-byte machine identifier, a 2-byte process id, and a 3-byte counter, starting with a random value. As we see, first 4 bytes represent creation date and we can use it:  sort by field _id and you’ll get documents in creation time order we can get creation time of the document from ObjectIdBut keep in mind, that this date has accuracy of seconds. If two documents are createdduring one second then their order is not defined in sorting by _id. So, if accuracy of second is enough for us then we do NOT need field like this: {  created_at: ISODate( 2015-01-18T12:07:47. 036Z )  // other fields}as creation date is included in _id. Get date from ObjectId: In mongoDB shell date can be retrieved by method getTimestamp(): &gt; db. users. findOne(). _id. getTimestamp()ISODate( 2015-01-18T09:07:47Z )And in python code - by attribute generation_time &gt;&gt;&gt; from pymongo import MongoClient&gt;&gt;&gt; db = MongoClient(). db_name&gt;&gt;&gt; user = db. users. find_one()&gt;&gt;&gt; user['_id']. generation_timedatetime. datetime(2015, 1, 18, 9, 7, 47, tzinfo=&lt;bson. tz_util. FixedOffset object at 0x10e758d50&gt;)Date is returned in UTC and in python it is aware datetime with timezone. To be clear, these versions i use:  mongoDB v2. 6. 6 pymongo v2. 7. 2P. S. thanks @eugeneglybin to pointing out. "
    }, {
    "id": 12,
    "url": "https://st4lk.github.io/en/blog/2014/12/15/set-url-for-tornado-handlers.html",
    "title": "Set url for Tornado handlers",
    "body": "2014/12/15 -  To set url for tornado handlers we can pass list of tuples (url regex, handler) into application initialisation: application = tornado. web. Application([  (r / , MainHandler),  (r /some/path/page/(?P&lt;pk&gt;[0-9]+)$ , PageHandler),])But it is more convenient to use wrapper tornado. web. url, that allows to assign meaningful names for paths (similar to django url). Nevertheless, in a couple of production projects that i had to work with, this wrapper wasn’t used. Also in some tornado examples from documentation (one, two, three) simple tuples are used, that can be confusing. So i think that it is worth to mention the advantages, that gives url wrapper. So, what disadvantages we’ll face in case of tuples, i. e. without using a url. Without url(): To represent needed path in code or in template, we have to manually enter the string. Example. **app. py**:import tornado. ioloopimport tornado. webclass MainHandler(tornado. web. RequestHandler):  def get(self):    self. render( home. html , title= My title , pages=[1, 2, 3])class PageHandler(tornado. web. RequestHandler):  def get(self, page_n):    email_text =  Please visit this page: '/some/path/page/{page_n}/' . format(      page_n=1)    send_email('some@person. com', email_text)    self. render( page. html , title= Page , page_n=page_n)application = tornado. web. Application([  (r / , MainHandler),  (r /some/path/page/(?P&lt;page_n&gt;[0-9]+)/$ , PageHandler),])if __name__ ==  __main__ :  application. listen(8888)  tornado. ioloop. IOLoop. instance(). start()home. htm: &lt;html&gt;  &lt;head&gt;   &lt;title&gt;{{ title }}&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;   &lt;div&gt;View pages:&lt;/div&gt;   &lt;ul&gt;    {% for page_n in pages %}     &lt;li&gt;&lt;a href= /some/path/page/{{ page_n }}/ &gt;{{ page_n }}&lt;/a&gt;&lt;/li&gt;    {% end %}   &lt;/ul&gt;  &lt;/body&gt; &lt;/html&gt;page. html: &lt;html&gt;  &lt;head&gt;   &lt;title&gt;{{ title }}&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;   &lt;div&gt;You are viewing page #{{ page_n }}&lt;/div&gt;   &lt;div&gt;Back to &lt;a href= / &gt;Home&lt;a&gt;&lt;/div&gt;  &lt;/body&gt; &lt;/html&gt;As we can see, even in this simple code we repeat path /some/path/page/ three times. What if we’ll need to change this string a little? We’ll have to make an autocorrect, that is uncomfortably and can lead to errors. Furthermore, some paths can be cumbersome and decrease code readability. With url(): Same example, but with url: app. py: import tornado. ioloopimport tornado. webfrom tornado. web import urlclass MainHandler(tornado. web. RequestHandler):  def get(self):    self. render( home. html , title= My title , pages=[1, 2, 3])class PageHandler(tornado. web. RequestHandler):  def get(self, page_n):    email_text =  Please visit this page: '{url}' . format(      url=self. reverse_url('page', 1))    send_email('some@person. com', email_text)    self. render( page. html , title= Page , page_n=page_n)application = tornado. web. Application([  url(r / , MainHandler, name= home ),  url(r /some/path/page/(?P&lt;page_n&gt;[0-9]+)/$ , PageHandler, name= page ),])if __name__ ==  __main__ :  application. listen(8888)  tornado. ioloop. IOLoop. instance(). start()home. html: &lt;html&gt;  &lt;head&gt;   &lt;title&gt;{{ title }}&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;   &lt;div&gt;View pages:&lt;/div&gt;   &lt;ul&gt;    {% for page_n in pages %}     &lt;li&gt;&lt;a href= {{reverse_url('page', page_n)}} &gt;{{ page_n }}&lt;/a&gt;&lt;/li&gt;    {% end %}   &lt;/ul&gt;  &lt;/body&gt; &lt;/html&gt;page. html: &lt;html&gt;  &lt;head&gt;   &lt;title&gt;{{ title }}&lt;/title&gt;  &lt;/head&gt;  &lt;body&gt;   &lt;div&gt;You are viewing page #{{ page_n }}&lt;/div&gt;   &lt;div&gt;Back to &lt;a href= {{reverse_url('home')}} &gt;Home&lt;a&gt;&lt;/div&gt;  &lt;/body&gt; &lt;/html&gt;Paths now have meaningful names that are used in url representation by method reverse_url. If it is needed to change some path, we’ll do it in one single place. Much more convenient! "
    }, {
    "id": 13,
    "url": "https://st4lk.github.io/en/blog/2014/11/02/django-celery-setup.html",
    "title": "Django celery setup",
    "body": "2014/11/02 -  To enable celery in new django project i often look in previous ones to refresh in my memory some steps: what settings should be specified, how to launch, how to stop and so on. Here i want to combine all together in one place. What we must get as a result:  Add delayed task executing in django project, controlled by celery, to not load current django process. Examples of such tasks: email sending, working with third party api, heavy processing and etc.  Use redis as a broker.  In admin we can see launched and finished tasks.  In admin we can see status of all workers (online/offline). Celery setup: Install redis: To be able to exchange messages between processes we need a broker, that will hold those messages. I choose redis, as it is common solution, it fast, easy to install, low memory usage, reliable. List of all possible brokers can be found here. Just in case let’s check our server (all examples for ubuntu): sudo apt-get updatesudo apt-get install build-essentialsudo apt-get install tcl8. 5Download last version. When i wrote this post, it was 2. 8. 17. wget http://download. redis. io/releases/redis-2. 8. 17. tar. gztar xzf redis-2. 8. 17. tar. gzcd redis-2. 8. 17makemake testsudo make installcd utilssudo . /install_server. shStart redis server sudo service redis_6379 startWe can stop it by command sudo service redis_6379 stopStart redis at system boot up sudo update-rc. d redis_6379 defaultsAlso we’ll need a redis driver for python, install it pip install redisInstall django-celery: In fact, it is not necessary to install some special django app to add celery functionality, as it written in docs. But for usefull integration with django admin it is easier to install django-celery (look why). pip install django-celeryAdd following configuration to settings. py: INSTALLED_APPS += ( djcelery , )# redis server addressBROKER_URL = 'redis://localhost:6379/0'# store task results in redisCELERY_RESULT_BACKEND = 'redis://localhost:6379/0'# task result life time until they will be deletedCELERY_TASK_RESULT_EXPIRES = 7*86400 # 7 days# needed for worker monitoringCELERY_SEND_EVENTS = True# where to store periodic tasks (needed for scheduler)CELERYBEAT_SCHEDULER =  djcelery. schedulers. DatabaseScheduler # add following lines to the end of settings. pyimport djcelerydjcelery. setup_loader()Task are defined in tasks. pyfile, that is placed in app folder: - proj/ - proj/__init__. py - proj/settings. py - proj/urls. py- users/ # some app - users/__init__. py - users/models. py - users/views. py - users/tasks. py # users tasks lives here- products/ - products/__init__. py - products/models. py - products/views. py - products/tasks. py # products tasks lives here- manage. pyCreate simplest task. users/tasks. py: # -*- coding: utf-8 -*-from celery. task import task@task(ignore_result=True, max_retries=1, default_retry_delay=10)def just_print():  print  Print from celery task Launch task: DebuggingTo check, that tasks are working, start  python manage. py runserver # django project python manage. py celery worker --concurrency=1 # celery worker: process, that will do the tasks python manage. py celery beat # celery beat: process, that will execute periodic tasksLast two commands can be combined into one (key -B):  python manage. py celery worker -B --concurrency=1Let’s try to launch our task just_print. There are two ways to start a task:  By scheduler, that will call the task every time period (10 seconds for example) or every particular time (like crontab do).  From code, in needed place and under needed conditions. Launch task by schedulerGo to admin page at address http://{host}/admin/djcelery/periodictask/ and press “Add periodic task”. Fill fields as it is shown in screenshot below and press save.  To indicate launch time instead of period, do the same, as in previous case, but fill Crontab (Interval must be blanked): NotePeriodic task can be created automatically at project start (when celery process is started), to not create them manually in admin (but they still will be shown in admin). To achieve it, we can define them in settings. py. Every 10 seconds: CELERYBEAT_SCHEDULE = {  'example-task': {    'task': 'apps. users. tasks. just_print',    'schedule': 10, # in seconds, or timedelta(seconds=10)  },}Every new minute (will start at 0 second of every minute): from celery. schedules import crontabCELERYBEAT_SCHEDULE = {  'example-task': {    'task': 'apps. users. tasks. just_print',    'schedule': crontab(),  },}Or for example every 7-th minute of every hour: from celery. schedules import crontabCELERYBEAT_SCHEDULE = {  'example-task': {    'task': 'apps. users. tasks. just_print',    'schedule': crontab(minute=7),  },}For details about periodic task creation in settings. py look celery documentation. Also, task can be decorated with @periodic_task instead of @task. And this task will be periodic. Period is defined by run_every argument. Value is the same, as in ‘schedule’ key in CELERYBEAT_SCHEDULE: from celery. task import periodic_task@periodic_task(ignore_result=True, run_every=10) # 10 seconds, or timedelta(seconds=10)def just_print():  print  Print from celery task or crontab from celery. task import periodic_taskfrom celery. schedules import crontab@periodic_task(ignore_result=True, run_every=crontab()) # every minutedef just_print():  print  Print from celery task Start task from codeTo start task from code call method . delay(). For example, from view: from . tasks import just_printclass UserListView(ListView):  model = User  def get_context_data(self, **kwargs):    just_print. delay()    return super(UserListView, self). get_context_data(**kwargs)Monitoring: In admin section Djcelery we can see rows Tasks and Workers.  But currently they are blanked inside. To see information there about workers status and tasks history we need to start celerycam: python manage. py celerycam --frequency=10. 0Now we can see, that 1 worker is online: And tasks status: By default, celerycam delete statuses of old tasks by following rules:  collector is started every hour (look clerey 3. 1 code, can’t find how to change it from settings. py) collector delete all task, that are exceed allowed life time. Life time of task status can be defined in settings. py (default values are shown): from datetime import timedeltaCELERYCAM_EXPIRE_SUCCESS = timedelta(days=1)CELERYCAM_EXPIRE_ERROR = timedelta(days=3)CELERYCAM_EXPIRE_PENDING = timedelta(days=5)Launch in production: In production all celery processes must be started as daemons. We can run a bash script to start/stop them or define a config for supervisor. Bash scriptscelery_start. sh: #!/bin/bashPYTHON=/path/to/bin/pythonPROJECT_FOLDER=/project_dir/project/PID_FOLDER=/path/to/pid/LOGS_FOLDER=/path/to/logs/BEAT_SHEDULE_FILE=/path/to/shedule/celerybeat-schedule # celery beat need to store the last run times of the tasks in a local database file$PYTHON ${PROJECT_FOLDER}manage. py celery worker --concurrency=1 --detach --pidfile=${PID_FOLDER}celery_worker. pid --logfile=${LOGS_FOLDER}celery_worker. log$PYTHON ${PROJECT_FOLDER}manage. py celery beat --detach --pidfile=${PID_FOLDER}celery_beat. pid --logfile=${LOGS_FOLDER}celery_beat. log -s ${BEAT_SHEDULE_FILE}$PYTHON ${PROJECT_FOLDER}manage. py celerycam --frequency=10. 0 --detach --pidfile=${PID_FOLDER}celerycam. pid --logfile=${LOGS_FOLDER}celerycam. logcelery_stop. sh: #!/bin/bashPYTHON=/path/to/bin/pythonPID_FOLDER=/path/to/pid/$PYTHON -m celery multi stopwait worker1 --pidfile=${PID_FOLDER}celerycam. pid$PYTHON -m celery multi stopwait worker1 --pidfile=${PID_FOLDER}celery_beat. pid$PYTHON -m celery multi stopwait worker1 --pidfile=${PID_FOLDER}celery_worker. pidsupervisord: Best practice is to launch celery processes under the control of supervisord. Create configuration for every celery process in django project, for example, in deploy folder: supervisor. celeryd. conf [program:djangoproject. celeryd]command=/path/to/bin/python /path/to/django_project/manage. py celeryd --concurrency=1user=www-datanumprocs=1directory=/path/to/django_projectstdout_logfile=/path/to/log/celery_worker. logstderr_logfile=/path/to/log/celery_worker. logautostart=trueautorestart=truestartsecs=10stopwaitsecs = 120priority=998supervisor. celerybeat. conf [program:djangoproject. celerybeat]command=/path/to/bin/python /path/to/django_project/manage. py celery beat -s /path/to/celerybeat-scheduleuser=www-datanumprocs=1directory=/path/to/django_projectstdout_logfile=/path/to/log/celery_beat. logstderr_logfile=/path/to/log/celery_beat. logautostart=trueautorestart=truestartsecs=10stopwaitsecs = 120priority=998supervisor. celerycam. conf [program:djangoproject. celerycam]command=/path/to/bin/python /path/to/django_project/manage. py celerycam --frequency=10. 0user=www-datanumprocs=1directory=/path/to/django_projectstdout_logfile=/path/to/log/celerycam. logstderr_logfile=/path/to/log/celerycam. logautostart=trueautorestart=truestartsecs=10stopwaitsecs = 120priority=998Change all /path/to/ to correct paths and also define needed user. Now create symlink to your celery configs in /etc/supervisor/conf. d/ folder, to let supervisor know about them: cd /etc/supervisor/conf. dsudo ln -s /path/to/django_project/deploy/supervisor. celeryd. confsudo ln -s /path/to/django_project/deploy/supervisor. celerybeat. confsudo ln -s /path/to/django_project/deploy/supervisor. celerycam. confRestart supervisor: sudo supervisorctl reloadCheck, that all needed processes are started: ps aux | grep pythonOther: In periodic tasks section of admin (http://{host}/admin/djcelery/periodictask/) we can see a celery. backend_cleanup task: This task is cleaning all old task results (not statuses), that are stored in database. Old means that they were created more time ago, than defined in settings. CELERY_TASK_RESULT_EXPIRES. But, as we are storing task results in redis, we don’t need this cleanup task. So, we can just delete it from admin page. Redis will drop all old entries by itself. "
    }, {
    "id": 14,
    "url": "https://st4lk.github.io/en/blog/2014/09/09/nested-sql-queries-django.html",
    "title": "Nested SQL queries in Django",
    "body": "2014/09/09 -  Did you know, that Django ORM can do nested SQL queries? Shame on me, but i’ve found it not so long ago. So, lets say we have such models of Nursery and Pet: class Nursery(models. Model):  title = models. CharField(max_length=50)class Pet(models. Model):  name = models. CharField(max_length=50)  nursery = models. ForeignKey(Nursery, related_name='pets')We need to get all pets, that are placed in given nurseries. For example, in nurseries with title starts with “Moscow”: nurseries = Nursery. objects. filter(title__startswith= Moscow )Pet. objects. filter(nursery__in=nurseries)That code will do only one SQL query, that will have nested subquery: SELECT  users_pet .  id ,  users_pet .  name ,  users_pet .  nursery_id  FROM  users_pet  WHERE  users_pet .  nursery_id  IN (SELECT  users_nursery .  id  FROM  users_nursery  WHERE  users_nursery .  title  LIKE Moscow%)But, keep in mind, that in spite of only one query is executed, it is not always means better performance. It depends on exact database, that is used. For example, as it written in django documentation, in case of MySQL two queries will be more effective than one with a nested subquery. Because MySQL don’t optimize nested queries very well. So, according to django docs, such code will bring us better performance: nurseries = Nursery. objects. filter(title__startswith= Moscow ). values_list('pk', flat=True)Pet. objects. filter(nursery__in=list(nurseries))in spite of two queries being executed than one: SELECT  users_nursery .  id  FROM  users_nursery  WHERE  users_nursery .  title  LIKE Moscow%SELECT  users_pet .  id ,  users_pet .  name ,  users_pet .  nursery_id  FROM  users_pet  WHERE  users_pet .  nursery_id  IN (1, 2)"
    }, {
    "id": 15,
    "url": "https://st4lk.github.io/en/blog/2014/05/29/new-relic-free-shirt.html",
    "title": "Free t-shirt from New Relic",
    "body": "2014/05/29 -  New Relic - service for site monitoring. It shows statistic of your application, where it spend most of time, how often it access the database and many other information. For description of this cool service i need to write a separate post. Here i want to tell about another thing - how i’ve got a free t-shirt from New Relic. Actually, it is needed just to register and setup New Relic on a working web site. As it said in their offer. Of course, when i setup newrelic, i don’t have a target to get a t-shirt. I learned about it accidentally. For fun i entered my address, but expecting that nobody will deliver something from USA to Russia for free. Сuriously enough, i don’t receive any warnings. Moreover, after couple of days i get an email with confirmation that delivery started. It was pleasant, but anyway i was waiting to receive some cancellation. To my surprise, after about 20 days i found in mailbox such notice: At postoffice i’ve got a packet: And a t-shirt inside:  "
    }, {
    "id": 16,
    "url": "https://st4lk.github.io/en/blog/2014/05/09/remote-url-localhost-server.html",
    "title": "Remote url for localhost server",
    "body": "2014/05/09 -  There is a nice tool called ngrok. It allows to bind the URL for your localhost server! For example, you’ve launched a django development server on you computer: python manage. py runserverand this project can be accessed by remote URL. What for?: At least i can imagine such tasks:  demonstrate project to customer check your site integration with payment system. For example paypal, where for receiving IPN messages you need a working site URL, even for sandboxHow:  Download ngrok from here Unpack downloaded archive Start django development server (by default on 8000 port) Start ngrok:  . /ngrok 8000    In console you’ll see something like this:  ngrokTunnel Status         onlineVersion            1. 6/1. 6Forwarding          http://51c85c8a. ngrok. com -&gt; 127. 0. 0. 1:8000Forwarding          https://51c85c8a. ngrok. com -&gt; 127. 0. 0. 1:8000Web Interface         127. 0. 0. 1:4040# Conn            0Avg Conn Time         0. 00ms   Now your server is bind to http://51c85c8a. ngrok. com Improvements: It is not very useful, that on every start ngrok will generate new url like ********. ngrok. com. But we can assign custom subdomain and project can be accessed by the same url. To do it we need:  register take auth token   let ngrok know your auth token (it is needed to do only once):   . /ngrok -authtoken your_auth_token 8000      set your custom subdomain like this:   . /ngrok -subdomain=mysupersite 8000   After that our local server will be shown at http://mysupersite. ngrok. com. "
    }, {
    "id": 17,
    "url": "https://st4lk.github.io/en/blog/2014/03/14/send-email-django-project-mandrill-service.html",
    "title": "Send email in django project with mandrill service",
    "body": "2014/03/14 -  To send email messages from server we can just use SMTP protocol. But there is another way - special email services. I’ll describe one of them here, mandrill. com. Advantages:  Detailed statistic of sent emails. How many were sent, to whom, when. How many were opened, what links were clicked.  Message templates. They can be modified through mandrill service, no need to create anything in django admin. It is possible to use variables in templates, set subject, even address of sender. In django application you just specify the template name and the context (variables).  No need of your own mail service. And no need to enable you domain in google/yandex mail (but still it will be useful).  Free plan, that allows to send 12000 messages per month. Disadvantages:  For click statistic all links in you email will be replaced by special redirect urls. Probably regular users will not mention that.  In some mail clients (for example gmail in browser) the field ‘sender’ along with specified address will contain the real address of mandrill. But it is possible to set DKIM and SPF DNS records for your domain, so sender address will be correctly shown everywhere.  If you want to send more than 12000 messages per month, you need to buy corresponding plan. To my mind, advantages are greater than disadvantages. Integration:    Register on https://mandrill. com/signup/     Create API_KEY:    Create message template: 3. 1. Set template name (for example template-1):  Edit the template contents: Statistic example:  Time send graph:  Message open and link clicks graph:  List of all sent messages:  Link statistic: Django integration: Gist contains example of integration with django and also standalone usage in python script. "
    }, {
    "id": 18,
    "url": "https://st4lk.github.io/en/blog/2014/03/07/mongodb-indexes.html",
    "title": "What you should know about mongodb indexes",
    "body": "2014/03/07 -  Recently i’ve completed course “M101P: MongoDB for Developers” (periodically repeats, next starts at April). During this course i’ve found to myself interesting features of mongodb. 1. Index selection. : Suppose we have collection with such document format: {  _id  : . . . ,  a  : 81810,  b  : 97482,  c  : 44288 }{  _id  : . . . ,  a  : 11734,  b  : 27893,  c  : 19485 }// and so on. Total 99999 documents. Collection has indexes: db. foo. ensureIndex({a: 1, b: 1, c: 1})db. foo. ensureIndex({c: -1})Question: what index will be used in query db. foo. find({'a':{'$lt':10000}, 'b':{'$gt': 5000}}). sort({'c':-1}) ? Intuitively this: {a: 1, b: 1, c: 1}, as it fully covers all needed fields. But, unfortunately, it is not. Firstly, index {a: 1, b: 1, c: 1} can’t be used simultaneously for find and sort, because find contains compare operators ($lt, $gt). Index can be used in such query db. foo. find({'a': 1, 'b': 2}). sort({'c':-1})But, as you see, we have another query. Well, ok. Probably index {a: 1, b: 1, c: 1} will be used only for find, and sort will be applied without index. Ah, look: db. foo. find({'a':{'$lt':10000}, 'b':{'$gt': 5000}}). sort({'c':-1}). explain(){   cursor  :  BtreeCursor c_-1 ,   n  : 9498,   nscanned  : 99999,   scanAndOrder  : false,  // other fields not interesting now}{a: 1, b: 1, c: 1} index not even been used, and {c: -1} index was used only for sorting, because it is a decision of mongodb query optimizer. That’s where manual index selection will be useful by $hint operator: db. foo. find({'a':{'$lt':10000}, 'b':{'$gt': 5000}}, {'a':1, 'c':1}). sort({'c':-1}). hint({a: 1, b: 1, c: 1}). explain(){   cursor  :  BtreeCursor a_1_b_1_c_1 ,   n  : 9498,   nscanned  : 9974,   scanAndOrder  : true,  // other fields not interesting now}Now index was used for find operation. Sort was performed without index. I think, that using index for filtering 9498 documents from 99999 and then sorting in memory much effective than apply full scan for 99999 documents and then sort 9498 elements using index. 2. Index direction: You’ve mention, that in previous example one of indexes has value “-1”: db. foo. ensureIndex({c: -1})It means “descending” index for key “c”. What the sense in that? db. foo. find(). sort({'c':-1}). explain(){   cursor  :  BtreeCursor c_-1 ,  // . . . }db. foo. find(). sort({'c':1}). explain(){   cursor  :  BtreeCursor c_-1 reverse ,  // . . . }Index was applied in both cases, but in second one it has a “reverse” order. Why it is needed to specify direction? It make sense when sorting on two or more fields: db. foo. ensureIndex({a:1, b:1, c:1})// index can't be useddb. foo. find(). sort({a:-1, b:1}). explain(){   cursor  :  BasicCursor ,   scanAndOrder  : true,  // . . . }// but here - candb. foo. find(). sort({a:1, b:1}). explain(){   cursor  :  BtreeCursor a_1_b_1_c_1 ,   scanAndOrder  : false,}// but for one field sorting any direction is good:db. foo. find(). sort({a:1}). explain(){   cursor  :  BtreeCursor a_1_b_1_c_1 ,   scanAndOrder  : false,}db. foo. find(). sort({a:-1}). explain(){   cursor  :  BtreeCursor a_1_b_1_c_1 reverse ,   scanAndOrder  : false,}So the rule is: when sorting on two or more fields index direction must be equal to all field directions being sorted by. 3. Indexes and aggregation: Aggregation is a very, very cool feature in mongodb. The ability to get . explain() data for aggregation queries is important. Such ability will be added in version 2. 6 (when i write this post version 2. 6 is not officially released). Here how we can do it in version 2. 6: db. foo. aggregate([  {$match: {a: {'$lt':10000}, b: {'$gt': 5000}}},  {$sort: {c: -1}},  {$group: {_id: null, a_total: {$sum:  $a }}}],{explain: true})But! It turns out, that we can get explain data even in current version 2. 4, but this feature is not documented!We can do it not directly, but with runCommand: db. foo. runCommand('aggregate', {pipeline:[  {$match: {a: {'$lt':10000}, b: {'$gt': 5000}}},  {$sort: {c: -1}},  {$group: {_id: null, a_total: {$sum:  $a }}}], explain: true})Output (clipped): {   serverPipeline  : [    {       query  : {         a  : {           $lt  : 10000        },         b  : {           $gt  : 5000        }      },       sort  : {         c  : -1      },      // . . .        cursor  : {         cursor  :  BtreeCursor c_-1 ,         n  : 9498,         nscanned  : 99999,         scanAndOrder  : false,        // . .       },      // . . .   ],   ok  : 1}Currently, $hint can’t be applied to aggregation: SERVER-7944. P. S. : By the way, every student, that complete the course “M101P: MongoDB for Developers”, receive such certificate: M101P. "
    }, {
    "id": 19,
    "url": "https://st4lk.github.io/en/blog/2014/01/30/async-bitcoin-rpc-client.html",
    "title": "Async Bitcoin RPC client",
    "body": "2014/01/30 - Star To work with Bitcoin RPC from python there is a library Python-BitcoinRPC. But recently i need to call API from tornado application. Mentioned lib works in synchronous, i. e. blocking mode. For tornado it will be much better to use asynchronous version. Tried to search for existing solution, but can’t find it. So i create my async fork, that use tornado’s AsyncHTTPClient: https://github. com/st4lk/python-bitcoinrpc-tornado. Example (print current number of blocks in Bitcoin network): from bitcoinrpc_async. authproxy import AsyncAuthServiceProxyfrom tornado import ioloop, genBITCOIN_RPC_URL =  http://user:password@127. 0. 0. 1:8332 @gen. coroutinedef show_block_count():  service = AsyncAuthServiceProxy(BITCOIN_RPC_URL)  result = yield service. getblockcount()  print resultio_loop = ioloop. IOLoop. instance()io_loop. add_callback(show_block_count)io_loop. start()"
    }, {
    "id": 20,
    "url": "https://st4lk.github.io/en/blog/2013/12/29/tornado-web-application-example.html",
    "title": "Tornado web application example",
    "body": "2013/12/29 - Star Tornado - async web framework for python. I’ll cover shortly pros and cons about tornado and introduce typical web project, that is built on top of it. By describing pros and cons i mean my own point of view in compare with django. Tornado pros: 1. Asynchronous. : In tornado core there is a infinite loop called “ioloop”, that listen for events. All that happens in one single thread. For example, somebody wants to fetch /home/ url. For that event HomeHandler is registered and ioloop will call it. The handler code is executing. What happens with ioloop? It is blocked now, until handler code is finished. If another user will fetch the url, he has to wait until previous request will be processed. What’s the point then? There is a mechanism of callbacks. For long task processing we ask: do that hard operation (db access, or external http fetch) and when you’ll finish, call this callback please to let me know. For know, go ahead. Ioloop continue to process other events. At some time, our long task will be finished and corresponding event will be raised and registered callback will be called. But keep in mind, that functions itself must be async. Operation like time. sleep(10) will block ioloop anyway, even if it is called as async function. So for async operations special functions and libraries are used. But anyway, what benefits gives async style with only one started thread? Why it is bad to create new process or thread for each new request? Because process and even thread is expensive in terms of computer resources. Imagine that for each request we’ll create new thread. Then, if 1000 users will access simultaneously, then we’ll need to create 1000 threads! I suppose regular server can’t afford it. Of course, we always can limit max number of threads. In this case new user have to wait until some thread will be freed. Lets imagine more interesting task - online chat. When somebody writes message, all members must see it. What solutions comes to mind for django? For example, every member will send ajax request every 5 seconds to get new messages. This will eat server resources very quickly in case of big amount of chat members. For each ajax request it is needed to open and close connection, spawn new thread. It cost a lot. Another variant will be keep-alive http requests, when we hold connection without closing. But it is a C10K problem. At some time it will be needed to spawn 10k thread (or even more). That’s where async solutions will help. It is possible to use WebSockets or http request with keep-alive. In async frameworks such requests will not eat all computer resources. 2. WebSocket: In some way it is a result of async. Look here for websocket description. 3. Less ORM and html-template dependency: For example django’s built-in ORM works only with SQL databases. If you want to connect to mongodb, you’ll probably can’t use third party applications, that are linked to django’s ORM. Also some built-in apps, for example admin. You’ll face same situation, if you want to use SQLAlchemy instead of django’s ORM. Many apps are using django’s template system. Want to use Jinja2? Well, then adopt all this apps for it! Tornado has less problems with that. But at the same time tornado doesn’t have such ready applications as django does. So you’ll have to write some tools manually anyway (admin, hello). Also, not all database drivers has async support. Currently i know async drivers for mongodb, postgresql. Not sure about mysql. Tornado cons: 1. Less popularity than django. : It means, that you’ll have to write many things by yourself, whereas django has third-party apps. For example, admin. 2. Code complexity. : Async code is a little bit harder to read and write. So it is harder to start working with tornado, that with django. Typical project example for tornado: Here https://github. com/st4lk/acl_webapp. It is an ACL application, i. e. application with access rights. Rights are based on models: every user have field permissions: permissions = {  'model_name_1': ['read',],  'model_name_2': ['read', 'write'],  'model_name_3': ['read', 'write', 'delete'],}In this case user has rights “read only” for model ‘model_name_1’, “read and write” for ‘model_name_2” and “read, write and delete” for model_name_3. Project follows django structure: it has applications, each of them solves some certain task. Here are app examples: accountsnewspagesand so on. Each application contains models, handlers, forms. Base handlers:  ListHandler DetailHandler CreateHandler DeleteHandler and so on, as in djangoAll settings are defined in settings. py. Used technologies:  mongodb (database) motor (async driver for DB) schematics (abstract models for DB) WTForms (forms) Jinja2 (html templates)P. S. It is possible, that i’ve made a mistake in tornado describing and understanding. I’ll be glad to read it in comments! "
    }, {
    "id": 21,
    "url": "https://st4lk.github.io/en/blog/2013/11/13/django-project-beginning-working-server.html",
    "title": "Django project: from beginning to working server",
    "body": "2013/11/13 - In this post i’ll describe my experience of deploying django project to VPS-hosting. Steps of deploying and server setup:  Creation of django project Deploy setup Server setup DeployCreation of django project: Assume, that project is called myproject. First, create it on local machine. We’ll need pip and virtualenv (if not installed yet). Create virtual environment: virtualenv venv-myprojectAcitvate it: # on *nix systemsource venv-myproject/bin/activateREM on windowsvenv\Scripts\activate. batNext it is need to create a django project skeleton. We can just install django and use command ‘django-admin. py startproject’. This will create a very basic and poor project. Many time will be needed to add common used things, settings. Very useful to have your own template of a project with common used settings. I like cookiecutter, it is easy to use. You can take this template https://github. com/pydanny/cookiecutter-djangopackag and modify it. Here is my: https://github. com/st4lk/cookiecutter-django. Install cookiecutter: pip install cookiecutterCreate django project from template (my template for example): cookiecutter https://github. com/st4lk/cookiecutter-django. gitAfter this you’ll see question in console (project name, etc. ). Initial project creation is done. Then we work with project locally. At some moment we are ready to deploy it. But currently our server has just OS installed and nothing else. Lets fix that (i’ll describe server with ubuntu). Deploy setup: I deside to talk about deploy now, because it will affect all other actions. Once i was working with project, that used makesite utility for deploy. I like it and use it with some of my projects. Links:  project at github: https://github. com/klen/makesite description from author (in russian): http://klen. github. io/deploy-setup-ru. htmlmakesite can install, update, unistall project on server. At project creation makesite create all nessesary environment automatically, based on specific templates. Environment includes: creation of virtualenv, nginx configuration, uwsgi, supervisor, database initialization. So all the project needs to be started. Project install command: makesite install &lt;project_name&gt;When we need to deploy a project, we push it to remote repository (that is hosted on github, bitbucket or any other server). Then, we access our server by ssh (were project is launched) and enter such command: makesite update &lt;project_name&gt;Thats all, the project is updated. makesite configuration:    Create makesite. ini. Lets call it “main”, as we’ll have another one.    [Main] # production mode mode=project # project domain, will be used in nginx config domain=example. com # postgres superuser (for database creation) pguser=postgres # postgres superuser password pgpassword=secret_password # database name dbname=project_name # postgres user, that will have access to database (same, as in django settings. py) dbuser=project_user # postgres user password dbpassword=project_user_password # postgres host name pghost=127. 0. 0. 1 # remote repository address src=git+git@bitbucket. org:username/example. git # user, the makesite will work from src_user=root      Create in root folder of project file makesite. ini, we’ll call it “additional”:   [Main] template=virtualenv,db-postgres,django,uwsgi django_settings=&lt;project_name. settings&gt;   where &lt;project_name. settings&gt; is path to settings module, relative to ‘manage. py’. It is needed to use . , not /. Default settings for uwsgi, nginx and supervisor will be taken from https://github. com/klen/makesite/tree/master/makesite/templates/uwsgi/deploy. If you need to customize them, add following lines to “additional” makesite. ini [Templates]uwsgi=%(source_dir)s/deploy/makesite_templates/uwsgiCreate path ‘deploy/makesite_templates’ in your project, copy uwsgi dir from https://github. com/klen/makesite/tree/master/makesite/templates. Then modify needed configs here ‘deploy/makesite_templates/uwsgi/deploy/’ In summary we hanve:  main makesite. ini (we’ll need it later on server) additional makesite. ini, that is placed in project root path deploy/makesite_templates/uwsgi if required. Don’t forget to put all this stuff (except main makesite. ini) under git and push the changes. . Server setup: All commands are checked for Ubuntu 12. 04. Access server with root account. Check python version: python -VI assert, that it is 2. 7 # We are root# Update the systemapt-get updateapt-get upgrade# Compilation utilsapt-get install build-essentialapt-get install libpq-dev# Config localelocale-gen ru_RU. UTF-8locale-gen en_US. UTF-8dpkg-reconfigure locales# git setupapt-get install git# setup-toolsapt-get install python-setuptools python-pip# Another compilation packagesapt-get install python2. 7-devapt-get install python-dev# postgres setupapt-get install postgresql# modify /etc/postgresql/{postgres_version}/main/pg_hba. conf# line#  local  all       postgres                peer # change to#  local  all       postgres                md5 sudo -u postgres psql template1#postgres superuser password\password postgres# !!! enter password and remember it# press ctrl+d to exit# postgres utilsapt-get install pgagentapt-get install pgadmin3#If you need to customize postgres settings, modify this file:# /etc/postgresql/{postgres_version}/main/postgresql. conf# Restart postgres/etc/init. d/postgresql restart# uwsgipip install uwsgi# nginx, virtualenv, supervisoreasy_install elementtreeapt-get install python-virtualenv nginx supervisor# working with imagesapt-get install libjpeg-devpip install -I Pillow# Create ssh key, that will be used by makesitecdtest -d . ssh || mkdir . sshcd ~/. sshssh-keygen# Enter file in which to save the key : makesite# Other question just hit enter: Generating public/private rsa key pair.  Enter file in which to save the key (/home/ubuntu/. ssh/id_rsa): makesite Enter passphrase (empty for no passphrase): Enter same passphrase again: Your identification has been saved in makesite.  Your public key has been saved in makesite. pub. # Copy to id_rsacp makesite id_rsacp makesite. pub id_rsa. pub# ! Important# Tak public key makesite. pub and add it to your remote repository. # Description from github: https://help. github. com/articles/generating-ssh-keys#step-3-add-your-ssh-key-to-github# Create path, where project will be placedmkdir /var/www# Put  main  makesite. ini into /var/www# makesite setuppip install makesite# Check bash settingsmakesite shell -p /var/www# Write them into ~/. bashrccat &gt;&gt; ~/. bashrc# copy and paste previous output# it was something like:# Makesite integration# ====================export MAKESITE_HOME=/var/wwwsource /usr/local/lib/python2. 7/dist-packages/makesite/shell. sh# enter# ctrl+d to exit# load settings for current sessionsource ~/. bashrc# it is usefulapt-get install mercurial# Now install project. It takes some time. makesite install &lt;project_name&gt;# If you see message# OPERATION SUCCESSFUL# at the end, then installation is done successfully. # Check, that dependencies are installed from requirements. txt:cd /var/www/{project_name}/mastersource . virtualenv/bin/activatepip freeze# Check, that database is create, tables are synced, migration are applied. # Check, that STATIC_ROOT and MEDIA_ROOT have right values. # corresponding folders must be placed here:# /var/www/{project_name}/master/# if some folders are missed, you can create in manuall# but change owner and group to www-data# update static# (virtualenv must be activated)cd /var/www/{project_name}/master/source python manage. py collectstatic --noinput# Restart nginx/etc/init. d/nginx restartThats all, project is launched. Deploy: As it written before, to update a project make:  push to remote repo access server by ssh and from server type makesite update &lt;project_name&gt;"
    }, {
    "id": 22,
    "url": "https://st4lk.github.io/en/blog/2013/09/26/django-logging-settings.html",
    "title": "Django logging settings",
    "body": "2013/09/26 -   Gist Let’s look at default django logging settings and try to make them more useful. Here what we have in settings. py after command django-admin. py startproject project_name (django 1. 5): # A sample logging configuration. The only tangible logging# performed by this configuration is to send an email to# the site admins on every HTTP 500 error when DEBUG=False. # See http://docs. djangoproject. com/en/dev/topics/logging for# more details on how to customize your logging configuration. LOGGING = {  'version': 1,  'disable_existing_loggers': False,  'filters': {    'require_debug_false': {      '()': 'django. utils. log. RequireDebugFalse'    }  },  'handlers': {    'mail_admins': {      'level': 'ERROR',      'filters': ['require_debug_false'],      'class': 'django. utils. log. AdminEmailHandler'    }  },  'loggers': {    'django. request': {      'handlers': ['mail_admins'],      'level': 'ERROR',      'propagate': True,    },  }}As it written in comments, logger ‘django. request’ will sent email message to admins on every HTTP 500 error (they are in fact uncaught exceptions) when DEBUG=False. Email list is set in settings. ADMINS. But, that’s not all. There are also default settings, they are set in django. utils. log. DEFAULT_LOGGING. Here how they look like for version 1. 5. 4 (actual version can be found on github): DEFAULT_LOGGING = {  'version': 1,  'disable_existing_loggers': False,  'filters': {    'require_debug_false': {      '()': 'django. utils. log. RequireDebugFalse',    },    'require_debug_true': {      '()': 'django. utils. log. RequireDebugTrue',    },  },  'handlers': {    'console':{      'level': 'INFO',      'filters': ['require_debug_true'],      'class': 'logging. StreamHandler',    },    'null': {      'class': 'django. utils. log. NullHandler',    },    'mail_admins': {      'level': 'ERROR',      'filters': ['require_debug_false'],      'class': 'django. utils. log. AdminEmailHandler'    }  },  'loggers': {    'django': {      'handlers': ['console'],    },    'django. request': {      'handlers': ['mail_admins'],      'level': 'ERROR',      'propagate': False,    },    'py. warnings': {      'handlers': ['console'],    },  }}What do they mean?: Logger ‘django’ writes all logs from child loggers to console with message level WARNING and higher (‘level’ for logger ‘django’ is not set, by default it is WARNING). Child loggers are: ‘django. db. backends’, ‘django. contrib. gis’ and so on. But except ‘django. request’ which has ‘propagate’: False. Logger ‘py. warnings’ also writes messages from module warnings into console. For example DeprecationWarnings. Example of useful (to my mind) config: For simplicity only one root logger can be defined, that will catch logs from all modules. If settings. DEBUG = True it will write messages to console and into special debug log-file. And if settings. DEBUG = False, messages will be written only in production log-file, with level INFO and higher only. When settings. DEBUG = True all SQL queries will be written in console and in debug log-file, that is very useful. If you wish, it is possible to define separate logger ‘django. db’ with ‘propagate’: False and set to it needed settings. As i define root logger, other loggers will have empty handlers list, to not duplicate messages. Source at gist. github Settings will work with django 1. 5+: For earlier django version it is probably needed co create file log. py somewhere: import loggingfrom django. conf import settingstry:  from logging import NullHandlerexcept ImportError:  class NullHandler(logging. Handler):    def emit(self, record):      passclass RequireDebugTrue(logging. Filter):  def filter(self, record):    return settings. DEBUGclass RequireDebugFalse(logging. Filter):  def filter(self, record):    return not settings. DEBUGAnd use path to this file for NullHandler, RequireDebugTrue, RequireDebugFalse instead of django. utils. log. . . . . Now you can do in any source file: import logginglogger = logging. getLogger(__name__)logger. debug( some message )logger. warning( oops, it is a warning )logger. error( bad, very bad )try:  # do somethingexcept ValueError:  logger. exception( I know it could happen )and all your logs will be processed, depending on DEBUG. "
    }, {
    "id": 23,
    "url": "https://st4lk.github.io/en/blog/2013/08/20/python-logging-every-day.html",
    "title": "Python logging for every day",
    "body": "2013/08/20 -   Gist When writing a small python program or script, it is sometimes needed to output debug message or maybe event. It is known, that python has logging module exactly for that purpose. But in my case usually such thing happens: it is lack of time and hands just writes print instead of logging, because i can’t remember all those complicated logging settings. But then, if script is launched often or i must provide it to customer, i have to replace all prints with logging, because it is better to log all messages to file. So, not to keep in my head all these settings, i am writing this post. Logging requirements will be the following::  All logs must be written to file and in console (duplicated) Log file is rotated (not exceed specified size) All third party libraries logs are also handled Works on python 2. 5+ 3. 0+ Log message contains: message, logger name, source file name, source file line, timestamp, message level (DEBUG/INFO/WARNING etc) It is possible to log unicode strings. How to use these settings: In snippet there are three types of settings: using logging classes, with fileConfig and with dictConfig. Choose whatever you like. The most simple is the first, it works on most python versions: 2. 5+, 3. 0+. Insert those settings in your script - and you logging is set up. Now all log messages are stored to file and are printed to console. In code logging options are set for root logger: It stands at the top of loggers hierarchy and consequently it handles all messages from every logger (for better understanding of logging module you can search posts in internet. For example, there is good tutorial in python documentation: http://docs. python. org/2/howto/logging. html#logging-basic-tutorial). Example: Lets write simple script: import logginglogging. info('started')logging. info('finished')If run it as is, messages will not be printed anywhere. Now add settigns from my snippet (using classes): ################################################### LOGGING CLASS SETTINGS (py25+, py30+) ####################################################### also will work with py23, py24 without 'encoding' argimport loggingimport logging. handlersf = logging. Formatter(fmt='%(levelname)s:%(name)s: %(message)s '  '(%(asctime)s; %(filename)s:%(lineno)d)',  datefmt= %Y-%m-%d %H:%M:%S )handlers = [  logging. handlers. RotatingFileHandler('rotated. log', encoding='utf8',    maxBytes=100000, backupCount=1),  logging. StreamHandler()]root_logger = logging. getLogger()root_logger. setLevel(logging. DEBUG)for h in handlers:  h. setFormatter(f)  h. setLevel(logging. DEBUG)  root_logger. addHandler(h)################################## END LOGGING SETTINGS ##################################logging. info('started')logging. info('finished')Run the script. In console (and in file rotated. log) we can see messages: INFO:root: started (2013-08-21 01:52:31; test. py:21)INFO:root: finished (2013-08-21 01:52:31; test. py:22)Check, that logs from used libs are also handled. I create dummy lib thirdpartylib with code: import logginglogger = logging. getLogger(__name__)def do_something():  logger. debug('something is done in thirdpartylib')Now call do_something from your script: import thirdpartyliblogging. info('started')thirdpartylib. do_something()logging. info('finished')Output will be the following: INFO:root: started (2013-08-21 01:57:27; test. py:135)DEBUG:thirdpartylib: something is done in thirdpartylib (2013-08-21 01:57:27; __init__. py:5)INFO:root: finished (2013-08-21 01:57:27; test. py:137)Snippet: "
    }, {
    "id": 24,
    "url": "https://st4lk.github.io/en/blog/2013/07/11/django-querysetcount-cache.html",
    "title": "Django queryset.count cache",
    "body": "2013/07/11 -  Once I mentioned, that my django application makes several similiar queries like SELECT COUNT(*) . . . . As it turns out (for me it was surprise), queryset. count() has not obvious cache. But let me start the story from the beginning (and sorry for my english :) ). As it is known, django’s queryset is lazy and have cache. Lets say we have such model: class Item(models. Model):  name = models. CharField(max_length=50)When some django query is declared there is no database attempt (thats why it is called “lazy”): items = Item. objects. all()The attempt is made when we actually access elements from queryset, for example in cycle: for item in items:   print item. nameDuring executing line for item in items: sql query was performed: SELECT  main_item .  id ,  main_item .  name  FROM  main_item ;At next access to elements from queryset, database attempt will not occur, because all elements were touched and consequently put into cache. So, this code will hit the DB only once: for item in items: # hit the database   print item. namefor item in items: # cache   print item. nameNevertheless there are some cases, when database can be touched again. To not put very much information in a single post, refer to documentation: https://docs. djangoproject. com/en/dev/topics/db/queries/#caching-and-querysets. Now directly about count: Keeping in mind, that queryset has cache, i thought, that . count() will also be cached. But it is not (not always precisely). If count() method was called before queryset is put in cache, then database will be touched every time count() is called: items = Item. objects. all() # not hit DBitems. count() # hit DBitems. count() # hit DBitems. count() # hit DBfor item in items: # hit DB and put into cache   print item. nameНowever, if source queryset is cached, then count will not touch the database: items = Item. objects. all() # no DB hitfor item in items: # hit DB and put to cache   print item. nameitems. count() # cacheitems. count() # cacheitems. count() # cacheAll this concerns django templates too. The code, that made identical queries like SELECT COUNT(*) . . . has several checks like: {% if items. count %}and also some count output: {{ items. count }}Before these lines the queryset elements were not being accessed. As a result, each mentioned line create database query. Again, if such cycle was before: {% for item in items %}  {{item. name}}{% endfor %}then {{ items. count }} doesn’t hit the DB. Approaches to exclude redundant database calls.    If we know, that we’ll need to access all elements from query set, it is ok to use len.   Python code:    len(items) # DB len(items) # cache for item in items: # cache   # . . .     and reversed, that is also acceptable:    for item in items: # DB   # . . .  len(items) # cache len(items) # cache    Django template:    {{ items|length }} # DB {{ items|length }} # cache {% if items|length %} # cache {% for item in items %} # cache    and reversed:    {% for item in items %} # DB {{ items|length }} # cache {{ items|length }} # cache {% if items|length %} # cache      If it is needed to know only count of elements, or source queryset for count is different to queryset, that will be used to access elements, then we shall use count(). But it is better, if it will be called only once.   If count is needed more than one time, then instead of this:    {{ items. count }} {{ items. count }}    we can create additional variable in template context (in view, that renders this template):    # views. py context['items_count'] = items. count() # template {{ items_count }} {{ items_count }}    or we can use {% with items. count as items_count %} and not declare any additional variable in context:    # template {% with items. count as items_count %}    {{ items_count }}    {{ items_count }} {% endwith %}   Of course, everywere in this post word “cache” means queryset internal cache, and not caching. "
    }, {
    "id": 25,
    "url": "https://st4lk.github.io/en/blog/2013/06/20/unicode-string-formatting.html",
    "title": "Unicode string formatting",
    "body": "2013/06/20 - Did you know, if one of values in string formatting expression with % operator is unicode, then result string will also be unicode? &gt;&gt;&gt;  Hello, %s  % u Alex u'Hello, Alex'&gt;&gt;&gt;  Hello, %s  % u Алексей u'Hello,  0410 043b 0435 043a 0441 0435 0439'I used to work with . format string method and its behavior is more attractive to me: type of source string is saved and if some parameter contains non-ascii symbols, UnicodeEncodeError exception is raised. &gt;&gt;&gt;  Hello, {0} . format(u Alex )'Hello, Alex'&gt;&gt;&gt;  Hello, {0} . format(u Алексей )Traceback (most recent call last): File  &lt;stdin&gt; , line 1, in &lt;module&gt;UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-6: ordinal not in range(128)&gt;&gt;&gt; u Hello, {0} . format(u Алексей )u'Hello,  0410 043b 0435 043a 0441 0435 0439'Is it a big deal, what string type is returned? Well, sometimes yes. For example when working with urlparse. parse_qs, type of string make sense. So it is better to keep in mind, that code like: &gt;&gt;&gt;  Hello, %s  % valuecan return a unicode string. Some links:  % documentation . format documentation"
    }, {
    "id": 26,
    "url": "https://st4lk.github.io/en/blog/2013/05/22/parse-url-which-chontains-unicode-query-using-urlp.html",
    "title": "Parse url which contains unicode query, using urlparse.parse_qs",
    "body": "2013/05/22 - Task: get dictionary of URL GET query. For example, we have following url: http://example. com/?key=value&amp;a=bit is needed to get a dict: {'key': ['value'], 'a': ['b']}Values are lists, because one key may have several values: In: http://example. com/?key=value&amp;a=b&amp;a=cOut: {'key': ['value'], 'a': ['b', 'c']}In python there is a function urlparse. parse_qs for that purpose: &gt;&gt;&gt; import urlparse&gt;&gt;&gt; query =  key=value&amp;a=b &gt;&gt;&gt; urlparse. parse_qs(query){'a': ['b'], 'key': ['value']}So, on input parse_qs receives query, without “http://exapmle. com/?”. To get only query we can use urlparse. urlparse: &gt;&gt;&gt; import urlparse&gt;&gt;&gt; url =  http://example. com/?key=value&amp;a=b &gt;&gt;&gt; query = urlparse. urlparse(url). query&gt;&gt;&gt; query'key=value&amp;a=b'&gt;&gt;&gt; params = urlparse. parse_qs(query)&gt;&gt;&gt; params{'a': ['b'], 'key': ['value']}Lets restore original url, using urllib. urlencode: &gt;&gt;&gt; import urllib&gt;&gt;&gt; urllib. urlencode(params, doseq=True)'a=b&amp;key=value'The order of parameters doesn’t matter, so it’s ok. URL with unicode parameter: According to RFC3986, URL can contain only limited set of characters consisting of digits, letters, and a few graphic symbols from US-ASCII set. And some of characters are reserved ( : ,  / ,  ? ,  # ,  [ ,  ] ,  @ ,  ! ,  $ ,  &amp; ,  ' ,  ( ,  ) ,  * ,  + ,  , ,  ; ,  = ). If it is needed to send nonprintable or reserved characters in URL (for example as query param value), they must be Percent-Encoded: %HH, where HH is hexadecimal digits. Suppose we need to send u”значение”. In python string u значение  contains unicode code points and we need to get bytes, to be able to percent-encode them. So first lets encode the unicode string using, for example, utf8 encoding: &gt;&gt;&gt; value = u'значение'&gt;&gt;&gt; value_utf8 = value. encode('utf8')&gt;&gt;&gt; value_utf8' d0 b7 d0 bd d0 b0 d1 87 d0 b5 d0 bd d0 b8 d0 b5'Now encode those bytes, using Percent-Encoding (%HH) to be able to include in url: &gt;&gt;&gt; value_url = urllib. quote(value_utf8)&gt;&gt;&gt; value_url'%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5'Full URL: &gt;&gt;&gt; url =  http://example. com/?key=%s&amp;a=b  % value_url&gt;&gt;&gt; url'http://example. com/?key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'Again, lets get the query dict: &gt;&gt;&gt; query = urlparse. urlparse(url). query&gt;&gt;&gt; query'key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'&gt;&gt;&gt; params = urlparse. parse_qs(query)&gt;&gt;&gt; params{'a': ['b'], 'key': [' d0 b7 d0 bd d0 b0 d1 87 d0 b5 d0 bd d0 b8 d0 b5']}As we can see, parse_qs decoded value from Percent-Encoding and returned bytes. Now we can get unicode, as we remember, that encoding was utf8: &gt;&gt;&gt; params['key'][0]. decode('utf8')u' 0437 043d 0430 0447 0435 043d 0438 0435'&gt;&gt;&gt; print params['key'][0]. decode('utf8')значениеOk. Restore original query from the dict: &gt;&gt;&gt; urllib. urlencode(params, doseq=True)'a=b&amp;key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5'We’ve got same parameters as it was in original url. Same steps with URL, that was returned from django’s request. get_full_path(). For some reason, request. get_full_path() returns not the str string, but unicode (tried on django 1. 4, 1. 5): &gt;&gt;&gt; request. get_full_path()u'/?key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'Repeat same steps with this URL: &gt;&gt;&gt; url = request. get_full_path()&gt;&gt;&gt; query = urlparse. urlparse(url). query&gt;&gt;&gt; queryu'key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'&gt;&gt;&gt; params = urlparse. parse_qs(query)&gt;&gt;&gt; params{u'a': [u'b'], u'key': [u' d0 b7 d0 bd d0 b0 d1 87 d0 b5 d0 bd d0 b8 d0 b5']}Interesting, that value for u’key’ is unicode string, that contains bytes! Of course, decoding of that string will fail: &gt;&gt;&gt; params['key'][0]. decode('utf8')Traceback (most recent call last): File  &lt;stdin&gt; , line 1, in &lt;module&gt; File  C:\Python27\lib\encodings tf_8. py , line 16, in decode  return codecs. utf_8_decode(input, errors, True)UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-15: ordinal not in range(128)Same error using urlencode: &gt;&gt;&gt; urllib. urlencode(params, doseq=True)Traceback (most recent call last): File  &lt;stdin&gt; , line 1, in &lt;module&gt; File  C:\Python27\lib rllib. py , line 1337, in urlencode  l. append(k + '=' + quote_plus(str(elt)))UnicodeEncodeError: 'ascii' codec can't encode characters in position 0-15: ordinal not in range(128)For me there are two surprises:  django returned url as unicode (what for? why not just str as url can contain only ascii characters) parse_qs returned unicode string, that contains bytes. Solution is simple, just always give str to parse_qs: &gt;&gt;&gt; url = request. get_full_path()&gt;&gt;&gt; url = url. encode('ascii')&gt;&gt;&gt; url'/?key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'Or, which is the same: &gt;&gt;&gt; url = request. get_full_path()&gt;&gt;&gt; url = str(url)&gt;&gt;&gt; url'/?key=%D0%B7%D0%BD%D0%B0%D1%87%D0%B5%D0%BD%D0%B8%D0%B5&amp;a=b'Links:  Question about this problem on stackoverflow Great presentation about python strings and encoding: http://nedbatchelder. com/text/unipain. html"
    }, {
    "id": 27,
    "url": "https://st4lk.github.io/en/blog/2013/04/12/django-admin-site-optimisation.html",
    "title": "Django admin site optimisation",
    "body": "2013/04/12 -  It is known, that less amount of database attempts leads to better performance. Usually admin page - part of site with low traffic, but anyway it is good, if no additional database calls happen there. It is more pleasant to use it, because page renders faster and also frees server resources. In this post i’ll try to reduce some database attempts in admin page, when model method __unicode__ contains related object field. I suppose, that examples describe the situation more clear. 1. unicode at admin change form page: Lets take a look at following example: there are SubwayStation, SubwayLine and City models. SubwayLine is ForeignKey for SubwayStation and City - ForeignKey for SubwayLine. models. py: from django. db import modelsclass City(models. Model):  name = models. CharField(max_length=50)  def __unicode__(self):    return self. nameclass SubwayLine(models. Model):  name = models. CharField(max_length=50)  city = models. ForeignKey(City, related_name='lines')  def __unicode__(self):    return u {0}, {1} . format(self. city, self. name)class SubwayStation(models. Model):  name = models. CharField(max_length=50)  line = models. ForeignKey(SubwayLine, related_name='stations')  def __unicode__(self):    return self. nameadmin. py: from django. contrib import adminfrom . models import SubwayLine, SubwayStation, Cityclass SubwayLineAdmin(admin. ModelAdmin):  list_display = 'name',class SubwayStationAdmin(admin. ModelAdmin):  list_display = 'name', 'line'class CityAdmin(admin. ModelAdmin):  list_display = 'name',admin. site. register(SubwayLine, SubwayLineAdmin)admin. site. register(SubwayStation, SubwayStationAdmin)admin. site. register(City, CityAdmin)Now visit SubwayStation change page As we can see, __unicode__ for SubwayLine consist of two fields: related City. name and its own SubwayLine. name. Check database attempts were performed to show this page (attempts to tables django_session, auth_user, django_content_type are not taken into account) : SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id  FROM  main_subwaystation  WHERE  main_subwaystation .  id  = 1 ;SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwayline ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 2 ;Following requests are made:  first - get station record second - get all lines others - get city for each lineSo we have (2 + amount of lines) attempts. It is a lot. There are two ways to fix the situation:    Use select_related in queryset   It is needed to define form for SubwayStation ModelAdmin:   admin. py:    from django. contrib import admin from django import forms from . models import SubwayLine, SubwayStation, City class SubwayLineAdmin(admin. ModelAdmin):   list_display = 'name', class SubwayStationForm(forms. ModelForm):   def __init__(self, *args, **kwargs):     super(SubwayStationForm, self). __init__(*args, **kwargs)     self. fields['line']. queryset = SubwayLine. objects. all()\       . select_related('city')   class Meta:     model = SubwayStation class SubwayStationAdmin(admin. ModelAdmin):   list_display = 'name',   form = SubwayStationForm class CityAdmin(admin. ModelAdmin):   list_display = 'name', admin. site. register(SubwayLine, SubwayLineAdmin) admin. site. register(SubwayStation, SubwayStationAdmin) admin. site. register(City, CityAdmin)    And now we have only two database calls (thanks to INNER JOIN):    SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id  FROM  main_subwaystation  WHERE  main_subwaystation .  id  = 1 ; SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_subwayline  INNER JOIN  main_city  ON ( main_subwayline .  city_id  =  main_city .  id );      Use raw_id_fields (usefull, when there are big amount of records so it is not comfortable and expensive to show them in selectbox)   Set raw_id_fields for SubwayStationAdmin:   admin. py:    class SubwayStationAdmin(admin. ModelAdmin):   list_display = 'name',   raw_id_fields = 'line',    Here how page now looks like:     Check database attempts:    SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id  FROM  main_subwaystation  WHERE  main_subwaystation .  id  = 1 ; SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwayline  WHERE  main_subwayline .  id  = 1 ; SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)    Three calls, where third is for showing city in SubwayStations’s unicode. Good. If we want, we can go further and make only two calls (exclude third request). We’ll need to redefine ForeignKeyRawIdWidget widget:   admin. py:    from django. contrib import admin from django. contrib. admin. widgets import ForeignKeyRawIdWidget from django. contrib. admin. sites import site from django. utils. text import Truncator from django. utils. html import escape from django import forms from . models import SubwayLine, SubwayStation, City class SubwayLineAdmin(admin. ModelAdmin):   list_display = 'name', class StationForeignKeyRawIdWidget(ForeignKeyRawIdWidget):   def label_for_value(self, value):     key = self. rel. get_related_field(). name     try:       obj = self. rel. to. _default_manager. select_related('city'). using(self. db). get(**{key: value})       return '&amp;nbsp;&lt;strong&gt;%s&lt;/strong&gt;' % escape(Truncator(obj). words(14, truncate='. . . '))     except (ValueError, self. rel. to. DoesNotExist):       return '' class SubwayStationForm(forms. ModelForm):   def __init__(self, *args, **kwargs):     super(SubwayStationForm, self). __init__(*args, **kwargs)     self. fields['line']. widget = StationForeignKeyRawIdWidget(       SubwayStation. _meta. get_field( line ). rel, site)   class Meta:     model = SubwayStation class SubwayStationAdmin(admin. ModelAdmin):   list_display = 'name',   raw_id_fields = 'line',   form = SubwayStationForm class CityAdmin(admin. ModelAdmin):   list_display = 'name', admin. site. register(SubwayLine, SubwayLineAdmin) admin. site. register(SubwayStation, SubwayStationAdmin) admin. site. register(City, CityAdmin)    And now only two requests are made:    SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id  FROM  main_subwaystation  WHERE  main_subwaystation .  id  = 1 ; SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_subwayline  INNER JOIN  main_city  ON ( main_subwayline .  city_id  =  main_city .  id ) WHERE  main_subwayline .  id  = 1 ;    Note, that given code with redefined ForeignKeyRawIdWidget will work with django 1. 4, 1. 5. For earlier version method label_for_value differs, so it is needed to copy this method from class ForeignKeyRawIdWidget from module django/contrib/admin/widgets. py and add to it . select_related('city').  2. __unicode__ in inline forms: Described solutions can be applied to inline admin objects. Lets say, that we have additional model District. And SubwayStation has ForeignKey to it: models. py: # . . . class District(models. Model):  name = models. CharField(max_length=50)  city = models. ForeignKey(City, related_name='districts')  def __unicode__(self):    return u {0}, {1} . format(self. city, self. name)class SubwayStation(models. Model):  name = models. CharField(max_length=50)  line = models. ForeignKey(SubwayLine, related_name='stations')  district = models. ForeignKey(District, related_name='stations')  def __unicode__(self):    return self. name# . . . As we can see, District __unicode__ includes related field City. name. Now add SubwayStation as inline to SubwayLine: admin. py: # . . . class SubwayStationInline(admin. TabularInline):  model = SubwayStation  extra = 0class SubwayLineAdmin(admin. ModelAdmin):  list_display = 'name',  inlines = SubwayStationInline,# . . . Open SubwayLine change page: Check database calls (i’ve made only two stations for this line, and three districts in total): SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwayline  WHERE  main_subwayline .  id  = 1 ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwaystation .  district_id  FROM  main_subwaystation  WHERE  main_subwaystation .  line_id  = 1 ORDER BY  main_subwaystation .  id  ASC;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city ;SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id  FROM  main_district ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id  FROM  main_district ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id  FROM  main_district ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)For each inline station there is request to get all distrcits + for each district request to get city. Here we can exclude city request for each district by adding select_related('city') in Inline form: admin. py: # . . . from django import formsfrom . models import SubwayLine, SubwayStation, City, Districtclass SubwayStationForm(forms. ModelForm):  def __init__(self, *args, **kwargs):    super(SubwayStationForm, self). __init__(*args, **kwargs)    self. fields['district']. queryset = District. objects. all()\      . select_related('city')  class Meta:    model = SubwayStationclass SubwayStationInline(admin. TabularInline):  model = SubwayStation  form = SubwayStationForm  extra = 0class SubwayLineAdmin(admin. ModelAdmin):  list_display = 'name',  inlines = SubwayStationInline,# . . . Now there are no separate city requests: SELECT  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwayline  WHERE  main_subwayline .  id  = 1 ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwaystation .  district_id  FROM  main_subwaystation  WHERE  main_subwaystation .  line_id  = 1 ORDER BY  main_subwaystation .  id  ASC;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;1,)SELECT  main_city .  id ,  main_city .  name  FROM  main_city ;SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_district  INNER JOIN  main_city  ON ( main_district .  city_id  =  main_city .  id );SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_district  INNER JOIN  main_city  ON ( main_district .  city_id  =  main_city .  id );SELECT  main_district .  id ,  main_district .  name ,  main_district .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_district  INNER JOIN  main_city  ON ( main_district .  city_id  =  main_city .  id );3. __unicode__ at change list page: Lets look at SubwayStation change list page. I’ll remind the code: models. py: # . . . class SubwayLine(models. Model):  name = models. CharField(max_length=50)  city = models. ForeignKey(City, related_name='lines')  def __unicode__(self):    return u {0}, {1} . format(self. city, self. name)class SubwayStation(models. Model):  name = models. CharField(max_length=50)  line = models. ForeignKey(SubwayLine, related_name='stations')  def __unicode__(self):    return self. name# . . . admin. py: # . . . class SubwayStationAdmin(admin. ModelAdmin):  list_display = 'name', 'line'# . . . Open the page: For each station there is line field. And unicode of line includes city name. Database attempts: SELECT COUNT(*) FROM  main_subwaystation ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_subwaystation  INNER JOIN  main_subwayline  ON ( main_subwaystation .  line_id  =  main_subwayline .  id ) INNER JOIN  main_city  ON ( main_subwayline .  city_id  =  main_city .  id ) ORDER BY  main_subwaystation .  id  DESC;Looks good. Django have already used select_related at change list page. But there is a little trap. Reading documentation about select_related carefully we can see, that  by default, select_related() does not follow foreign keys that have null=True. So in our case select_related works, because we don’t have null=True for ForeignKey. Lets add null=True: models. py: # . . . class SubwayStation(models. Model):  name = models. CharField(max_length=50)  line = models. ForeignKey(SubwayLine, related_name='stations', null=True, blank=True)  def __unicode__(self):    return self. name# . . . Open change list page and look at database attempts: SELECT COUNT(*) FROM  main_subwaystation ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id  FROM  main_subwaystation  INNER JOIN  main_subwayline  ON ( main_subwaystation .  line_id  =  main_subwayline .  id ) ORDER BY  main_subwaystation  SC;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;SELECT  main_city .  id ,  main_city .  name  FROM  main_city  WHERE  main_city .  id  = 1 ;Oops. Now, to show __unicode__ for each SubwayLine separate database call to main_city table happens. To avoid this amount of calls, we have to redefine queryset for change list page (specify select_related with needed fields explicitly): # . . . class SubwayStationAdmin(admin. ModelAdmin):  list_display = 'name', 'line'  def queryset(self, request):    qs = super(SubwayStationAdmin, self). queryset(request)    qs = qs. select_related('line__city')    return qs# . . . Now all good: SELECT COUNT(*) FROM  main_subwaystation ;SELECT  main_subwaystation .  id ,  main_subwaystation .  name ,  main_subwaystation .  line_id ,  main_subwayline .  id ,  main_subwayline .  name ,  main_subwayline .  city_id ,  main_city .  id ,  main_city .  name  FROM  main_subwaystation  INNER JOIN  main_subwayline  ON ( main_subwaystation .  line_id  =  main_subwayline .  id ) LEFT OUTER JOIN  main_city  ON ( main_subwayline .  city_id  =  main_city .  id ) ORDER BY  main_subwaystation .  id  DESC;"
    }, {
    "id": 28,
    "url": "https://st4lk.github.io/en/blog/2013/03/05/python-mutable-default-arguments.html",
    "title": "Python function with mutable default arguments",
    "body": "2013/03/05 -  In python default function arguments are created during executing instruction def and not at each function call. If argument value is immutable object (for example string, integer, tuple) it is ok, but if value is mutable, then there can be a trap: def foo(l=[]):  l. append('x')  return lIt seems, that every call to foo() will return list [‘x’]. But: &gt;&gt;&gt; foo()['x']&gt;&gt;&gt; foo()['x', 'x']&gt;&gt;&gt; foo()['x', 'x', 'x']So, if it is needed to create empty list at every call, you should do: def bar(l=None):  if l is None:    l = []  l. append('x')  return lHowever, sometimes this behaviour can be usefull. Here is a way to know function call count: from itertools import countdef bar(call_count=count()):  return next(call_count)&gt;&gt;&gt; bar()0&gt;&gt;&gt; bar()1&gt;&gt;&gt; bar()2&gt;&gt;&gt; bar()3"
    }, {
    "id": 29,
    "url": "https://st4lk.github.io/en/blog/2013/02/26/multilanguage-site-django-without-redirects.html",
    "title": "Multilanguage site on django without redirects",
    "body": "2013/02/26 - Star Starting from django 1. 4, it is possible to set url prefix for each activated language. For example, we want site, that will have russian and english version. To do it, add following to settings. py: # default language, it will be used, if django can't recognize user's languageLANGUAGE_CODE = 'ru'# list of activated languagesLANGUAGES = (  ('ru', 'Russian'),  ('en', 'English'),)# enable django’s translation systemUSE_I18N = True# specify path for translation filesLOCALE_PATHS = (  os. path. join(PROJECT_DIR, 'locale'),)# add LocaleMiddlewareMIDDLEWARE_CLASSES = (  # . . .  'django. middleware. locale. LocaleMiddleware',  # . . . )And in urls. py use i18n_patterns instead of patterns: from django. conf. urls import urlfrom django. conf. urls. i18n import i18n_patternsurlpatterns = i18n_patterns('',  url(r'^about/$', 'about. view', name='about'),)In this case on access to /about/, django will try to discover language preference and make redirect to url with corresponding language prefix. Let’s say, that django has discovered russian language, then browser will be redirected to /ru/about/. But, if we’ll type url /en/about/, no redirects will occur and english version will be shown, because url already has prefix /en/. But with such pattern can be problems with site indexing by some search engines. For example yandex. ru refused to index my site for a long time, because it doesn’t fetched http code 200 from site root. With google there were no such problems, but anyway, in help page it is not recommended to use redirects. From http://support. google. com/webmasters/bin/answer. py?hl=en&amp;answer=182192:  Avoid automatic redirection based on the user’s perceived language. These redirections could prevent users (and search engines) from viewing all the versions of your site. So i decided to make tool, that will work without redirects. The algorithm is following:  If url doesn’t have language prefix, then default language will be used (settings. LANGUAGE_CODE) If url has prefix, then language will be determined from this prefix (/en/ = english), but for default language there is no prefix. Luckily, it is needed a little peace of code for this, we need just to change LocaleMiddleware and i18n_patterns. I’ve made it in repo https://github. com/st4lk/django-solid-i18n-urls. Installation:    Setup django-solid-i18n-urls, for example using pip:   pip install solid_i18n      Change LocaleMiddleware to SolidLocaleMiddleware:   MIDDLEWARE_CLASSES = (  # . . .   # remove 'django. middleware. locale. LocaleMiddleware',  'solid_i18n. middleware. SolidLocaleMiddleware',   # . . . )      Use solid_i18n_patterns instead of i18n_patterns:   from django. conf. urls import url from solid_i18n. urls import solid_i18n_patterns  urlpatterns = solid_i18n_patterns('',   url(r'^about/$', 'about. view', name='about'), )   Repo on github: https://github. com/st4lk/django-solid-i18n-urls. UPDATED: Option settings. SOLID_I18N_USE_REDIRECTS is added (False by default). If it is True, then redirects will be used with following rules:  On request to url without language prefix, for example '/', language will be determied from user preferences. If that language is not equal to default (settings. LANGUAGE_CODE), then he will be redirected to corresponding url with prefix. But if it is equal, then url without prefix will be shown.  On request to url with language prefix behaviour remains the same, i. e. language from prefix is used. Example: # settings. py: LANGUAGE_CODE = 'ru'SOLID_I18N_USE_REDIRECTS = TrueSuppose preferred user language is english. Then at request to '/' will be redirect to '/en/'. And if preferred language is russian, then no redirect will occur, i. e. on request to ‘/’ exactly that url will be rendered. Note: In such approach there is some nasty case. Imagine, that preferred browser language - english. But user wants to see russian version, that is rendered at url without prefix. If for switching language simple links are used like &lt;a href= {{ specific language url}} &gt;, then user will be always redirected to english version (from '/' to '/en/'). To avoid this, it is needed to write information about choosen language to user cookies. It can be done using builtin django set_language view. "
    }, {
    "id": 30,
    "url": "https://st4lk.github.io/en/blog/2013/02/04/script-downloading-music-vkcom-vkontakteru.html",
    "title": "Script for downloading music from vkontakte",
    "body": "2013/02/04 -   Gist A quick search of corresponding python script doesn’t give results. In post on habra link is broken. So i decided to write my own bicycle, it is avaliable here. Launch (needs installed python interpreter): python vkcom_audio_download. pyTested on python 2. 6 and 2. 7. No external libraries required. Algorithm: Script checks saved access_token. If it doesn’t exists or expired, then page with authorisation is opened in browser. After confirmation you will be redirected to https://oauth. vk. com/blank. htm#… It is needed to copy this url and paste to script console. After that downloading is started. If audio track is present on disk - no downloading happens for it. You account info will be requested by application with app_id = 3358129. It is possible to create your own Standalone-application with audio access at http://vk. com/editapp?act=create. And replace APP_ID with yours. Script avaliable here: https://gist. github. com/4708673. Script code: "
    }, {
    "id": 31,
    "url": "https://st4lk.github.io/en/blog/2013/01/03/sublime-text-and-github-gists.html",
    "title": "Sublime text and github gists",
    "body": "2013/01/03 -  In Sublime text there are big variety of useful tools, that help to write code. I’ve learned only a small part of them, currently i’m trying to write in Vintage mode (vim style cursor management). But now i want to tell about integration github gists with sublime text. If you don’t now, github gists let you save snippets as a separate file without creation full repository. But it have many repository features - versions, possibility to fork. What we’ll have at the end: Save snippet from sublime, give to it description with key words. Then again from sublime we can find snippet by key words and see it in editor. All snippets are saved in github, so they are avaliable from other computers. Remember, that sublime text has its own snippet functionality. But it is more preferable for small auto insertion. For example, when you write def in python, insert function template: def function():  passGithub gist are more suitable for something bigger - some function for exact purpose. Lets setup this functionality: Gist plugin setup: The easiest way to do it is with sublime package manager. Here is instruction for setup. In sublime press ctrl + shift + p, enter install and then gist:  Give access to your github account: Press Preferences-&gt;Package settigns-&gt;Gist-&gt;Settings User.  You can either set login + password, either token. If needed settings are not in Settings User, then they can be copied from Settings Default. But don’t change default settings. To receive token, run in system command line (curl must be installed): curl -v -u USERNAME -X POST https://api. github. com/authorizations --data  {\ scopes\ :[\ gist\ ]} where USERNAME - your github login Gist creation: Write code of your snippet in new sublime tab. I wrote code for taking content of csv file as list of lists. Press ctrl + shift + p, then gist create public and enter. Here is avaliable fuzzy searching, so i just enter public.  Give description to our snippet. It is important to include meaningful words, because search will use them. I write “Python: Get csv lines”.  There will be also request for file name, it can be skipped by pressing enter. Search for a gist: Open sublime command line by pressing ctrl + shift + p and enter gist open.  Then enter key words “python csv” And see our snippet.  This snippet is also created at github: https://gist. github. com/3931305. Links:  Code editor sublime text Repo of Gist plugin Video lesson about sublime and gist at tutsplus. com Video course about sublime at tutsplus. com"
    }, {
    "id": 32,
    "url": "https://st4lk.github.io/en/blog/2012/11/29/mobileesp-easily-detect-mobile-web-site-visitors.html",
    "title": "MobileESP: Easily detect mobile web site visitors",
    "body": "2012/11/29 - Script will be useful, if you want to show different version of site for desktop computers and mobile devices. Big variety of methods to detect mobile type. Avaliable in different languages, including python. The port to python was made by me with help from my freelance customer. Here how it can be used in django project: from mobileesp import mdetectuser_agent = request. META. get( HTTP_USER_AGENT )http_accept = request. META. get( HTTP_ACCEPT )if user_agent and http_accept:  agent = mdetect. UAgentInfo(userAgent=user_agent, httpAccept=http_accept)  #Do first! For iPhone, Android, Windows Phone 7, etc.   if agent. detectTierIphone():    HttpResponseRedirect('/myapp/i/')  #Then catch all other mobile devices  if agent. detectMobileQuick():    HttpResponseRedirect('/myapp/m/')#For traditional computers and tablets (iPad, Android, etc. )return HttpResponseRedirect('/myapp/d/')Script on code. google. com, all methods have comments with description. Description at project site. "
    }, {
    "id": 33,
    "url": "https://st4lk.github.io/en/blog/2012/11/18/debug-django-project-embedded-python-debugger-pdb.html",
    "title": "Debug django project with embedded python debugger pdb",
    "body": "2012/11/18 - I use sublime-text as code editor. It doesn’t have a debugger, so to debug django projects i often used print var_nameand look for output in local development server console. I use this approach today also, but sometimes it is great to run code step by step to see variables at each step. It can be done with embedded python debugger pdb: import pdb; pdb. set_trace()I. e. we put this line in needed place of code, where we want so stop. It is a breakpoint. Now refresh project page in browser. When project code will reach this line, browser will hang and in console we’ll see: (Pdb)We now inside debugger and can input commands, for example these:  l - (list), look the source code n - (step next) step to next line without entering inside function s - (step in) step inside to next line, i. e. if we are standing at function call, we’ll go inside this function r - (step out) step to first line after current block of code. For example, if current line is inside cycle and r is pressed, next line will be first line after this cycle.  c - (continue) continue until next breakpoint, i. e. until pdb. set_trace() p - (print) execude python code or just show variable: p var_nameExample: Suppose we have such a view: Insert import pdb; pdb. set_trace() in needed place and run dev server, if it not already started: In browser access the page, that calls this view. The page will hang: In console we see (Pdb): Lets look, where we now by command l: Make two next steps with n: Look value of variables about and about. content: Continue with c: Page is shown in browser: "
    }, {
    "id": 34,
    "url": "https://st4lk.github.io/en/blog/2012/10/08/cloud-service-openshift.html",
    "title": "Cloud service Openshift",
    "body": "2012/10/08 -  I know a few hosting providers with free account and python availability. It is Google App Engine and Alwaysdata. But recently i found great project Openshift from RedHat and this blog site is working on it. Let me describe mentioned hostings first. Google app engine: The main trouble of google app engine - limited amount of python libraries. Of course, you can use pure python libs, but you can’t setup libs that requires C compilation. For example, pycurl library can’t be used. Also GAE uses special data bases, so libs to work with them are also special. Therefore starting of django application is not trivial, because django works with SQL databases. In additional, django can be used only with version 1. 2, 1. 3 (while 1. 4 currently avaliable). Anyway, here some useful links: list of avaliable libraries, django-rocket-engine project. Alwaysdata: With free account you can’t setup libraries, that requires C compilation. But here we have familiar data bases - mysql, postgres, mongodb. So usual django application can be started easily. I often use this hosting to demonstrate simple projects. Openshift: Openshift - it is PaaS, i. e. platform as a service. We don’t have root access to operating system. The system is provided to us by service in a working state, and we can make only limited amount of actions on it. It is similiar to google app engine, but here we have much more freedom. First of all it is great, that many things can be installed manually. Any version of python, django. If you want SQL/Postgres/mongodb as data base - you are welcome. You can setup libraries and compile them. There are additional cool things: cron, statistics, phpmyadmin and etc. I’ve mentioned django, but it is possible to use any framework, it just what i’ve tried. Handy style of deploying appliction on server. Just type git push special_application_url from you git-repository and thats it! Files automaticly refreshes on server and server restarts. Process of restarting can be managed by special scripts. For example, you can tell the script to setup necessary libraries from requirements. txt, compile static files (manage. py collectstatic) and so on. This script will be runned at every server restart. For free account it is available 1 Gb of disc space and 3 small ‘gears’. As I understand, gear - it is some isolated environment with its RAM. For a small gear size of RAM is equal to 512. If power of one gear is not enough, then second is started, then third and so on. To keep it simple, here is example of load, that simple DLE site can process (from openshift description): 15 pages/second, Hundreds of articles, 50k visitors per month. For a simple site it is quite enough. For one account you can create 3 applications (at least for free). Of course, there is possibility, that this freebie will end at some time. But existing code you can always run at other hosting - it doesn’t have special featers as GAE does. Finally you can buy payment account at openshift. On openshift site it is described in detail, how to start application. I will describe my experience. Steps to start python 2. 7. 3 + django application:  register setup git (if not already) setup special program “rhc” (it can be skipped, but this program helps a lot), manul: get-started follow instructions https://github. com/ehazlett/openshift-diy-py27-django (simple application without data base) or follow instructions https://github. com/st4lk/lexev (application with mysql, the code of this blog)That it! Application is available at http://&lt;app_name&gt;-&lt;namespace&gt;. rhcloud. com/ Link to domain: Well, not quite yet. It will be nice to have normal address… Suppose we have domain (for example lexev. org). Lets link our application to that domain. Openshift doesn’t have DNS servers, that can be set for domain. Instead of it Openshift sujest to use CNAME. It can be done in domain management page. That all nice, but i’ve bought my domain at nic. ru. To create CNAME record there i must buy additional access… But, as it turned out, there is way out! Let’s use free service http://freedns. afraid. org. So,  register at freedns. afraid. org   set DNS servers for domain:    ns1. afraid. org ns2. afraid. org ns3. afraid. org ns4. afraid. org      add domain at afraid. org: http://freedns. afraid. org/domain/add. php, in my case it is lexev. org. Don’t forget to select Shared State: Private, or you domain will be avaliable to all users at afraid. org     for newly created domain add subdomen with type CNAME, as on picture (replace lexev. org to needed domain and set correct url of application at destination field):     at subdomens page we see two records: one with CNAME and one without (it is on top probably) press record without CNAME change nothing, but click “Forward to a URL”   type as on picture (again replace lexev. org to needed domain):       finally the subdomens must be:    For now looks that all done. Some time will be needed for new DNS servers to start work with newly domain. Request to lexev. org will be redirected to www. lexev. org. And www. lexev. org points to application at openshift. It should be noted, that openshift also supports PHP, Ruby, Java, Node. js, Perl ! Thanks for reading and sorry for my english! :) "
    }, {
    "id": 35,
    "url": "https://st4lk.github.io/en/blog/2012/08/31/aphorism-messenger.html",
    "title": "Aphorism messenger",
    "body": "2012/08/31 -  I have an interesting project I want to tell you about. The idea to create it was born when I was learning Java. I have read couple of books, made some small task programs but I would like to create something bigger. Project summary: Desktop program (client), that lives in a tray and periodically shows aphorisms. It takes aphorisms from web-service (server), so data base with aphorisms lives in one place and it doesn’t attached with each client. Aphorims have rating - it is amount of likes. When pop-up window with quotation arrives, user sees its rating and can edit it by giving or remove likes. Along with aphorism in the window there is an author. User can click it and look for an author biography. Also, there is an ability to look for your favorite aphorisms and list of top favorite aphorisms. The result (download, only in russian): Full project (client and server) is written in Java. Server hosts at google app engine, provides resources in xml format. It is a REST web service, so it can be used in many ways, for example for web site. Specification is avaliable here. Here is an example of resource: show random aphorism. Links:  detailed description of project (russian) client program server client source server source video review"
    }];

var idx = lunr(function () {
    this.ref('id')
    this.field('title')
    this.field('body')

    documents.forEach(function (doc) {
        this.add(doc)
    }, this)
});
function lunr_search(term) {
    document.getElementById('lunrsearchresults').innerHTML = '<ul></ul>';
    if(term) {
        document.getElementById('lunrsearchresults').innerHTML = "<p>Search results for '" + term + "'</p>" + document.getElementById('lunrsearchresults').innerHTML;
        //put results on the screen.
        var results = idx.search(term);
        if(results.length>0){
            //console.log(idx.search(term));
            //if results
            for (var i = 0; i < results.length; i++) {
                // more statements
                var ref = results[i]['ref'];
                var url = documents[ref]['url'];
                var title = documents[ref]['title'];
                var body = documents[ref]['body'].substring(0,160)+'...';
                document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML + "<li class='lunrsearchresult'><a href='" + url + "'><span class='title'>" + title + "</span><br /><span class='body'>"+ body +"</span><br /><span class='url'>"+ url +"</span></a></li>";
            }
        } else {
            document.querySelectorAll('#lunrsearchresults ul')[0].innerHTML = "<li class='lunrsearchresult'>No results found...</li>";
        }
    }
    return false;
}
</script>

<style>
    #lunrsearchresults {padding-top: 0.2rem;}
    .lunrsearchresult {padding-bottom: 1rem;}
    .lunrsearchresult .title {color: #d9230f;}
    .lunrsearchresult .url {color: silver;}
    .lunrsearchresult a {display: block; color: #777;}
    .lunrsearchresult a:hover, .lunrsearchresult a:focus {text-decoration: none;}
    .lunrsearchresult a:hover .title {text-decoration: underline;}
</style>

<form onsubmit="return lunr_search(document.getElementById('lunrsearch').value);">
    <p><input type="text" class="form-control" id="lunrsearch" name="q" maxlength="255" value="" placeholder="Search" /></p>
</form>
<div id="lunrsearchresults">
    <ul></ul>
</div>


  </div>

</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/en/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li class="p-name"></li><li><a class="u-email" href="mailto:alexevseev@gmail.com">alexevseev@gmail.com</a></li></ul>
      </div>

      <div class="footer-col footer-col-2"></div>

    </div>

  </div>

</footer>
<script id="dsq-count-scr" src="//lexev-dev.disqus.com/count.js" async></script></body>

</html>
